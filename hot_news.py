import os
import smtplib
import requests
import json
import re
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ====================== æ–°é—»æºé…ç½® ======================
NEWS_SOURCES = {
    # æ—¶æ”¿ç±»
    "people": {"name": "äººæ°‘ç½‘", "category": "æ—¶æ”¿", "enabled": True},
    "xinhua": {"name": "æ–°åç½‘", "category": "æ—¶æ”¿", "enabled": True},
    
    # ç»¼åˆçƒ­ç‚¹
    "weibo": {"name": "å¾®åšçƒ­æœ", "category": "çƒ­ç‚¹", "enabled": True},
    "zhihu": {"name": "çŸ¥ä¹çƒ­æ¦œ", "category": "çƒ­ç‚¹", "enabled": True},
    "baidu": {"name": "ç™¾åº¦çƒ­æœ", "category": "çƒ­ç‚¹", "enabled": True},
    
    # ç»¼åˆæ–°é—»
    "toutiao": {"name": "ä»Šæ—¥å¤´æ¡", "category": "çƒ­ç‚¹", "enabled": True},
    "sina": {"name": "æ–°æµªæ–°é—»", "category": "çƒ­ç‚¹", "enabled": True},
    "netease": {"name": "ç½‘æ˜“æ–°é—»", "category": "çƒ­ç‚¹", "enabled": True},
    
    # ä¸“ä¸šåª’ä½“
    "thepaper": {"name": "æ¾æ¹ƒæ–°é—»", "category": "æ—¶æ”¿", "enabled": True},
    
    # ç§‘æŠ€ç±»
    "ithome": {"name": "ITä¹‹å®¶", "category": "ç§‘æŠ€", "enabled": True},
}

# ====================== æ–°é—»æŠ“å–å‡½æ•° ======================

def get_people_news():
    """è·å–äººæ°‘ç½‘è¦é—»"""
    news_list = []
    try:
        url = "http://www.people.com.cn/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # äººæ°‘ç½‘è¦é—»
        items = soup.select('.news_box .news a, .rmw_list a, .hdNews a', limit=15)
        
        for i, item in enumerate(items[:15], 1):
            title = item.text.strip()
            if title and len(title) > 4 and 'äººæ°‘ç½‘' not in title:
                news_list.append(f"{i}. {title}")
        
        if not news_list:
            items = soup.find_all('a', href=re.compile(r'/n1/'), limit=15)
            for i, item in enumerate(items[:15], 1):
                title = item.text.strip()
                if title and len(title) > 4:
                    news_list.append(f"{i}. {title}")
        
    except Exception as e:
        logging.error(f"äººæ°‘ç½‘æŠ“å–å¤±è´¥: {e}")
    
    return news_list[:10] if news_list else ["äººæ°‘ç½‘ï¼šæš‚æ— æ•°æ®"]

def get_xinhua_news():
    """è·å–æ–°åç½‘è¦é—»"""
    news_list = []
    try:
        url = "http://www.xinhuanet.com/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ–°åç½‘å¤´æ¡æ–°é—»
        items = soup.select('.tit, .news-item h3, .hdNews a', limit=15)
        
        for i, item in enumerate(items[:15], 1):
            title = item.text.strip()
            if title and len(title) > 4 and 'æ–°åç½‘' not in title:
                news_list.append(f"{i}. {title}")
        
    except Exception as e:
        logging.error(f"æ–°åç½‘æŠ“å–å¤±è´¥: {e}")
    
    return news_list[:10] if news_list else ["æ–°åç½‘ï¼šæš‚æ— æ•°æ®"]

def get_weibo_hot():
    """è·å–å¾®åšçƒ­æœ"""
    news_list = []
    try:
        # ä½¿ç”¨APIæ¥å£
        url = "https://weibo.com/ajax/side/hotSearch"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://weibo.com/'
        }
        response = requests.get(url, headers=headers, timeout=10)
        data = response.json()
        
        for i, item in enumerate(data['data']['realtime'][:15], 1):
            title = item['note']
            hot = item.get('num', 0)
            if title and 'æ¨è' not in title:
                if hot > 0:
                    news_list.append(f"{i}. {title} ğŸ”¥{hot//10000}w")
                else:
                    news_list.append(f"{i}. {title}")
        
    except Exception as e:
        logging.error(f"å¾®åšçƒ­æœæŠ“å–å¤±è´¥: {e}")
        try:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥æŠ“å–é¡µé¢
            url = "https://s.weibo.com/top/summary"
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            items = soup.select('.td-02 a', limit=15)
            for i, item in enumerate(items[:15], 1):
                title = item.text.strip()
                if title and 'çƒ­æœ' not in title:
                    news_list.append(f"{i}. {title}")
        except:
            pass
    
    return news_list[:10] if news_list else ["å¾®åšçƒ­æœï¼šæš‚æ— æ•°æ®"]

def get_zhihu_hot():
    """è·å–çŸ¥ä¹çƒ­æ¦œ"""
    news_list = []
    try:
        url = "https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total?limit=50"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://www.zhihu.com/hot'
        }
        response = requests.get(url, headers=headers, timeout=10)
        data = response.json()
        
        for i, item in enumerate(data['data'][:15], 1):
            title = item['target']['title']
            if title:
                news_list.append(f"{i}. {title}")
        
    except Exception as e:
        logging.error(f"çŸ¥ä¹çƒ­æ¦œæŠ“å–å¤±è´¥: {e}")
    
    return news_list[:10] if news_list else ["çŸ¥ä¹çƒ­æ¦œï¼šæš‚æ— æ•°æ®"]

def get_baidu_hot():
    """è·å–ç™¾åº¦çƒ­æœ"""
    news_list = []
    try:
        url = "https://top.baidu.com/board?tab=realtime"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç™¾åº¦çƒ­æœæ ‡é¢˜
        items = soup.select('.c-single-text-ellipsis', limit=15)
        
        for i, item in enumerate(items[:15], 1):
            title = item.text.strip()
            if title and len(title) > 2:
                news_list.append(f"{i}. {title}")
        
    except Exception as e:
        logging.error(f"ç™¾åº¦çƒ­æœæŠ“å–å¤±è´¥: {e}")
    
    return news_list[:10] if news_list else ["ç™¾åº¦çƒ­æœï¼šæš‚æ— æ•°æ®"]

def get_toutiao_hot():
    """è·å–ä»Šæ—¥å¤´æ¡çƒ­æ¦œ"""
    news_list = []
    try:
        url = "https://www.toutiao.com/hot-event/hot-board/?origin=toutiao_pc"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://www.toutiao.com/'
        }
        response = requests.get(url, headers=headers, timeout=10)
        data = response.json()
        
        for i, item in enumerate(data['data'][:15], 1):
            title = item['Title']
            hot = item.get('HotValue', 0)
            if title:
                if hot > 10000:
                    news_list.append(f"{i}. {title} ğŸ”¥{hot//10000}w")
                else:
                    news_list.append(f"{i}. {title}")
        
    except Exception as e:
        logging.error(f"ä»Šæ—¥å¤´æ¡æŠ“å–å¤±è´¥: {e}")
    
    return news_list[:10] if news_list else ["ä»Šæ—¥å¤´æ¡ï¼šæš‚æ— æ•°æ®"]

def get_sina_news():
    """è·å–æ–°æµªæ–°é—»"""
    news_list = []
    try:
        url = "https://news.sina.com.cn/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ–°æµªå¤´æ¡æ–°é—»
        items = soup.select('.blk122, .news-item h2 a, .news-top a', limit=15)
        
        for i, item in enumerate(items[:15], 1):
            title = item.text.strip()
            if title and len(title) > 4:
                news_list.append(f"{i}. {title}")
        
    except Exception as e:
        logging.error(f"æ–°æµªæ–°é—»æŠ“å–å¤±è´¥: {e}")
    
    return news_list[:10] if news_list else ["æ–°æµªæ–°é—»ï¼šæš‚æ— æ•°æ®"]

def get_netease_news():
    """è·å–ç½‘æ˜“æ–°é—»"""
    news_list = []
    try:
        url = "https://news.163.com/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç½‘æ˜“æ–°é—»å¤´æ¡
        items = soup.select('.news_title h3 a, .ndi_main a, .top_news_tt a', limit=15)
        
        for i, item in enumerate(items[:15], 1):
            title = item.text.strip()
            if title and len(title) > 4:
                news_list.append(f"{i}. {title}")
        
    except Exception as e:
        logging.error(f"ç½‘æ˜“æ–°é—»æŠ“å–å¤±è´¥: {e}")
    
    return news_list[:10] if news_list else ["ç½‘æ˜“æ–°é—»ï¼šæš‚æ— æ•°æ®"]

def get_thepaper_news():
    """è·å–æ¾æ¹ƒæ–°é—»"""
    news_list = []
    try:
        url = "https://www.thepaper.cn/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ¾æ¹ƒæ–°é—»å¤´æ¡
        items = soup.select('.news_tu h2 a, .newscontent h2 a, .pdtt_t a', limit=15)
        
        for i, item in enumerate(items[:15], 1):
            title = item.text.strip()
            if title and len(title) > 4:
                news_list.append(f"{i}. {title}")
        
    except Exception as e:
        logging.error(f"æ¾æ¹ƒæ–°é—»æŠ“å–å¤±è´¥: {e}")
    
    return news_list[:10] if news_list else ["æ¾æ¹ƒæ–°é—»ï¼šæš‚æ— æ•°æ®"]

def get_ithome_news():
    """è·å–ITä¹‹å®¶æ–°é—»"""
    news_list = []
    try:
        url = "https://www.ithome.com/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ITä¹‹å®¶æ–°é—»
        items = soup.select('.title a, .news_title a, .bl a', limit=15)
        
        for i, item in enumerate(items[:15], 1):
            title = item.text.strip()
            if title and len(title) > 4:
                news_list.append(f"{i}. {title}")
        
    except Exception as e:
        logging.error(f"ITä¹‹å®¶æŠ“å–å¤±è´¥: {e}")
    
    return news_list[:10] if news_list else ["ITä¹‹å®¶ï¼šæš‚æ— æ•°æ®"]

# ====================== æ–°é—»åˆ†ç±»ä¸æ•´ç† ======================

def categorize_news(news_data):
    """å°†æ–°é—»æŒ‰ç…§ç±»åˆ«åˆ†ç±»"""
    categorized = {
        "æ—¶æ”¿": [],
        "ç»æµ": [],
        "æ°‘ç”Ÿ": [],
        "ç§‘æŠ€": [],
        "çƒ­ç‚¹": []
    }
    
    # æºåˆ°ç±»åˆ«çš„æ˜ å°„
    source_to_category = {
        "people": "æ—¶æ”¿",
        "xinhua": "æ—¶æ”¿",
        "thepaper": "æ—¶æ”¿",
        "weibo": "çƒ­ç‚¹",
        "zhihu": "çƒ­ç‚¹",
        "baidu": "çƒ­ç‚¹",
        "toutiao": "çƒ­ç‚¹",
        "sina": "çƒ­ç‚¹",
        "netease": "çƒ­ç‚¹",
        "ithome": "ç§‘æŠ€"
    }
    
    for source_id, data in news_data.items():
        category = source_to_category.get(source_id, "çƒ­ç‚¹")
        for news in data['news']:
            # æ¸…æ´—æ–°é—»æ ‡é¢˜
            clean_news = re.sub(r'\d+\.\s*', '', news)  # ç§»é™¤å‰é¢çš„åºå·
            clean_news = re.sub(r'ğŸ”¥\d+w', '', clean_news).strip()  # ç§»é™¤çƒ­åº¦æ ‡ç­¾
            
            # æ ¹æ®å…³é”®è¯è¿›ä¸€æ­¥åˆ†ç±»
            final_category = category
            if category == "çƒ­ç‚¹":
                # å…³é”®è¯åˆ†ç±»
                tech_keywords = ['AI', 'äººå·¥æ™ºèƒ½', 'èŠ¯ç‰‡', '5G', 'äº’è”ç½‘', 'ç§‘æŠ€', 'æ•°ç ', 'æ‰‹æœº', 'ç”µè„‘', 'è½¯ä»¶', 'æ¸¸æˆ']
                economy_keywords = ['ç»æµ', 'è‚¡å¸‚', 'é‡‘è', 'æŠ•èµ„', 'GDP', 'æ¶ˆè´¹', 'è´¸æ˜“', 'è´§å¸']
                people_keywords = ['æ°‘ç”Ÿ', 'æ•™è‚²', 'åŒ»ç–—', 'ç¤¾ä¿', 'å°±ä¸š', 'ä½æˆ¿', 'å…»è€', 'äº¤é€š']
                politics_keywords = ['å¤–äº¤', 'å›½é˜²', 'æ”¿åºœ', 'ä¼šè®®', 'æ”¿ç­–', 'æ³•å¾‹', 'æ³•è§„']
                
                if any(keyword in clean_news for keyword in tech_keywords):
                    final_category = "ç§‘æŠ€"
                elif any(keyword in clean_news for keyword in economy_keywords):
                    final_category = "ç»æµ"
                elif any(keyword in clean_news for keyword in people_keywords):
                    final_category = "æ°‘ç”Ÿ"
                elif any(keyword in clean_news for keyword in politics_keywords):
                    final_category = "æ—¶æ”¿"
            
            categorized[final_category].append({
                'source': data['name'],
                'title': clean_news,
                'original': news
            })
    
    # æ¯ä¸ªç±»åˆ«åªä¿ç•™å‰5æ¡
    for category in categorized:
        categorized[category] = categorized[category][:5]
    
    return categorized

# ====================== é‚®ä»¶ç”Ÿæˆå‡½æ•° ======================

def generate_html_email(categorized_news, news_data):
    """ç”ŸæˆHTMLæ ¼å¼çš„é‚®ä»¶"""
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    current_time = datetime.now().strftime("%H:%M:%S")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_news = sum(len(items) for items in categorized_news.values())
    source_count = len([s for s in NEWS_SOURCES if NEWS_SOURCES[s]['enabled']])
    
    html = f"""
    <!DOCTYPE html>
    <html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>æ¯æ—¥çƒ­ç‚¹æ–°é—» - {today}</title>
        <style>
            body {{
                font-family: 'Microsoft YaHei', 'PingFang SC', Arial, sans-serif;
                line-height: 1.6;
                color: #333;
                max-width: 900px;
                margin: 0 auto;
                padding: 20px;
                background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            }}
            .container {{
                background: white;
                border-radius: 15px;
                box-shadow: 0 10px 30px rgba(0,0,0,0.1);
                padding: 30px;
                margin-top: 20px;
            }}
            .header {{
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                padding: 40px;
                border-radius: 12px;
                margin-bottom: 30px;
                text-align: center;
            }}
            .header h1 {{
                margin: 0;
                font-size: 32px;
                font-weight: bold;
            }}
            .header .subtitle {{
                margin-top: 10px;
                opacity: 0.9;
                font-size: 16px;
            }}
            .stats {{
                display: flex;
                justify-content: space-around;
                background: #f8f9fa;
                padding: 15px;
                border-radius: 10px;
                margin-bottom: 30px;
                font-size: 14px;
            }}
            .stat-item {{
                text-align: center;
            }}
            .stat-value {{
                font-size: 24px;
                font-weight: bold;
                color: #667eea;
            }}
            .category-section {{
                margin-bottom: 40px;
                border: 1px solid #e1e4e8;
                border-radius: 12px;
                padding: 25px;
                background: white;
            }}
            .category-title {{
                font-size: 22px;
                margin-bottom: 20px;
                padding-bottom: 10px;
                border-bottom: 3px solid;
                display: flex;
                align-items: center;
            }}
            .category-1 {{ color: #dc3545; border-color: #dc3545; }} /* æ—¶æ”¿ - çº¢è‰² */
            .category-2 {{ color: #28a745; border-color: #28a745; }} /* ç»æµ - ç»¿è‰² */
            .category-3 {{ color: #17a2b8; border-color: #17a2b8; }} /* æ°‘ç”Ÿ - é’è‰² */
            .category-4 {{ color: #ffc107; border-color: #ffc107; }} /* ç§‘æŠ€ - é»„è‰² */
            .category-5 {{ color: #6f42c1; border-color: #6f42c1; }} /* çƒ­ç‚¹ - ç´«è‰² */
            
            .news-item {{
                margin-bottom: 15px;
                padding: 15px;
                background: #f8f9fa;
                border-radius: 8px;
                border-left: 4px solid;
                transition: all 0.3s ease;
            }}
            .news-item:hover {{
                transform: translateX(5px);
                box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            }}
            .news-title {{
                font-weight: 500;
                margin-bottom: 5px;
                font-size: 16px;
            }}
            .news-source {{
                font-size: 13px;
                color: #6c757d;
            }}
            .news-source::before {{
                content: "ğŸ“° ";
            }}
            .footer {{
                text-align: center;
                margin-top: 50px;
                padding-top: 20px;
                border-top: 1px solid #e1e4e8;
                color: #6a737d;
                font-size: 14px;
            }}
            .category-icon {{
                margin-right: 10px;
                font-size: 24px;
            }}
            .hot-badge {{
                background: #ff6b6b;
                color: white;
                padding: 2px 8px;
                border-radius: 12px;
                font-size: 12px;
                margin-left: 8px;
                display: inline-block;
            }}
            .news-rank {{
                display: inline-block;
                width: 24px;
                height: 24px;
                line-height: 24px;
                text-align: center;
                background: #667eea;
                color: white;
                border-radius: 50%;
                margin-right: 10px;
                font-size: 14px;
            }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>ğŸ“° æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’</h1>
            <div class="subtitle">{today} | æ›´æ–°æ—¶é—´: {current_time}</div>
        </div>
        
        <div class="container">
            <div class="stats">
                <div class="stat-item">
                    <div class="stat-value">{total_news}</div>
                    <div>ç²¾é€‰æ–°é—»</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value">{source_count}</div>
                    <div>æ–°é—»æ¥æº</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value">{len(categorized_news)}</div>
                    <div>æ–°é—»ç±»åˆ«</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value">{datetime.now().strftime('%H:%M')}</div>
                    <div>å‘å¸ƒæ—¶é—´</div>
                </div>
            </div>
    """
    
    # ç±»åˆ«æ˜ å°„
    category_info = {
        "æ—¶æ”¿": {"icon": "ğŸ›ï¸", "class": "category-1", "desc": "å›½å®¶å¤§äº‹ æ”¿ç»è¦é—»"},
        "ç»æµ": {"icon": "ğŸ“ˆ", "class": "category-2", "desc": "è´¢ç»åŠ¨æ€ å¸‚åœºè¶‹åŠ¿"},
        "æ°‘ç”Ÿ": {"icon": "ğŸ ", "class": "category-3", "desc": "ç¤¾ä¼šç”Ÿæ´» ç™¾å§“å…³æ³¨"},
        "ç§‘æŠ€": {"icon": "ğŸ’»", "class": "category-4", "desc": "ç§‘æŠ€åˆ›æ–° æ•°ç å‰æ²¿"},
        "çƒ­ç‚¹": {"icon": "ğŸ”¥", "class": "category-5", "desc": "å…¨ç½‘çƒ­è®® ç„¦ç‚¹è¯é¢˜"}
    }
    
    # æŒ‰ç±»åˆ«æ˜¾ç¤ºæ–°é—»
    for category_idx, (category, news_items) in enumerate(categorized_news.items(), 1):
        if news_items:
            info = category_info.get(category, {"icon": "ğŸ“°", "class": "category-5", "desc": ""})
            
            html += f"""
            <div class="category-section">
                <div class="category-title {info['class']}">
                    <span class="category-icon">{info['icon']}</span>
                    {category} <small style="margin-left: 10px; font-size: 14px; opacity: 0.8;">{info['desc']}</small>
                </div>
            """
            
            for i, item in enumerate(news_items, 1):
                # æå–çƒ­åº¦ä¿¡æ¯
                original_news = item['original']
                hot_html = ""
                if 'ğŸ”¥' in original_news:
                    hot_match = re.search(r'ğŸ”¥(\d+w)', original_news)
                    if hot_match:
                        hot_html = f'<span class="hot-badge">ğŸ”¥{hot_match.group(1)}</span>'
                
                html += f"""
                <div class="news-item" style="border-left-color: {info['class'].replace('category-', 'var(--color-')})">
                    <div class="news-title">
                        <span class="news-rank">{i}</span>
                        {item['title']}
                        {hot_html}
                    </div>
                    <div class="news-source">{item['source']}</div>
                </div>
                """
            
            html += "</div>"
    
    # æ–°é—»æ¥æºè¯´æ˜
    html += """
            <div class="category-section" style="background: #f0f2f5;">
                <div class="category-title" style="color: #495057; border-color: #495057;">
                    ğŸ“‹ ä»Šæ—¥æ–°é—»æ¥æº
                </div>
                <div style="display: flex; flex-wrap: wrap; gap: 10px;">
    """
    
    for source_id, config in NEWS_SOURCES.items():
        if config['enabled'] and source_id in news_data:
            html += f"""
                    <span style="background: #e9ecef; padding: 5px 12px; border-radius: 20px; font-size: 13px;">
                        {config['name']} ({len(news_data[source_id]['news'])}æ¡)
                    </span>
            """
    
    html += """
                </div>
            </div>
            
            <div class="footer">
                <p>ğŸ“§ æœ¬é‚®ä»¶ç”± GitHub Actions è‡ªåŠ¨ç”Ÿæˆå¹¶å‘é€ | æ¯æ—¥æ—©8ç‚¹å‡†æ—¶æ¨é€</p>
                <p>ğŸ”§ æŠ€æœ¯æ”¯æŒï¼šPython + BeautifulSoup + Requests + GitHub Actions</p>
                <p>â° æ•°æ®é‡‡é›†æ—¶é—´ï¼š{}</p>
            </div>
        </div>
    </body>
    </html>
    """.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    
    return html

def generate_text_email(categorized_news):
    """ç”Ÿæˆçº¯æ–‡æœ¬æ ¼å¼çš„é‚®ä»¶"""
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    current_time = datetime.now().strftime("%H:%M:%S")
    
    text = f"""
    ğŸ“° æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’ ({today})
    ============================================
    æ›´æ–°æ—¶é—´: {current_time}
    
    """
    
    for category, news_items in categorized_news.items():
        if news_items:
            text += f"\nã€{category}ã€‘\n"
            for i, item in enumerate(news_items, 1):
                text += f"  {i}. {item['title']} [{item['source']}]\n"
            text += "\n"
    
    text += """
    ============================================
    æ¥æºï¼šäººæ°‘ç½‘ã€æ–°åç½‘ã€å¾®åšã€çŸ¥ä¹ã€ç™¾åº¦ã€å¤´æ¡ã€æ–°æµªã€ç½‘æ˜“ã€æ¾æ¹ƒã€ITä¹‹å®¶ç­‰
    æ—¶é—´ï¼šæ¯æ—¥æ—©8ç‚¹è‡ªåŠ¨å‘é€
    æŠ€æœ¯æ”¯æŒï¼šGitHub Actions + Python
    """
    
    return text

# ====================== ä¸»å‡½æ•° ======================

def main():
    logging.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ¯æ—¥çƒ­ç‚¹æ–°é—»æ”¶é›†ä»»åŠ¡")
    logging.info("=" * 60)
    
    # è·å–ç¯å¢ƒå˜é‡
    sender = os.getenv('EMAIL_SENDER')
    password = os.getenv('EMAIL_PASSWORD')
    receiver = os.getenv('EMAIL_RECEIVER')
    
    if not all([sender, password, receiver]):
        logging.error("âŒ é”™è¯¯ï¼šç¯å¢ƒå˜é‡æœªå®Œå…¨è®¾ç½®")
        return False
    
    # æ–°é—»æºå‡½æ•°æ˜ å°„
    source_functions = {
        'people': get_people_news,
        'xinhua': get_xinhua_news,
        'weibo': get_weibo_hot,
        'zhihu': get_zhihu_hot,
        'baidu': get_baidu_hot,
        'toutiao': get_toutiao_hot,
        'sina': get_sina_news,
        'netease': get_netease_news,
        'thepaper': get_thepaper_news,
        'ithome': get_ithome_news
    }
    
    # æ”¶é›†æ‰€æœ‰æ–°é—»
    news_data = {}
    for source_id, config in NEWS_SOURCES.items():
        if config['enabled']:
            try:
                logging.info(f"ğŸ“¡ æ­£åœ¨æŠ“å– {config['name']}...")
                news_list = source_functions[source_id]()
                news_data[source_id] = {
                    'name': config['name'],
                    'news': news_list
                }
                logging.info(f"   âœ… æˆåŠŸæŠ“å– {len(news_list)} æ¡æ–°é—»")
                time.sleep(1.5)  # ç¤¼è²Œè®¿é—®é—´éš”
            except Exception as e:
                logging.error(f"   âŒ æŠ“å–å¤±è´¥: {e}")
                news_data[source_id] = {
                    'name': config['name'],
                    'news': [f"æ•°æ®è·å–å¤±è´¥"]
                }
    
    # åˆ†ç±»æ•´ç†æ–°é—»
    logging.info("\nğŸ“Š æ­£åœ¨åˆ†ç±»æ•´ç†æ–°é—»...")
    categorized_news = categorize_news(news_data)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_by_category = {cat: len(items) for cat, items in categorized_news.items()}
    logging.info("ğŸ“ˆ æ–°é—»åˆ†ç±»ç»Ÿè®¡:")
    for category, count in total_by_category.items():
        logging.info(f"   {category}: {count} æ¡")
    
    # ç”Ÿæˆå¹¶å‘é€é‚®ä»¶
    try:
        logging.info(f"\nğŸ“§ æ­£åœ¨ç”Ÿæˆå¹¶å‘é€é‚®ä»¶åˆ° {receiver}...")
        
        # åˆ›å»ºé‚®ä»¶
        msg = MIMEMultipart('alternative')
        msg['From'] = f"æ¯æ—¥æ–°é—»é€Ÿé€’ <{sender}>"
        msg['To'] = receiver
        today_str = datetime.now().strftime('%mæœˆ%dæ—¥')
        msg['Subject'] = f"ğŸ“° æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’ {today_str}"
        
        # æ·»åŠ çº¯æ–‡æœ¬ç‰ˆæœ¬
        text_content = generate_text_email(categorized_news)
        part1 = MIMEText(text_content, 'plain', 'utf-8')
        msg.attach(part1)
        
        # æ·»åŠ HTMLç‰ˆæœ¬
        html_content = generate_html_email(categorized_news, news_data)
        part2 = MIMEText(html_content, 'html', 'utf-8')
        msg.attach(part2)
        
        # å‘é€é‚®ä»¶
        smtp_server = 'smtp.qq.com'
        smtp_port = 587
        
        server = smtplib.SMTP(smtp_server, smtp_port)
        server.starttls()
        server.login(sender, password)
        server.sendmail(sender, receiver, msg.as_string())
        server.quit()
        
        logging.info("âœ… é‚®ä»¶å‘é€æˆåŠŸï¼")
        return True
        
    except Exception as e:
        logging.error(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
