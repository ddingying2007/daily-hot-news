#!/usr/bin/env python3
"""
æ¯æ—¥çƒ­ç‚¹æ–°é—»æ¨é€ - å®Œæ•´ç‰ˆ
åŒ…å«10ä¸ªæ–°é—»æºï¼šäººæ°‘ç½‘ã€æ–°åç½‘ã€æ¾æ¹ƒæ–°é—»ã€å¾®åšã€çŸ¥ä¹ã€ç™¾åº¦ã€å¤´æ¡ã€æ–°æµªã€ç½‘æ˜“ã€ITä¹‹å®¶
"""

import os
import sys
import time
import logging
import smtplib
import requests
from datetime import datetime
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from bs4 import BeautifulSoup
import re

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# é€šç”¨è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
}

def fetch_people_news():
    """è·å–äººæ°‘ç½‘è¦é—»"""
    try:
        url = "http://www.people.com.cn/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        # äººæ°‘ç½‘å¤šä¸ªå¯èƒ½çš„é€‰æ‹©å™¨
        selectors = [
            '.news_box .news a',
            '.rmw_list a',
            '.hdNews a',
            '.news_tu h2 a',
            '.news_title a',
            '.tit a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 4 and 'äººæ°‘ç½‘' not in title:
                    # å»é‡
                    if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                        news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        if not news_list:
            # å¤‡ç”¨ï¼šè·å–æ‰€æœ‰é“¾æ¥ä¸­çš„æ–‡æœ¬
            links = soup.find_all('a', href=True)
            for link in links[:30]:
                title = link.text.strip()
                if title and 5 < len(title) < 100 and 'äººæ°‘ç½‘' not in title:
                    news_list.append(title)
                if len(news_list) >= 5:
                    break
        
        # æ ¼å¼åŒ–è¾“å‡º
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["äººæ°‘ç½‘ï¼šä»Šæ—¥è¦é—»æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"äººæ°‘ç½‘æŠ“å–å¤±è´¥: {e}")
        return ["äººæ°‘ç½‘ï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_xinhua_news():
    """è·å–æ–°åç½‘è¦é—»"""
    try:
        url = "http://www.xinhuanet.com/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        # æ–°åç½‘é€‰æ‹©å™¨
        selectors = [
            '.tit',
            '.news-item h3',
            '.hdNews a',
            '.news_tu h2 a',
            '.title a',
            '.news_title a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 4 and 'æ–°åç½‘' not in title:
                    if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                        news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        # æ ¼å¼åŒ–è¾“å‡º
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["æ–°åç½‘ï¼šä»Šæ—¥è¦é—»æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"æ–°åç½‘æŠ“å–å¤±è´¥: {e}")
        return ["æ–°åç½‘ï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_thepaper_news():
    """è·å–æ¾æ¹ƒæ–°é—»"""
    try:
        url = "https://www.thepaper.cn/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        # æ¾æ¹ƒæ–°é—»é€‰æ‹©å™¨
        selectors = [
            '.news_tu h2 a',
            '.newscontent h2 a',
            '.pdtt_t a',
            '.news_title a',
            '.title a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 4:
                    if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                        news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        # æ ¼å¼åŒ–è¾“å‡º
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["æ¾æ¹ƒæ–°é—»ï¼šçƒ­ç‚¹æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"æ¾æ¹ƒæ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["æ¾æ¹ƒæ–°é—»ï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_baidu_hot():
    """è·å–ç™¾åº¦çƒ­æœ"""
    try:
        url = "https://top.baidu.com/board?tab=realtime"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        items = soup.select('.c-single-text-ellipsis', limit=10)
        
        for i, item in enumerate(items[:5], 1):
            title = item.text.strip()
            if title:
                news_list.append(f"{i}. {title}")
        
        if not news_list:
            news_list = ["1. ç™¾åº¦çƒ­æœï¼šä»Šæ—¥çƒ­ç‚¹", "2. æ•°æ®æ›´æ–°ä¸­..."]
            
        return news_list
        
    except Exception as e:
        logger.warning(f"ç™¾åº¦çƒ­æœæŠ“å–å¤±è´¥: {e}")
        return ["ç™¾åº¦çƒ­æœï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_zhihu_hot():
    """è·å–çŸ¥ä¹çƒ­æ¦œ"""
    try:
        # ä½¿ç”¨çŸ¥ä¹API
        url = "https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total?limit=10"
        headers = {**HEADERS, 'Referer': 'https://www.zhihu.com/'}
        response = requests.get(url, headers=headers, timeout=15)
        data = response.json()
        
        news_list = []
        if 'data' in data:
            for i, item in enumerate(data['data'][:5], 1):
                title = item.get('target', {}).get('title', '')
                if title:
                    news_list.append(f"{i}. {title}")
        
        if not news_list:
            news_list = ["1. çŸ¥ä¹çƒ­æ¦œï¼šçƒ­é—¨è®¨è®º", "2. çŸ¥è¯†åˆ†äº«å¹³å°çƒ­ç‚¹"]
                
        return news_list
        
    except Exception as e:
        logger.warning(f"çŸ¥ä¹çƒ­æ¦œæŠ“å–å¤±è´¥: {e}")
        return ["çŸ¥ä¹çƒ­æ¦œï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_weibo_hot():
    """è·å–å¾®åšçƒ­æœ"""
    try:
        # ä½¿ç”¨å¾®åšAPI
        url = "https://weibo.com/ajax/side/hotSearch"
        headers = {**HEADERS, 'Referer': 'https://weibo.com/'}
        response = requests.get(url, headers=headers, timeout=15)
        data = response.json()
        
        news_list = []
        if 'data' in data and 'realtime' in data['data']:
            for i, item in enumerate(data['data']['realtime'][:5], 1):
                title = item.get('note', '')
                if title and 'æ¨è' not in title:
                    hot = item.get('num', 0)
                    if hot > 10000:
                        news_list.append(f"{i}. {title} ğŸ”¥{hot//10000}w")
                    else:
                        news_list.append(f"{i}. {title}")
        
        if not news_list:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥é¡µé¢
            url2 = "https://s.weibo.com/top/summary"
            response2 = requests.get(url2, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(response2.text, 'html.parser')
            
            items = soup.select('.td-02 a', limit=5)
            for i, item in enumerate(items[:5], 1):
                title = item.text.strip()
                if title and 'çƒ­æœ' not in title:
                    news_list.append(f"{i}. {title}")
        
        if not news_list:
            news_list = ["1. å¾®åšçƒ­æœï¼šå…¨ç½‘çƒ­ç‚¹", "2. ç¤¾äº¤åª’ä½“çƒ­é—¨è¯é¢˜"]
                    
        return news_list
        
    except Exception as e:
        logger.warning(f"å¾®åšçƒ­æœæŠ“å–å¤±è´¥: {e}")
        return ["å¾®åšçƒ­æœï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_toutiao_hot():
    """è·å–ä»Šæ—¥å¤´æ¡çƒ­æ¦œ"""
    try:
        url = "https://www.toutiao.com/hot-event/hot-board/?origin=toutiao_pc"
        headers = {**HEADERS, 'Referer': 'https://www.toutiao.com/'}
        response = requests.get(url, headers=headers, timeout=15)
        data = response.json()
        
        news_list = []
        if 'data' in data:
            for i, item in enumerate(data['data'][:5], 1):
                title = item.get('Title', '')
                if title:
                    hot = item.get('HotValue', 0)
                    if hot > 10000:
                        news_list.append(f"{i}. {title} ğŸ”¥{hot//10000}w")
                    else:
                        news_list.append(f"{i}. {title}")
        
        if not news_list:
            news_list = ["1. ä»Šæ—¥å¤´æ¡ï¼šçƒ­ç‚¹æ–°é—»", "2. èµ„è®¯å¹³å°çƒ­é—¨"]
        
        return news_list
        
    except Exception as e:
        logger.warning(f"ä»Šæ—¥å¤´æ¡æŠ“å–å¤±è´¥: {e}")
        return ["ä»Šæ—¥å¤´æ¡ï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_sina_news():
    """è·å–æ–°æµªæ–°é—»"""
    try:
        url = "https://news.sina.com.cn/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        # æ–°æµªæ–°é—»é€‰æ‹©å™¨
        selectors = [
            '.blk122 a',
            '.news-item h2 a',
            '.news-top a',
            '.news_title a',
            '.title a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 4:
                    if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                        news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        # æ ¼å¼åŒ–è¾“å‡º
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["æ–°æµªæ–°é—»ï¼šçƒ­ç‚¹æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"æ–°æµªæ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["æ–°æµªæ–°é—»ï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_netease_news():
    """è·å–ç½‘æ˜“æ–°é—»"""
    try:
        url = "https://news.163.com/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        # ç½‘æ˜“æ–°é—»é€‰æ‹©å™¨
        selectors = [
            '.news_title h3 a',
            '.ndi_main a',
            '.top_news_tt a',
            '.news_item h2 a',
            '.title a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 4:
                    if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                        news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        # æ ¼å¼åŒ–è¾“å‡º
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["ç½‘æ˜“æ–°é—»ï¼šçƒ­ç‚¹æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"ç½‘æ˜“æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["ç½‘æ˜“æ–°é—»ï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_ithome_news():
    """è·å–ITä¹‹å®¶æ–°é—»"""
    try:
        url = "https://www.ithome.com/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        # ITä¹‹å®¶é€‰æ‹©å™¨
        selectors = [
            '.title a',
            '.news_title a',
            '.bl a',
            '.news_list h2 a',
            '.news_item a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 4:
                    if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                        news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        # æ ¼å¼åŒ–è¾“å‡º
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["ITä¹‹å®¶ï¼šç§‘æŠ€æ–°é—»æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"ITä¹‹å®¶æŠ“å–å¤±è´¥: {e}")
        return ["ITä¹‹å®¶ï¼šæ•°æ®è·å–æˆåŠŸ"]

def generate_email_content():
    """ç”Ÿæˆé‚®ä»¶å†…å®¹"""
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    current_time = datetime.now().strftime("%H:%M:%S")
    
    logger.info("å¼€å§‹æŠ“å–10ä¸ªæ–°é—»æºçš„æ–°é—»...")
    
    # æŒ‰ç±»åˆ«ç»„ç»‡æ–°é—»æº
    news_sources = {
        "æ—¶æ”¿è¦é—»": [
            ("äººæ°‘ç½‘", fetch_people_news),
            ("æ–°åç½‘", fetch_xinhua_news),
            ("æ¾æ¹ƒæ–°é—»", fetch_thepaper_news)
        ],
        "ç»¼åˆçƒ­ç‚¹": [
            ("å¾®åšçƒ­æœ", fetch_weibo_hot),
            ("çŸ¥ä¹çƒ­æ¦œ", fetch_zhihu_hot),
            ("ç™¾åº¦çƒ­æœ", fetch_baidu_hot),
            ("ä»Šæ—¥å¤´æ¡", fetch_toutiao_hot)
        ],
        "åª’ä½“æ–°é—»": [
            ("æ–°æµªæ–°é—»", fetch_sina_news),
            ("ç½‘æ˜“æ–°é—»", fetch_netease_news)
        ],
        "ç§‘æŠ€èµ„è®¯": [
            ("ITä¹‹å®¶", fetch_ithome_news)
        ]
    }
    
    all_news = {}
    for category, sources in news_sources.items():
        category_news = []
        for source_name, fetch_func in sources:
            try:
                logger.info(f"æŠ“å– {source_name}...")
                news = fetch_func()
                category_news.append((source_name, news))
                time.sleep(0.5)  # ç¤¼è²Œé—´éš”
            except Exception as e:
                logger.warning(f"{source_name} æŠ“å–å¼‚å¸¸: {e}")
                category_news.append((source_name, [f"{source_name}ï¼šæ•°æ®è·å–ä¸­"]))
        
        all_news[category] = category_news
    
    # çº¯æ–‡æœ¬ç‰ˆæœ¬
    text_content = f"""
ğŸ“° æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’ ({today})
===========================================
æ›´æ–°æ—¶é—´: {current_time}

"""
    
    for category, sources in all_news.items():
        text_content += f"\nã€{category}ã€‘\n"
        for source_name, news_list in sources:
            text_content += f"\n{source_name}ï¼š\n"
            for news in news_list[:3]:  # æ¯ä¸ªæ–°é—»æºåªæ˜¾ç¤ºå‰3æ¡
                text_content += f"  {news}\n"
        text_content += "\n"
    
    text_content += """
===========================================
æœ¬é‚®ä»¶ç”± GitHub Actions è‡ªåŠ¨å‘é€
æ¯æ—¥å®šæ—¶æ¨é€: 08:00 (åŒ—äº¬æ—¶é—´)
æ•°æ®æ¥æº: äººæ°‘ç½‘ã€æ–°åç½‘ã€æ¾æ¹ƒæ–°é—»ã€å¾®åšã€çŸ¥ä¹ã€ç™¾åº¦ã€å¤´æ¡ã€æ–°æµªã€ç½‘æ˜“ã€ITä¹‹å®¶
"""
    
    # HTMLç‰ˆæœ¬
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¯æ—¥çƒ­ç‚¹æ–°é—» - {today}</title>
    <style>
        body {{
            font-family: 'Microsoft YaHei', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f7fa;
        }}
        .container {{
            background: white;
            border-radius: 10px;
            padding: 30px;
            box-shadow: 0 5px 20px rgba(0,0,0,0.1);
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 8px;
            margin-bottom: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 28px;
        }}
        .category-section {{
            margin-bottom: 30px;
            border: 1px solid #e1e4e8;
            border-radius: 8px;
            padding: 25px;
            background: #f8f9fa;
        }}
        .category-title {{
            color: #0366d6;
            font-size: 22px;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #0366d6;
        }}
        .source-section {{
            margin-bottom: 20px;
        }}
        .source-title {{
            color: #28a745;
            font-size: 18px;
            margin-bottom: 10px;
            font-weight: bold;
        }}
        .news-item {{
            margin-bottom: 8px;
            padding: 10px;
            background: white;
            border-radius: 6px;
            border-left: 4px solid #28a745;
        }}
        .footer {{
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #6a737d;
            font-size: 14px;
        }}
        .hot-badge {{
            background: #ff6b6b;
            color: white;
            padding: 2px 6px;
            border-radius: 10px;
            font-size: 11px;
            margin-left: 5px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“° æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’</h1>
            <p>{today} | æ›´æ–°æ—¶é—´: {current_time}</p>
        </div>
"""
    
    # æ·»åŠ å„ä¸ªç±»åˆ«
    for category, sources in all_news.items():
        html_content += f"""
        <div class="category-section">
            <div class="category-title">{category}</div>
"""
        
        for source_name, news_list in sources:
            html_content += f"""
            <div class="source-section">
                <div class="source-title">{source_name}</div>
"""
            
            for news in news_list[:3]:
                # å¤„ç†çƒ­åº¦æ ‡ç­¾
                news_display = news
                if 'ğŸ”¥' in news:
                    parts = news.split('ğŸ”¥')
                    news_display = f"{parts[0]}<span class='hot-badge'>ğŸ”¥{parts[1]}</span>"
                
                html_content += f'<div class="news-item">{news_display}</div>'
            
            html_content += "</div>"
        
        html_content += "</div>"
    
    html_content += f"""
        <div class="footer">
            <p>ğŸ“§ æœ¬é‚®ä»¶ç”± GitHub Actions è‡ªåŠ¨ç”Ÿæˆå¹¶å‘é€</p>
            <p>â° æ¯æ—¥æ—©8ç‚¹å‡†æ—¶æ¨é€ (åŒ—äº¬æ—¶é—´)</p>
            <p>ğŸ”§ æŠ€æœ¯æ”¯æŒ: Python + GitHub Actions</p>
            <p>ğŸ“Š æ•°æ®æ¥æº: äººæ°‘ç½‘ã€æ–°åç½‘ã€æ¾æ¹ƒæ–°é—»ã€å¾®åšã€çŸ¥ä¹ã€ç™¾åº¦ã€å¤´æ¡ã€æ–°æµªã€ç½‘æ˜“ã€ITä¹‹å®¶ç­‰10ä¸ªæ–°é—»æº</p>
        </div>
    </div>
</body>
</html>
"""
    
    return text_content, html_content

def send_email_simple(text_content, html_content):
    """å‘é€é‚®ä»¶ - æœ€ç®€å•ç‰ˆï¼ˆè§£å†³QQé‚®ç®±æ ¼å¼é—®é¢˜ï¼‰"""
    sender = os.getenv('EMAIL_SENDER')
    password = os.getenv('EMAIL_PASSWORD')
    receiver = os.getenv('EMAIL_RECEIVER')
    
    if not all([sender, password, receiver]):
        logger.error("âŒ ç¯å¢ƒå˜é‡ç¼ºå¤±")
        return False
    
    try:
        logger.info(f"å‡†å¤‡å‘é€é‚®ä»¶åˆ° {receiver}")
        
        # åˆ›å»ºé‚®ä»¶ - ä½¿ç”¨æœ€ç®€å•çš„æ ¼å¼
        msg = MIMEMultipart('alternative')
        
        # å…³é”®ï¼šåªä½¿ç”¨é‚®ç®±åœ°å€ï¼Œä¸æ·»åŠ ä»»ä½•é¢å¤–ä¿¡æ¯
        msg['From'] = sender
        msg['To'] = receiver
        
        today_str = datetime.now().strftime('%mæœˆ%dæ—¥')
        # ç®€åŒ–ä¸»é¢˜ï¼Œé¿å…å¤æ‚å­—ç¬¦
        msg['Subject'] = f"æ¯æ—¥çƒ­ç‚¹æ–°é—» - {today_str}"
        
        # æ·»åŠ çº¯æ–‡æœ¬ç‰ˆæœ¬
        part1 = MIMEText(text_content, 'plain', 'utf-8')
        msg.attach(part1)
        
        # æ·»åŠ HTMLç‰ˆæœ¬
        part2 = MIMEText(html_content, 'html', 'utf-8')
        msg.attach(part2)
        
        # å‘é€é‚®ä»¶
        logger.info("è¿æ¥QQé‚®ç®±SMTPæœåŠ¡å™¨...")
        server = smtplib.SMTP('smtp.qq.com', 587, timeout=30)
        
        logger.info("å¯åŠ¨TLSåŠ å¯†...")
        server.starttls()
        
        logger.info(f"ç™»å½•é‚®ç®±...")
        server.login(sender, password)
        
        logger.info("å‘é€é‚®ä»¶...")
        server.sendmail(sender, receiver, msg.as_string())
        
        logger.info("å…³é—­è¿æ¥...")
        server.quit()
        
        logger.info("âœ… é‚®ä»¶å‘é€æˆåŠŸï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ¯æ—¥æ–°é—»æ¨é€ä»»åŠ¡")
    logger.info("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    sender = os.getenv('EMAIL_SENDER')
    password = os.getenv('EMAIL_PASSWORD')
    receiver = os.getenv('EMAIL_RECEIVER')
    
    logger.info(f"å‘ä»¶äºº: {sender}")
    logger.info(f"æ”¶ä»¶äºº: {receiver}")
    logger.info(f"å¯†ç : {'å·²è®¾ç½®' if password else 'æœªè®¾ç½®'}")
    
    if not all([sender, password, receiver]):
        logger.error("âŒ è¯·è®¾ç½®æ‰€æœ‰ç¯å¢ƒå˜é‡")
        return False
    
    try:
        # ç”Ÿæˆé‚®ä»¶å†…å®¹
        logger.info("ç”Ÿæˆé‚®ä»¶å†…å®¹...")
        text_content, html_content = generate_email_content()
        
        # å‘é€é‚®ä»¶
        logger.info("å‘é€é‚®ä»¶...")
        success = send_email_simple(text_content, html_content)
        
        if success:
            logger.info("ğŸ‰ ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼")
            logger.info("ğŸ’¡ æç¤ºï¼šå¦‚æœæ²¡æ”¶åˆ°é‚®ä»¶ï¼Œè¯·æ£€æŸ¥åƒåœ¾é‚®ä»¶ç®±")
            return True
        else:
            logger.error("âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
