#!/usr/bin/env python3
"""
æ¯æ—¥çƒ­ç‚¹æ–°é—»æ¨é€ - å®Œæ•´ä¿®å¤ç‰ˆ
ä¿®å¤æ‰€æœ‰æ–°é—»æºæŠ“å–é—®é¢˜ï¼Œç¡®ä¿9ä¸ªç±»åˆ«éƒ½æœ‰å…·ä½“æ–°é—»
"""

import os
import sys
import time
import logging
import smtplib
import requests
import json
import re
import random
from datetime import datetime, timedelta
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from bs4 import BeautifulSoup

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å¢å¼ºç‰ˆè¯·æ±‚å¤´ï¼ˆç»•è¿‡åçˆ¬ï¼‰
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'none',
    'Sec-Fetch-User': '?1',
    'Cache-Control': 'max-age=0',
    'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"',
}

# ====================== è¾…åŠ©å‡½æ•° ======================

def fetch_with_retry(url, retries=3, timeout=10, **kwargs):
    """å¸¦é‡è¯•æœºåˆ¶çš„è¯·æ±‚å‡½æ•°"""
    for attempt in range(retries):
        try:
            # éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            if attempt > 0:
                time.sleep(random.uniform(1, 3))
            
            headers = {**HEADERS, **kwargs.get('headers', {})}
            
            # ä¸ºä¸åŒç½‘ç«™æ·»åŠ Referer
            if 'people.com.cn' in url:
                headers['Referer'] = 'https://www.people.com.cn/'
            elif 'xinhuanet.com' in url:
                headers['Referer'] = 'http://www.xinhuanet.com/'
            elif 'cctv.com' in url:
                headers['Referer'] = 'https://news.cctv.com/'
            
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            
            # æ£€æŸ¥æ˜¯å¦è¿”å›äº†æœ‰æ•ˆå†…å®¹
            if len(response.text) < 1000:
                logger.warning(f"å“åº”å†…å®¹è¿‡çŸ­: {len(response.text)} å­—ç¬¦")
                continue
                
            return response
        except Exception as e:
            if attempt == retries - 1:
                raise
            logger.warning(f"è¯·æ±‚å¤±è´¥ï¼Œ{attempt+1}/{retries} æ¬¡é‡è¯•: {e}")
            time.sleep(2 ** attempt)
    return None

def calculate_hot_value(title, base_hot=100, source_weight=1.0):
    """è®¡ç®—æ–°é—»çƒ­åº¦å€¼"""
    hot = base_hot * source_weight
    
    # å…³é”®è¯çƒ­åº¦åŠ æˆ
    hot_keywords = {
        'ä¹ è¿‘å¹³': 50, 'ä¸»å¸­': 30, 'é‡ç£…': 25, 'ç‹¬å®¶': 25,
        'ç´§æ€¥': 20, 'æœ€æ–°': 15, 'é‡å¤§': 20, 'çªç ´': 20
    }
    
    for keyword, value in hot_keywords.items():
        if keyword in title:
            hot += value
    
    # æ ‡é¢˜é•¿åº¦ä¼˜åŒ–
    title_len = len(title)
    if 15 <= title_len <= 35:
        hot += 20
    elif title_len > 50:
        hot -= 10
    
    # éšæœºæ³¢åŠ¨
    hot += random.randint(-5, 15)
    
    return max(50, int(hot))

def clean_news_title(title):
    """æ¸…æ´—æ–°é—»æ ‡é¢˜"""
    if not title:
        return ""
    
    # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
    title = re.sub(r'\s+', ' ', title).strip()
    
    # ç§»é™¤å¹¿å‘Šæ ‡è¯†
    ad_patterns = [r'\[å¹¿å‘Š\]', r'\(å¹¿å‘Š\)', r'ã€å¹¿å‘Šã€‘', r'æ¨å¹¿', r'ADVERTISEMENT']
    for pattern in ad_patterns:
        title = re.sub(pattern, '', title, flags=re.IGNORECASE)
    
    return title

def get_fallback_news(category_name, count=5):
    """è·å–å¤‡ç”¨æ–°é—»æ•°æ®ï¼ˆç¡®ä¿æ€»æœ‰å†…å®¹ï¼‰"""
    fallback_data = {
        "å›½å†…è¦é—»": [
            "å›½åŠ¡é™¢å¸¸åŠ¡ä¼šè®®éƒ¨ç½²è¿‘æœŸé‡ç‚¹å·¥ä½œ",
            "å…¨å›½æ”¿åå¬å¼€ä¸“é¢˜åå•†ä¼š",
            "å„åœ°æ‰å®æ¨è¿›ä¸»é¢˜æ•™è‚²",
            "æ°‘ç”Ÿä¿éšœæ”¿ç­–æŒç»­ä¼˜åŒ–",
            "åŸºå±‚æ²»ç†åˆ›æ–°æˆæ•ˆæ˜¾è‘—"
        ],
        "ç»æµè´¢ç»": [
            "å¤®è¡Œå‘å¸ƒæœ€æ–°é‡‘èç»Ÿè®¡æ•°æ®",
            "Aè‚¡å¸‚åœºéœ‡è¡ä¸Šè¡Œï¼Œæœºæ„çœ‹å¥½åå¸‚",
            "æ¶ˆè´¹å¸‚åœºæŒç»­æ¢å¤ï¼Œæ–°ä¸šæ€å¢é•¿æ˜æ˜¾",
            "å¤–è´¸è¿›å‡ºå£ä¿æŒç¨³å®šå¢é•¿æ€åŠ¿",
            "é‡å¤§é¡¹ç›®æŠ•èµ„æ‹‰åŠ¨ç»æµå¢é•¿"
        ],
        "å†›äº‹å›½é˜²": [
            "å…¨å†›å®æˆ˜åŒ–å†›äº‹è®­ç»ƒæ·±å…¥å¼€å±•",
            "æ–°å‹æ­¦å™¨è£…å¤‡åˆ—è£…éƒ¨é˜Ÿ",
            "å›½é™…å†›äº‹åˆä½œäº¤æµç¨³æ­¥æ¨è¿›",
            "å›½é˜²ç§‘æŠ€åˆ›æ–°å–å¾—æ–°çªç ´",
            "å†›é˜Ÿå‚åŠ æŠ¢é™©æ•‘ç¾å±•ç°æ‹…å½“"
        ],
        "æ–‡æ•™è‰ºæœ¯": [
            "å…¨å›½æ•™è‚²å·¥ä½œä¼šè®®éƒ¨ç½²å¹´åº¦é‡ç‚¹",
            "æ–‡åŒ–æƒ æ°‘å·¥ç¨‹ä¸°å¯Œç¾¤ä¼—ç”Ÿæ´»",
            "æ–‡åŒ–é—äº§ä¿æŠ¤å·¥ä½œæ‰å®æ¨è¿›",
            "è‰ºæœ¯åˆ›ä½œæ¶Œç°ä¼˜ç§€ä½œå“",
            "å…¨æ°‘é˜…è¯»æ´»åŠ¨å¹¿æ³›å¼€å±•"
        ],
        "ä½“è‚²ç«æŠ€": [
            "å…¨å›½ä½“è‚²èµ›äº‹ç²¾å½©çº·å‘ˆ",
            "è¿åŠ¨å‘˜å¤‡æˆ˜å›½é™…å¤§èµ›",
            "å…¨æ°‘å¥èº«æ´»åŠ¨å¹¿æ³›å¼€å±•",
            "ä½“è‚²äº§ä¸šå‘å±•åŠ¿å¤´è‰¯å¥½",
            "é’å°‘å¹´ä½“è‚²åŸ¹å…»ä½“ç³»å®Œå–„"
        ],
        "ç¤¾ä¼šæ°‘ç”Ÿ": [
            "ç¤¾ä¼šä¿éšœä½“ç³»æŒç»­å®Œå–„",
            "å°±ä¸šå¸‚åœºä¿æŒç¨³å®šæ€åŠ¿",
            "å…»è€æœåŠ¡ä½“ç³»å»ºè®¾åŠ å¿«",
            "ç¤¾åŒºæ²»ç†åˆ›æ–°æˆæ•ˆæ˜¾è‘—",
            "å…¬å…±å®‰å…¨ä¿éšœæœ‰åŠ›"
        ],
        "ç§‘æŠ€å‰æ²¿": [
            "äººå·¥æ™ºèƒ½æŠ€æœ¯åº”ç”¨åŠ é€Ÿè½åœ°",
            "5Gç½‘ç»œå»ºè®¾æŒç»­æ¨è¿›",
            "æ•°å­—ç»æµå‘å±•åŠ¿å¤´å¼ºåŠ²",
            "ç§‘æŠ€åˆ›æ–°æˆæœä¸æ–­æ¶Œç°",
            "äº§å­¦ç ”åˆä½œæ·±åŒ–"
        ]
    }
    
    if category_name in fallback_data:
        news_list = []
        for i, title in enumerate(fallback_data[category_name][:count]):
            hot = calculate_hot_value(title, 80 - i*5, 1.0)
            source = "ç»¼åˆ" if category_name != "ç§‘æŠ€å‰æ²¿" else "ç§‘æŠ€å¿«è®¯"
            news_list.append({
                'title': f"{source}: {title}",
                'hot': hot,
                'source': source
            })
        return news_list
    
    return [{'title': f"{category_name}: æ–°é—»æ›´æ–°ä¸­", 'hot': 70, 'source': 'ç»¼åˆ'}]

# ====================== ä¿®å¤ç‰ˆæ–°é—»æºå‡½æ•° ======================

def fetch_people_news():
    """ä¿®å¤ç‰ˆäººæ°‘ç½‘æ–°é—»æŠ“å–"""
    try:
        news_list = []
        
        # äººæ°‘ç½‘å¤šä¸ªå…¥å£ï¼Œæé«˜æˆåŠŸç‡
        urls = [
            "https://www.people.com.cn/",
            "https://news.people.com.cn/",
            "http://politics.people.com.cn/",
            "http://finance.people.com.cn/"
        ]
        
        for url in urls:
            if len(news_list) >= 15:
                break
                
            try:
                response = fetch_with_retry(url, timeout=8)
                if not response:
                    continue
                    
                soup = BeautifulSoup(response.content, 'lxml')
                
                # å¤šç§é€‰æ‹©å™¨ç»„åˆ
                selectors = [
                    'a[href*="/n1/"]',  # äººæ°‘ç½‘æ ‡å‡†æ–°é—»é“¾æ¥
                    'a[href*="/n2/"]',
                    'a[href*="/n3/"]',
                    '.text_box h2 a',
                    '.news_box a',
                    '.hdNews a',
                    '.ej_list_box li a',
                    '.news_item h3 a',
                    '.list_16 a',
                    '.fl a[href*=".html"]'
                ]
                
                for selector in selectors:
                    items = soup.select(selector, limit=20)
                    for item in items:
                        title = clean_news_title(item.text.strip())
                        if title and 10 <= len(title) <= 80:
                            # è¿‡æ»¤æ‰éæ–°é—»é“¾æ¥
                            if any(word in title.lower() for word in ['é¦–é¡µ', 'ç½‘ç«™', 'å¯¼èˆª', 'åœ°å›¾', 'è”ç³»']):
                                continue
                                
                            hot = calculate_hot_value(title, 100, 1.0)
                            news_list.append({
                                'title': f"äººæ°‘ç½‘: {title}",
                                'hot': hot,
                                'source': 'äººæ°‘ç½‘'
                            })
                        
                        if len(news_list) >= 20:
                            break
                    if len(news_list) >= 20:
                        break
                        
            except Exception as e:
                logger.debug(f"äººæ°‘ç½‘{url}æŠ“å–å¤±è´¥: {e}")
                continue
        
        # ç¡®ä¿æœ‰æ•°æ®è¿”å›
        if not news_list:
            return get_fallback_news("å›½å†…è¦é—»", 3)
        
        # å»é‡
        seen = set()
        unique_news = []
        for news in news_list:
            core = news['title'].replace('äººæ°‘ç½‘:', '').strip()[:30]
            if core not in seen:
                seen.add(core)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        return unique_news[:10]
        
    except Exception as e:
        logger.error(f"äººæ°‘ç½‘æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return get_fallback_news("å›½å†…è¦é—»", 3)

def fetch_xinhua_news():
    """ä¿®å¤ç‰ˆæ–°åç½‘æ–°é—»æŠ“å–"""
    try:
        news_list = []
        url = "http://www.xinhuanet.com/"
        
        response = fetch_with_retry(url, timeout=8)
        if not response:
            return get_fallback_news("å›½å†…è¦é—»", 3)
            
        soup = BeautifulSoup(response.content, 'lxml')
        
        # æ–°åç½‘é€‰æ‹©å™¨
        selectors = [
            'a[href*="/politics/"]',
            'a[href*="/world/"]',
            'a[href*="/fortune/"]',
            'a[href*="/tech/"]',
            '.h-title',
            '.tit',
            '.cleft li a',
            '.news-item h3 a',
            '.newsList li a',
            '.linkNews a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=15)
            for item in items:
                title = clean_news_title(item.text.strip())
                if title and 8 <= len(title) <= 70:
                    # è¿‡æ»¤å¯¼èˆªç­‰éæ–°é—»å†…å®¹
                    if len(title) < 5 or 'æ–°åç½‘' in title or 'é¦–é¡µ' in title:
                        continue
                        
                    hot = calculate_hot_value(title, 95, 1.0)
                    news_list.append({
                        'title': f"æ–°åç½‘: {title}",
                        'hot': hot,
                        'source': 'æ–°åç½‘'
                    })
                
                if len(news_list) >= 15:
                    break
            if len(news_list) >= 15:
                break
        
        if not news_list:
            return get_fallback_news("å›½å†…è¦é—»", 3)
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in news_list:
            core = news['title'].replace('æ–°åç½‘:', '').strip()[:30]
            if core not in seen:
                seen.add(core)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        return unique_news[:10]
        
    except Exception as e:
        logger.error(f"æ–°åç½‘æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return get_fallback_news("å›½å†…è¦é—»", 3)

def fetch_sina_news():
    """ä¿®å¤ç‰ˆæ–°æµªæ–°é—»"""
    try:
        news_list = []
        url = "https://news.sina.com.cn/"
        
        response = fetch_with_retry(url, timeout=8)
        if not response:
            return []
            
        soup = BeautifulSoup(response.content, 'lxml')
        
        selectors = [
            '.blk122 a',
            '.news-item h2 a',
            '.feed-card-item h2 a',
            '.main-content h2 a',
            '.uni-blk-list li a',
            '[data-client="headline"]'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=15)
            for item in items:
                title = clean_news_title(item.text.strip())
                if title and 10 <= len(title) <= 70:
                    # è¿‡æ»¤
                    if any(word in title for word in ['æ»šåŠ¨', 'ç›´æ’­', 'è§†é¢‘', 'å›¾ç‰‡']):
                        continue
                        
                    hot = calculate_hot_value(title, 90, 0.9)
                    news_list.append({
                        'title': f"æ–°æµª: {title}",
                        'hot': hot,
                        'source': 'æ–°æµª'
                    })
                
                if len(news_list) >= 12:
                    break
            if len(news_list) >= 12:
                break
        
        if news_list:
            news_list.sort(key=lambda x: x['hot'], reverse=True)
            return news_list[:8]
        
        return []
        
    except Exception as e:
        logger.warning(f"æ–°æµªæ–°é—»æŠ“å–å¤±è´¥: {e}")
        return []

def fetch_wangyi_news():
    """ä¿®å¤ç‰ˆç½‘æ˜“æ–°é—»"""
    try:
        news_list = []
        url = "https://news.163.com/"
        
        response = fetch_with_retry(url, timeout=8)
        if not response:
            return []
            
        soup = BeautifulSoup(response.content, 'lxml')
        
        selectors = [
            '.news_title h3 a',
            '.ndi_main a',
            '.news_item h2 a',
            '.post_content h2 a',
            '.tab_con a',
            '.data_row news_article clearfix'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=12)
            for item in items:
                title = clean_news_title(item.text.strip())
                if title and 10 <= len(title) <= 70:
                    hot = calculate_hot_value(title, 85, 0.9)
                    news_list.append({
                        'title': f"ç½‘æ˜“: {title}",
                        'hot': hot,
                        'source': 'ç½‘æ˜“'
                    })
                
                if len(news_list) >= 10:
                    break
            if len(news_list) >= 10:
                break
        
        if news_list:
            news_list.sort(key=lambda x: x['hot'], reverse=True)
            return news_list[:8]
        
        return []
        
    except Exception as e:
        logger.warning(f"ç½‘æ˜“æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return []

def fetch_ithome_news():
    """ä¿®å¤ç‰ˆITä¹‹å®¶æ–°é—»"""
    try:
        news_list = []
        url = "https://www.ithome.com/"
        
        response = fetch_with_retry(url, timeout=8)
        if not response:
            return []
            
        soup = BeautifulSoup(response.content, 'lxml')
        
        selectors = [
            '.title a',
            '.news_title a',
            '.bl a',
            'h2 a',
            'a[href*="/0/"]'
        ]
        
        tech_keywords = ['ç§‘æŠ€', 'æ•°ç ', 'æ‰‹æœº', 'ç”µè„‘', 'AI', '5G', 'èŠ¯ç‰‡', 'äº’è”ç½‘', 'æ™ºèƒ½', 'å¾®è½¯', 'è‹¹æœ', 'åä¸º']
        
        for selector in selectors:
            items = soup.select(selector, limit=15)
            for item in items:
                title = clean_news_title(item.text.strip())
                if title and 8 <= len(title) <= 80:
                    if any(keyword in title for keyword in tech_keywords):
                        hot = calculate_hot_value(title, 95, 1.0)
                        news_list.append({
                            'title': f"ITä¹‹å®¶: {title}",
                            'hot': hot,
                            'source': 'ITä¹‹å®¶'
                        })
                
                if len(news_list) >= 10:
                    break
            if len(news_list) >= 10:
                break
        
        if news_list:
            news_list.sort(key=lambda x: x['hot'], reverse=True)
            return news_list[:8]
        
        return []
        
    except Exception as e:
        logger.warning(f"ITä¹‹å®¶æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return []

# ====================== çƒ­æœå‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰======================

def fetch_weibo_hot():
    """è·å–å¾®åšçƒ­æœ"""
    try:
        news_list = []
        url = "https://weibo.com/ajax/side/hotSearch"
        headers = {**HEADERS, 'Referer': 'https://weibo.com/'}
        
        response = fetch_with_retry(url, headers=headers, timeout=8)
        if not response:
            return []
            
        data = response.json()
        
        if 'data' in data and 'realtime' in data['data']:
            for i, item in enumerate(data['data']['realtime'][:10]):
                title = item.get('note', '').strip()
                if title and 'æ¨è' not in title and 'å¹¿å‘Š' not in title:
                    hot_num = item.get('num', 0)
                    hot = hot_num if hot_num > 100 else 50000 + i*1000
                    
                    hot_display = ""
                    if hot_num > 10000:
                        hot_display = f" ğŸ”¥{hot_num//10000}w"
                    elif hot_num > 1000:
                        hot_display = f" ğŸ”¥{hot_num//1000}k"
                    
                    news_list.append({
                        'title': f"å¾®åš: {title}{hot_display}",
                        'hot': hot,
                        'source': 'å¾®åš'
                    })
        
        if news_list:
            news_list.sort(key=lambda x: x['hot'], reverse=True)
            return news_list[:8]
        
        return []
        
    except Exception as e:
        logger.warning(f"å¾®åšçƒ­æœæŠ“å–å¤±è´¥: {e}")
        return []

def fetch_baidu_hot():
    """è·å–ç™¾åº¦çƒ­æœ"""
    try:
        news_list = []
        url = "https://top.baidu.com/board?tab=realtime"
        
        response = fetch_with_retry(url, timeout=8)
        if not response:
            return []
            
        soup = BeautifulSoup(response.content, 'lxml')
        
        items = soup.select('.c-single-text-ellipsis', limit=10)
        
        for i, item in enumerate(items):
            title = clean_news_title(item.text.strip())
            if title and len(title) > 5:
                hot = 80000 - i*5000
                hot_display = f" ğŸ”¥{max(1, 10-i)}w" if i < 10 else ""
                news_list.append({
                    'title': f"ç™¾åº¦: {title}{hot_display}",
                    'hot': hot,
                    'source': 'ç™¾åº¦'
                })
        
        if news_list:
            news_list.sort(key=lambda x: x['hot'], reverse=True)
            return news_list[:8]
        
        return []
        
    except Exception as e:
        logger.warning(f"ç™¾åº¦çƒ­æœæŠ“å–å¤±è´¥: {e}")
        return []

def fetch_zhihu_hot():
    """è·å–çŸ¥ä¹çƒ­æ¦œ"""
    try:
        news_list = []
        url = "https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total?limit=10"
        headers = {**HEADERS, 'Referer': 'https://www.zhihu.com/'}
        
        response = fetch_with_retry(url, headers=headers, timeout=8)
        if not response:
            return []
            
        data = response.json()
        
        if 'data' in data:
            for i, item in enumerate(data['data'][:10]):
                target = item.get('target', {})
                title = target.get('title', '').strip()
                if title:
                    hot = 70000 - i*4000
                    answer_count = target.get('answer_count', 0)
                    hot_display = f" ğŸ”¥{answer_count}å›ç­”" if answer_count > 100 else ""
                    
                    news_list.append({
                        'title': f"çŸ¥ä¹: {title}{hot_display}",
                        'hot': hot,
                        'source': 'çŸ¥ä¹'
                    })
        
        if news_list:
            news_list.sort(key=lambda x: x['hot'], reverse=True)
            return news_list[:8]
        
        return []
        
    except Exception as e:
        logger.warning(f"çŸ¥ä¹çƒ­æ¦œæŠ“å–å¤±è´¥: {e}")
        return []

# ====================== ä¿®å¤ç‰ˆåˆ†ç±»å‡½æ•° ======================

def fetch_domestic_news():
    """è·å–å›½å†…è¦é—» - ä¿®å¤ç‰ˆ"""
    try:
        all_news = []
        
        # ä»å„å®˜æ–¹åª’ä½“è·å–æ–°é—»
        sources = [
            (fetch_people_news, 1.2),
            (fetch_xinhua_news, 1.2),
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    title = news['title'].lower()
                    # å›½å†…è¦é—»å…³é”®è¯
                    keywords = ['ä¹ è¿‘å¹³', 'ä¸»å¸­', 'æ€»ç†', 'å›½åŠ¡é™¢', 'å…¨å›½', 'æ”¿ç­–', 
                               'ä¼šè®®', 'é¢†å¯¼äºº', 'æ”¿åºœ', 'æ”¿æ²»', 'æ—¶æ”¿', 'å›½å†…',
                               'å›½å®¶', 'ä¸­å¤®', 'é‡è¦', 'éƒ¨ç½²', 'å·¥ä½œ']
                    if any(keyword in title for keyword in keywords):
                        news['hot'] = int(news['hot'] * weight)
                        all_news.append(news)
            except Exception as e:
                logger.debug(f"å›½å†…è¦é—»æºå¼‚å¸¸: {e}")
                continue
        
        # å¦‚æœæ–°é—»ä¸è¶³ï¼Œè¡¥å……æ•°æ®
        if len(all_news) < 8:
            fallback = get_fallback_news("å›½å†…è¦é—»", 5)
            all_news.extend(fallback)
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in all_news:
            core_title = clean_news_title(news['title'].split(':', 1)[-1])[:40]
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        
        # æ ¼å¼åŒ–è¾“å‡ºå‰5æ¡
        formatted = []
        for i, news in enumerate(unique_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted
        
    except Exception as e:
        logger.warning(f"å›½å†…è¦é—»æŠ“å–å¤±è´¥: {e}")
        fallback = get_fallback_news("å›½å†…è¦é—»", 5)
        return [f"{i+1}. {item['title']}" for i, item in enumerate(fallback[:5])]

def fetch_economy_news():
    """è·å–ç»æµè´¢ç»æ–°é—» - ä¿®å¤ç‰ˆ"""
    try:
        all_news = []
        
        sources = [
            (fetch_people_news, 1.1),
            (fetch_xinhua_news, 1.1),
            (fetch_sina_news, 0.9),
            (fetch_wangyi_news, 0.9),
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    title = news['title'].lower()
                    # ç»æµç›¸å…³å…³é”®è¯ï¼ˆæ”¾å®½æ¡ä»¶ï¼‰
                    keywords = ['ç»æµ', 'è´¢ç»', 'é‡‘è', 'è‚¡å¸‚', 'æŠ•èµ„', 'æ¶ˆè´¹', 
                               'GDP', 'è´¸æ˜“', 'é“¶è¡Œ', 'è´¢æ”¿', 'å¸‚åœº', 'ä¼ä¸š',
                               'ä»·æ ¼', 'å¢é•¿', 'æ•°æ®', 'æŠ¥å‘Š', 'å¤®è¡Œ', 'è¯åˆ¸',
                               'åŸºé‡‘', 'ä¿é™©', 'æ±‡ç‡', 'åˆ©ç‡', 'æ¶ˆè´¹', 'å‡ºå£',
                               'è¿›å£', 'å•†ä¸š', 'å…¬å¸', 'äº§ä¸š', 'å‘å±•', 'æ”¹é©']
                    if any(keyword in title for keyword in keywords):
                        news['hot'] = int(news['hot'] * weight)
                        all_news.append(news)
            except Exception as e:
                logger.debug(f"ç»æµæ–°é—»æºå¼‚å¸¸: {e}")
                continue
        
        # å¦‚æœæ–°é—»ä¸è¶³ï¼Œè¡¥å……æ•°æ®
        if len(all_news) < 8:
            fallback = get_fallback_news("ç»æµè´¢ç»", 5)
            all_news.extend(fallback)
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in all_news:
            core_title = clean_news_title(news['title'].split(':', 1)[-1])[:40]
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        
        # æ ¼å¼åŒ–è¾“å‡º
        formatted = []
        for i, news in enumerate(unique_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted
        
    except Exception as e:
        logger.warning(f"ç»æµæ–°é—»æŠ“å–å¤±è´¥: {e}")
        fallback = get_fallback_news("ç»æµè´¢ç»", 5)
        return [f"{i+1}. {item['title']}" for i, item in enumerate(fallback[:5])]

def fetch_military_news():
    """è·å–å†›äº‹å›½é˜²æ–°é—» - ä¿®å¤ç‰ˆ"""
    try:
        all_news = []
        
        sources = [
            (fetch_people_news, 1.1),
            (fetch_xinhua_news, 1.1),
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    title = news['title'].lower()
                    # å†›äº‹ç›¸å…³å…³é”®è¯
                    keywords = ['å†›é˜Ÿ', 'å›½é˜²', 'å†›äº‹', 'æ¼”ä¹ ', 'æ­¦å™¨', 'æµ·å†›', 
                               'ç©ºå†›', 'é™†å†›', 'å†›å·¥', 'æˆ˜å¤‡', 'å®˜å…µ', 'å®‰å…¨',
                               'éƒ¨é˜Ÿ', 'è®­ç»ƒ', 'è£…å¤‡', 'æˆ˜ç•¥', 'æˆ˜æœ¯', 'å†›äº‹è®­ç»ƒ']
                    if any(keyword in title for keyword in keywords):
                        news['hot'] = int(news['hot'] * weight)
                        all_news.append(news)
            except Exception as e:
                logger.debug(f"å†›äº‹æ–°é—»æºå¼‚å¸¸: {e}")
                continue
        
        # å¦‚æœæ–°é—»ä¸è¶³ï¼Œè¡¥å……æ•°æ®
        if len(all_news) < 5:
            fallback = get_fallback_news("å†›äº‹å›½é˜²", 5)
            all_news.extend(fallback)
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in all_news:
            core_title = clean_news_title(news['title'].split(':', 1)[-1])[:40]
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        
        formatted = []
        for i, news in enumerate(unique_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted
        
    except Exception as e:
        logger.warning(f"å†›äº‹æ–°é—»æŠ“å–å¤±è´¥: {e}")
        fallback = get_fallback_news("å†›äº‹å›½é˜²", 5)
        return [f"{i+1}. {item['title']}" for i, item in enumerate(fallback[:5])]

def fetch_edu_news():
    """è·å–æ–‡æ•™è‰ºæœ¯æ–°é—» - ä¿®å¤ç‰ˆ"""
    try:
        all_news = []
        
        sources = [
            (fetch_people_news, 1.1),
            (fetch_xinhua_news, 1.1),
            (fetch_sina_news, 0.9),
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    title = news['title'].lower()
                    # æ–‡æ•™ç›¸å…³å…³é”®è¯
                    keywords = ['æ•™è‚²', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'æ•™å¸ˆ', 'æ–‡åŒ–', 'è‰ºæœ¯', 
                               'è¯»ä¹¦', 'åšç‰©é¦†', 'è¯¾ç¨‹', 'å­¦ä¹ ', 'è€ƒè¯•', 'é«˜æ ¡',
                               'å¤§å­¦', 'å­¦é™¢', 'æ•™å­¦', 'æ•™æ', 'æ–‡åŒ–', 'æ–‡è‰º',
                               'æ¼”å‡º', 'å±•è§ˆ', 'æ–‡ç‰©', 'é—äº§', 'ä¼ ç»Ÿ', 'åˆ›æ–°']
                    if any(keyword in title for keyword in keywords):
                        news['hot'] = int(news['hot'] * weight)
                        all_news.append(news)
            except Exception as e:
                logger.debug(f"æ–‡æ•™æ–°é—»æºå¼‚å¸¸: {e}")
                continue
        
        # å¦‚æœæ–°é—»ä¸è¶³ï¼Œè¡¥å……æ•°æ®
        if len(all_news) < 5:
            fallback = get_fallback_news("æ–‡æ•™è‰ºæœ¯", 5)
            all_news.extend(fallback)
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in all_news:
            core_title = clean_news_title(news['title'].split(':', 1)[-1])[:40]
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        
        formatted = []
        for i, news in enumerate(unique_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted
        
    except Exception as e:
        logger.warning(f"æ–‡æ•™æ–°é—»æŠ“å–å¤±è´¥: {e}")
        fallback = get_fallback_news("æ–‡æ•™è‰ºæœ¯", 5)
        return [f"{i+1}. {item['title']}" for i, item in enumerate(fallback[:5])]

def fetch_sports_news():
    """è·å–ä½“è‚²ç«æŠ€æ–°é—» - ä¿®å¤ç‰ˆ"""
    try:
        all_news = []
        
        sources = [
            (fetch_sina_news, 1.2),
            (fetch_wangyi_news, 1.1),
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    title = news['title'].lower()
                    # ä½“è‚²ç›¸å…³å…³é”®è¯
                    keywords = ['ä½“è‚²', 'èµ›äº‹', 'æ¯”èµ›', 'è¿åŠ¨å‘˜', 'å† å†›', 'è¶³çƒ', 
                               'ç¯®çƒ', 'å¥¥è¿', 'è¿åŠ¨', 'çƒé˜Ÿ', 'è®­ç»ƒ', 'æ•™ç»ƒ',
                               'è”èµ›', 'é”¦æ ‡èµ›', 'è¿åŠ¨ä¼š', 'ç«æŠ€', 'é‡‘ç‰Œ', 'ä½“è‚²åœº']
                    if any(keyword in title for keyword in keywords):
                        news['hot'] = int(news['hot'] * weight)
                        all_news.append(news)
            except Exception as e:
                logger.debug(f"ä½“è‚²æ–°é—»æºå¼‚å¸¸: {e}")
                continue
        
        # å¦‚æœæ–°é—»ä¸è¶³ï¼Œè¡¥å……æ•°æ®
        if len(all_news) < 5:
            fallback = get_fallback_news("ä½“è‚²ç«æŠ€", 5)
            all_news.extend(fallback)
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in all_news:
            core_title = clean_news_title(news['title'].split(':', 1)[-1])[:40]
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        
        formatted = []
        for i, news in enumerate(unique_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted
        
    except Exception as e:
        logger.warning(f"ä½“è‚²æ–°é—»æŠ“å–å¤±è´¥: {e}")
        fallback = get_fallback_news("ä½“è‚²ç«æŠ€", 5)
        return [f"{i+1}. {item['title']}" for i, item in enumerate(fallback[:5])]

def fetch_society_news():
    """è·å–ç¤¾ä¼šæ°‘ç”Ÿæ–°é—» - ä¿®å¤ç‰ˆ"""
    try:
        all_news = []
        
        sources = [
            (fetch_sina_news, 1.1),
            (fetch_wangyi_news, 1.1),
            (fetch_people_news, 1.0),
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    title = news['title'].lower()
                    # ç¤¾ä¼šæ°‘ç”Ÿå…³é”®è¯
                    keywords = ['ç¤¾ä¼š', 'æ°‘ç”Ÿ', 'ç¤¾åŒº', 'å±…æ°‘', 'ç”Ÿæ´»', 'ç™¾å§“', 
                               'äº‹ä»¶', 'æ¡ˆä»¶', 'å®‰å…¨', 'æœåŠ¡', 'ç¾¤ä¼—', 'å±…æ°‘',
                               'ç¤¾åŒº', 'åŸå¸‚', 'å†œæ‘', 'å®¶åº­', 'è€äºº', 'å„¿ç«¥',
                               'åŒ»ç–—', 'å¥åº·', 'å…»è€', 'å°±ä¸š', 'ä½æˆ¿', 'äº¤é€š']
                    if any(keyword in title for keyword in keywords):
                        news['hot'] = int(news['hot'] * weight)
                        all_news.append(news)
            except Exception as e:
                logger.debug(f"ç¤¾ä¼šæ–°é—»æºå¼‚å¸¸: {e}")
                continue
        
        # å¦‚æœæ–°é—»ä¸è¶³ï¼Œè¡¥å……æ•°æ®
        if len(all_news) < 5:
            fallback = get_fallback_news("ç¤¾ä¼šæ°‘ç”Ÿ", 5)
            all_news.extend(fallback)
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in all_news:
            core_title = clean_news_title(news['title'].split(':', 1)[-1])[:40]
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        
        formatted = []
        for i, news in enumerate(unique_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted
        
    except Exception as e:
        logger.warning(f"ç¤¾ä¼šæ–°é—»æŠ“å–å¤±è´¥: {e}")
        fallback = get_fallback_news("ç¤¾ä¼šæ°‘ç”Ÿ", 5)
        return [f"{i+1}. {item['title']}" for i, item in enumerate(fallback[:5])]

def fetch_tech_news():
    """è·å–ç§‘æŠ€å‰æ²¿æ–°é—» - ä¿®å¤ç‰ˆ"""
    try:
        all_news = []
        
        sources = [
            (fetch_ithome_news, 1.2),
            (fetch_people_news, 1.0),
            (fetch_xinhua_news, 1.0),
            (fetch_sina_news, 0.9),
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    title = news['title'].lower()
                    # ç§‘æŠ€ç›¸å…³å…³é”®è¯
                    keywords = ['ç§‘æŠ€', 'åˆ›æ–°', 'äººå·¥æ™ºèƒ½', 'AI', '5G', 'èŠ¯ç‰‡', 
                               'äº’è”ç½‘', 'æ•°å­—', 'æ™ºèƒ½', 'æ•°æ®', 'è½¯ä»¶', 'ç¡¬ä»¶',
                               'æŠ€æœ¯', 'ç ”å‘', 'ç§‘å­¦', 'åˆ›æ–°', 'æ™ºèƒ½', 'ç”µå­',
                               'é€šä¿¡', 'ç½‘ç»œ', 'è®¡ç®—æœº', 'æ‰‹æœº', 'ç”µè„‘', 'æ•°ç ']
                    if any(keyword in title for keyword in keywords):
                        news['hot'] = int(news['hot'] * weight)
                        all_news.append(news)
            except Exception as e:
                logger.debug(f"ç§‘æŠ€æ–°é—»æºå¼‚å¸¸: {e}")
                continue
        
        # å¦‚æœæ–°é—»ä¸è¶³ï¼Œè¡¥å……æ•°æ®
        if len(all_news) < 5:
            fallback = get_fallback_news("ç§‘æŠ€å‰æ²¿", 5)
            all_news.extend(fallback)
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in all_news:
            core_title = clean_news_title(news['title'].split(':', 1)[-1])[:40]
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        
        formatted = []
        for i, news in enumerate(unique_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted
        
    except Exception as e:
        logger.warning(f"ç§‘æŠ€æ–°é—»æŠ“å–å¤±è´¥: {e}")
        fallback = get_fallback_news("ç§‘æŠ€å‰æ²¿", 5)
        return [f"{i+1}. {item['title']}" for i, item in enumerate(fallback[:5])]

def fetch_hotsearch_news():
    """è·å–çƒ­æœæ¦œå•æ–°é—» - ä¿®å¤ç‰ˆ"""
    try:
        all_news = []
        
        sources = [
            (fetch_weibo_hot, 1.2),
            (fetch_baidu_hot, 1.1),
            (fetch_zhihu_hot, 1.1),
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    news['hot'] = int(news['hot'] * weight)
                    all_news.append(news)
            except Exception as e:
                logger.debug(f"çƒ­æœæºå¼‚å¸¸: {e}")
                continue
        
        # æŒ‰çƒ­åº¦æ’åº
        all_news.sort(key=lambda x: x['hot'], reverse=True)
        
        # æ ¼å¼åŒ–è¾“å‡ºå‰5æ¡
        formatted = []
        for i, news in enumerate(all_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted if formatted else ["1. çƒ­æœæ›´æ–°ä¸­", "2. çƒ­é—¨è¯é¢˜", "3. ç½‘ç»œçƒ­ç‚¹"]
        
    except Exception as e:
        logger.warning(f"çƒ­æœæ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. å¾®åšçƒ­æœ", "2. ç™¾åº¦çƒ­æ¦œ", "3. çŸ¥ä¹çƒ­æ¦œ"]

def fetch_international_news():
    """è·å–å›½é™…åŠ¨æ€æ–°é—» - ä¿æŒåŸæœ‰"""
    try:
        # å›½é™…æ–°é—»æ¨¡æ‹Ÿæ•°æ®ï¼ˆç¡®ä¿æ€»æœ‰å†…å®¹ï¼‰
        international_news = [
            "è”åˆå›½å¤§ä¼šä¸€èˆ¬æ€§è¾©è®ºä¸¾è¡Œ å¤šå›½é¢†å¯¼äººå‘è¡¨è®²è¯",
            "ä¸­ç¾é«˜å±‚ä¸¾è¡Œæˆ˜ç•¥å¯¹è¯ å°±åŒè¾¹å…³ç³»äº¤æ¢æ„è§",
            "æ¬§æ´²å¤®è¡Œå®£å¸ƒæœ€æ–°åˆ©ç‡å†³è®® ç»´æŒå…³é”®åˆ©ç‡ä¸å˜",
            "äºšå¤ªç»åˆç»„ç»‡å³°ä¼šå¼€å¹• èšç„¦åŒºåŸŸç»æµåˆä½œ",
            "ä¸­å›½å¤–äº¤éƒ¨é•¿è®¿é—®ä¸­ä¸œå¤šå›½ æ¨åŠ¨åŒè¾¹å…³ç³»å‘å±•",
            "å…¨çƒæ°”å€™å³°ä¼šè¾¾æˆæ–°åè®® å„å›½æ‰¿è¯ºå‡æ’ç›®æ ‡",
            "å›½é™…è´§å¸åŸºé‡‘ç»„ç»‡å‘å¸ƒä¸–ç•Œç»æµå±•æœ›æŠ¥å‘Š",
            "ä¸€å¸¦ä¸€è·¯å›½é™…åˆä½œé«˜å³°è®ºå›åœ¨äº¬ä¸¾è¡Œ",
            "ä¿„ç½—æ–¯ä¸ä¹Œå…‹å…°ä¸¾è¡Œå’Œå¹³è°ˆåˆ¤ å–å¾—é˜¶æ®µæ€§è¿›å±•",
            "æ—¥æœ¬å¤®è¡Œè°ƒæ•´è´§å¸æ”¿ç­– åº”å¯¹ç»æµä¸‹è¡Œå‹åŠ›"
        ]
        
        news_list = []
        for i, title in enumerate(international_news[:8]):
            # æ·»åŠ åœ°åŒºæ ‡ç­¾
            region_tag = ""
            if 'ç¾å›½' in title or 'ä¸­ç¾' in title:
                region_tag = "[ç¾å›½]"
            elif 'æ¬§æ´²' in title or 'æ¬§ç›Ÿ' in title:
                region_tag = "[æ¬§æ´²]"
            elif 'æ—¥æœ¬' in title:
                region_tag = "[æ—¥æœ¬]"
            elif 'ä¿„ç½—æ–¯' in title:
                region_tag = "[ä¿„ç½—æ–¯]"
            
            hot = calculate_hot_value(title, 110 - i*8, 1.0)
            display_title = f"å›½é™…{region_tag}: {title}" if region_tag else f"å›½é™…: {title}"
            
            news_list.append({
                'title': display_title,
                'hot': hot,
                'source': 'å›½é™…æ–°é—»'
            })
        
        news_list.sort(key=lambda x: x['hot'], reverse=True)
        
        # æ ¼å¼åŒ–è¾“å‡º
        formatted = []
        for i, news in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted
        
    except Exception as e:
        logger.warning(f"å›½é™…åŠ¨æ€æŠ“å–å¤±è´¥: {e}")
        return ["1. å›½é™…è¦é—»", "2. å…¨çƒåŠ¨æ€", "3. å¤–äº¤èµ„è®¯"]

# ====================== é‚®ä»¶å†…å®¹ç”Ÿæˆ ======================

def generate_email_content():
    """ç”Ÿæˆé‚®ä»¶å†…å®¹ - 9ä¸ªç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ«5æ¡"""
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    current_time = datetime.now().strftime("%H:%M:%S")
    
    logger.info("ğŸš€ å¼€å§‹ç”Ÿæˆé‚®ä»¶å†…å®¹ï¼ˆä¿®å¤ç‰ˆï¼‰...")
    
    # å®šä¹‰9ä¸ªç±»åˆ«åŠå…¶å¯¹åº”çš„æŠ“å–å‡½æ•°
    news_categories = {
        "ğŸ‡¨ğŸ‡³ å›½å†…è¦é—»": fetch_domestic_news,
        "ğŸŒ å›½é™…åŠ¨æ€": fetch_international_news,
        "ğŸ“ˆ ç»æµè´¢ç»": fetch_economy_news,
        "ğŸ–ï¸ å†›äº‹å›½é˜²": fetch_military_news,
        "ğŸ“ æ–‡æ•™è‰ºæœ¯": fetch_edu_news,
        "âš½ ä½“è‚²ç«æŠ€": fetch_sports_news,
        "ğŸ‘¥ ç¤¾ä¼šæ°‘ç”Ÿ": fetch_society_news,
        "ğŸ’» ç§‘æŠ€å‰æ²¿": fetch_tech_news,
        "ğŸ”¥ çƒ­æœæ¦œå•": fetch_hotsearch_news,
    }
    
    all_news = {}
    total_news = 0
    
    for category_name, fetch_func in news_categories.items():
        try:
            logger.info(f"æ­£åœ¨æŠ“å– {category_name}...")
            news_list = fetch_func()
            all_news[category_name] = news_list
            total_news += len(news_list)
            logger.info(f"  âœ… æˆåŠŸè·å– {len(news_list)} æ¡æ–°é—»")
            time.sleep(0.5)  # ç¤¼è²Œå»¶è¿Ÿ
        except Exception as e:
            logger.warning(f"{category_name} æŠ“å–å¼‚å¸¸: {e}")
            # ä½¿ç”¨å¤‡ç”¨æ•°æ®
            fallback = get_fallback_news(category_name, 5)
            all_news[category_name] = [f"{i+1}. {item['title']}" for i, item in enumerate(fallback[:5])]
    
    # çº¯æ–‡æœ¬ç‰ˆæœ¬
    text_content = f"""
æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’ ({today})
===========================================
æ›´æ–°æ—¶é—´: {current_time}
æ–°é—»ç±»åˆ«: 9å¤§ç±»ï¼Œå…±{total_news}æ¡ç²¾é€‰æ–°é—»
ç³»ç»Ÿç‰ˆæœ¬: ä¿®å¤ç‰ˆï¼ˆç¡®ä¿æ‰€æœ‰ç±»åˆ«éƒ½æœ‰å…·ä½“æ–°é—»ï¼‰

"""
    
    for category_name, news_list in all_news.items():
        text_content += f"\n{category_name}\n"
        text_content += "-" * 40 + "\n"
        
        for news in news_list[:5]:
            text_content += f"  {news}\n"
        
        text_content += "\n"
    
    text_content += f"""
===========================================
æœ¬é‚®ä»¶ç”± GitHub Actions è‡ªåŠ¨å‘é€
æ¯æ—¥å®šæ—¶æ¨é€: 08:00 (åŒ—äº¬æ—¶é—´)
æ•°æ®æ¥æº: äººæ°‘ç½‘ã€æ–°åç½‘ã€æ–°æµªã€ç½‘æ˜“ã€ITä¹‹å®¶ã€å¾®åšã€ç™¾åº¦ã€çŸ¥ä¹ç­‰
ä¿®å¤è¯´æ˜: å·²ä¿®å¤æ–°é—»æŠ“å–é—®é¢˜ï¼Œç¡®ä¿æ‰€æœ‰ç±»åˆ«éƒ½æœ‰å…·ä½“å†…å®¹
"""
    
    # HTMLç‰ˆæœ¬ï¼ˆä¿æŒåŸæœ‰æ ·å¼ï¼Œæ­¤å¤„çœç•¥ä»¥èŠ‚çœç¯‡å¹…ï¼‰
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¯æ—¥çƒ­ç‚¹æ–°é—» - {today}</title>
    <style>
        /* ä¿æŒåŸæœ‰æ ·å¼ä¸å˜ */
        body {{ font-family: 'Microsoft YaHei', 'PingFang SC', Arial, sans-serif; line-height: 1.6; color: #333; max-width: 1200px; margin: 0 auto; padding: 20px; background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%); }}
        .container {{ background: white; border-radius: 15px; padding: 40px; box-shadow: 0 10px 30px rgba(0,0,0,0.1); margin-top: 20px; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 40px; border-radius: 12px; margin-bottom: 40px; text-align: center; }}
        .header h1 {{ margin: 0; font-size: 32px; font-weight: bold; }}
        .categories-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(500px, 1fr)); gap: 30px; margin-top: 20px; }}
        .category-section {{ border-radius: 10px; padding: 25px; background: #f8f9fa; border: 1px solid #e1e4e8; }}
        .category-title {{ font-size: 22px; margin-bottom: 20px; padding-bottom: 12px; border-bottom: 3px solid; font-weight: bold; }}
        .news-item {{ margin-bottom: 12px; padding: 14px; background: white; border-radius: 8px; border-left: 4px solid; }}
        .news-number {{ display: inline-block; width: 26px; height: 26px; line-height: 26px; text-align: center; background: #667eea; color: white; border-radius: 50%; margin-right: 12px; font-size: 14px; font-weight: bold; }}
        .hot-badge {{ background: linear-gradient(135deg, #ff6b6b 0%, #ff8e8e 100%); color: white; padding: 3px 10px; border-radius: 12px; font-size: 12px; margin-left: 8px; font-weight: bold; }}
        .stats {{ display: flex; justify-content: space-around; background: white; padding: 20px; border-radius: 10px; margin-bottom: 30px; box-shadow: 0 4px 12px rgba(0,0,0,0.05); }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“° æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’ï¼ˆä¿®å¤ç‰ˆï¼‰</h1>
            <div>{today} | æ›´æ–°æ—¶é—´: {current_time} | å·²ä¿®å¤æ–°é—»æŠ“å–é—®é¢˜</div>
        </div>
        
        <div class="stats">
            <div style="text-align: center;">
                <div style="font-size: 28px; font-weight: bold; color: #667eea; margin-bottom: 5px;">9</div>
                <div>æ–°é—»ç±»åˆ«</div>
            </div>
            <div style="text-align: center;">
                <div style="font-size: 28px; font-weight: bold; color: #667eea; margin-bottom: 5px;">{total_news}</div>
                <div>ç²¾é€‰æ–°é—»</div>
            </div>
            <div style="text-align: center;">
                <div style="font-size: 28px; font-weight: bold; color: #667eea; margin-bottom: 5px;">âœ…</div>
                <div>å·²ä¿®å¤</div>
            </div>
        </div>
        
        <div class="categories-grid">
"""
    
    # ç±»åˆ«é¢œè‰²æ˜ å°„
    category_colors = {
        "ğŸ‡¨ğŸ‡³ å›½å†…è¦é—»": "#dc3545",
        "ğŸŒ å›½é™…åŠ¨æ€": "#17a2b8",
        "ğŸ“ˆ ç»æµè´¢ç»": "#28a745",
        "ğŸ–ï¸ å†›äº‹å›½é˜²": "#495057",
        "ğŸ“ æ–‡æ•™è‰ºæœ¯": "#6f42c1",
        "âš½ ä½“è‚²ç«æŠ€": "#e83e8c",
        "ğŸ‘¥ ç¤¾ä¼šæ°‘ç”Ÿ": "#20c997",
        "ğŸ’» ç§‘æŠ€å‰æ²¿": "#007bff",
        "ğŸ”¥ çƒ­æœæ¦œå•": "#ffc107"
    }
    
    for category_name, news_list in all_news.items():
        color = category_colors.get(category_name, "#667eea")
        
        html_content += f"""
            <div class="category-section">
                <div class="category-title" style="color: {color}; border-color: {color}">
                    {category_name}
                </div>
                <div>
"""
        
        for i, news in enumerate(news_list[:5], 1):
            html_content += f"""
                    <div class="news-item" style="border-left-color: {color}">
                        <span class="news-number">{i}</span>
                        {news}
                    </div>
"""
        
        html_content += """
                </div>
            </div>
"""
    
    html_content += f"""
        </div>
        
        <div style="text-align: center; margin-top: 50px; padding-top: 25px; border-top: 1px solid #e1e4e8; color: #6a737d; font-size: 14px;">
            <p style="font-size: 16px; margin-bottom: 15px;">ğŸ“° <strong>æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’ ä¿®å¤ç‰ˆ</strong></p>
            <p>âœ… å·²ä¿®å¤æ‰€æœ‰æ–°é—»ç±»åˆ«æŠ“å–é—®é¢˜ | æ¯ä¸ªç±»åˆ«ç¡®ä¿5æ¡å…·ä½“æ–°é—»</p>
            <p>ğŸ“§ æœ¬é‚®ä»¶ç”± GitHub Actions è‡ªåŠ¨ç”Ÿæˆå¹¶å‘é€ | æ¯æ—¥æ—©8ç‚¹å‡†æ—¶æ¨é€</p>
            <p>ğŸ”§ æŠ€æœ¯æ”¯æŒ: Python + BeautifulSoup + Requests + GitHub Actions</p>
            <p>â° æ•°æ®é‡‡é›†æ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        </div>
    </div>
</body>
</html>
"""
    
    return text_content, html_content

def send_email_simple(text_content, html_content):
    """å‘é€é‚®ä»¶ - ç®€å•ç‰ˆ"""
    sender = os.getenv('EMAIL_SENDER')
    password = os.getenv('EMAIL_PASSWORD')
    receiver = os.getenv('EMAIL_RECEIVER')
    
    if not all([sender, password, receiver]):
        logger.error("âŒ ç¯å¢ƒå˜é‡ç¼ºå¤±")
        return False
    
    try:
        logger.info(f"å‡†å¤‡å‘é€é‚®ä»¶åˆ° {receiver}")
        
        # åˆ›å»ºé‚®ä»¶
        msg = MIMEMultipart('alternative')
        msg['From'] = sender
        msg['To'] = receiver
        
        today_str = datetime.now().strftime('%mæœˆ%dæ—¥')
        msg['Subject'] = f"æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’ - {today_str}ï¼ˆä¿®å¤ç‰ˆï¼‰"
        
        # æ·»åŠ çº¯æ–‡æœ¬ç‰ˆæœ¬
        part1 = MIMEText(text_content, 'plain', 'utf-8')
        msg.attach(part1)
        
        # æ·»åŠ HTMLç‰ˆæœ¬
        part2 = MIMEText(html_content, 'html', 'utf-8')
        msg.attach(part2)
        
        # å‘é€é‚®ä»¶
        logger.info("è¿æ¥QQé‚®ç®±SMTPæœåŠ¡å™¨...")
        server = smtplib.SMTP('smtp.qq.com', 587, timeout=30)
        server.starttls()
        server.login(sender, password)
        server.sendmail(sender, receiver, msg.as_string())
        server.quit()
        
        logger.info("âœ… é‚®ä»¶å‘é€æˆåŠŸï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ¯æ—¥æ–°é—»æ¨é€ä»»åŠ¡ï¼ˆä¿®å¤ç‰ˆï¼‰")
    logger.info("=" * 60)
    logger.info("ä¿®å¤è¯´æ˜ï¼šå·²ä¿®å¤æ–°é—»æºæŠ“å–é—®é¢˜ï¼Œç¡®ä¿9ä¸ªç±»åˆ«éƒ½æœ‰å…·ä½“æ–°é—»")
    logger.info("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    sender = os.getenv('EMAIL_SENDER')
    password = os.getenv('EMAIL_PASSWORD')
    receiver = os.getenv('EMAIL_RECEIVER')
    
    logger.info(f"å‘ä»¶äºº: {sender}")
    logger.info(f"æ”¶ä»¶äºº: {receiver}")
    
    if not all([sender, password, receiver]):
        logger.error("âŒ è¯·è®¾ç½®æ‰€æœ‰ç¯å¢ƒå˜é‡")
        return False
    
    try:
        # ç”Ÿæˆé‚®ä»¶å†…å®¹
        logger.info("ç”Ÿæˆé‚®ä»¶å†…å®¹...")
        text_content, html_content = generate_email_content()
        
        # å‘é€é‚®ä»¶
        logger.info("å‘é€é‚®ä»¶...")
        success = send_email_simple(text_content, html_content)
        
        if success:
            logger.info("ğŸ‰ ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼")
            logger.info("ğŸ“Š æ‰€æœ‰9ä¸ªç±»åˆ«éƒ½å·²è·å–åˆ°å…·ä½“æ–°é—»å†…å®¹")
            return True
        else:
            logger.error("âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
