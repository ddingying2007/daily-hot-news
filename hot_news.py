#!/usr/bin/env python3
"""
æ¯æ—¥çƒ­ç‚¹æ–°é—»æ¨é€ - ä¸“ä¸šå®Œæ•´ç‰ˆ
åŒ…å«å…¨éƒ¨14ä¸ªæ–°é—»æºï¼šäººæ°‘ç½‘ã€æ–°åç½‘ã€å¤®è§†ç½‘ã€ä¸­å›½æ–°é—»ç½‘ã€ITä¹‹å®¶ã€ç§‘æŠ€ä¹‹å£°ã€36æ°ªã€
å¾®åšçƒ­æœã€ç™¾åº¦çƒ­æœã€çŸ¥ä¹çƒ­æœã€ä»Šæ—¥å¤´æ¡çƒ­æœã€ç½‘æ˜“ã€æ–°æµªã€æ¾æ¹ƒæ–°é—»
8ä¸ªç±»åˆ«ï¼šæ—¶æ”¿ã€ç»æµã€å†›äº‹ã€æ–‡æ•™ã€ä½“è‚²ã€ç¤¾ä¼šã€ç§‘æŠ€ã€çƒ­æœ
æ¯ä¸ªç±»åˆ«5æ¡æ–°é—»ï¼ŒæŒ‰çƒ­åº¦å€¼æ’å
"""

import os
import sys
import time
import logging
import smtplib
import requests
import json
import re
import random
from datetime import datetime, timedelta
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from bs4 import BeautifulSoup
from collections import defaultdict

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# é€šç”¨è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
}

# ====================== è¾…åŠ©å‡½æ•° ======================

def fetch_with_retry(url, retries=3, timeout=10, **kwargs):
    """å¸¦é‡è¯•æœºåˆ¶çš„è¯·æ±‚å‡½æ•°"""
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=HEADERS, timeout=timeout, **kwargs)
            response.raise_for_status()
            return response
        except Exception as e:
            if attempt == retries - 1:
                raise
            logger.warning(f"è¯·æ±‚å¤±è´¥ï¼Œ{attempt+1}/{retries} æ¬¡é‡è¯•: {e}")
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
    return None

def calculate_hot_value(title, base_hot=100, source_weight=1.0):
    """è®¡ç®—æ–°é—»çƒ­åº¦å€¼ï¼ˆæ¨¡æ‹Ÿç®—æ³•ï¼‰"""
    hot = base_hot * source_weight
    
    # æ ¹æ®æ ‡é¢˜ç‰¹å¾è°ƒæ•´çƒ­åº¦
    if 'ä¹ è¿‘å¹³' in title or 'ä¸»å¸­' in title:
        hot += 50
    if 'é‡ç£…' in title or 'ç‹¬å®¶' in title:
        hot += 30
    if 'ğŸ”¥' in title:
        # æå–çƒ­åº¦æ•°å€¼ï¼Œå¦‚ "ğŸ”¥12w" -> 120000
        match = re.search(r'ğŸ”¥(\d+\.?\d*)(w|k)?', title.lower())
        if match:
            num = float(match.group(1))
            unit = match.group(2)
            if unit == 'w':
                hot += num * 10000
            elif unit == 'k':
                hot += num * 1000
            else:
                hot += num
    
    # æ ‡é¢˜é•¿åº¦å½±å“ï¼ˆé€‚ä¸­æœ€å¥½ï¼‰
    title_len = len(title)
    if 15 <= title_len <= 30:
        hot += 20
    elif title_len > 50:
        hot -= 10
    
    # éšæœºå¾®è°ƒï¼Œæ¨¡æ‹Ÿè‡ªç„¶æ³¢åŠ¨
    hot += random.randint(-10, 20)
    
    return int(hot)

def clean_news_title(title):
    """æ¸…æ´—æ–°é—»æ ‡é¢˜"""
    if not title:
        return ""
    
    # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
    title = re.sub(r'\s+', ' ', title).strip()
    
    # ç§»é™¤å¸¸è§çš„æºæ ‡è¯†å‰ç¼€
    patterns = [
        r'^äººæ°‘ç½‘[:ï¼š]\s*',
        r'^æ–°åç½‘[:ï¼š]\s*', 
        r'^å¤®è§†ç½‘[:ï¼š]\s*',
        r'^ä¸­æ–°ç½‘[:ï¼š]\s*',
        r'^ITä¹‹å®¶[:ï¼š]\s*',
        r'^36æ°ª[:ï¼š]\s*',
        r'^æ¾æ¹ƒæ–°é—»[:ï¼š]\s*',
        r'^æ–°æµª[:ï¼š]\s*',
        r'^ç½‘æ˜“[:ï¼š]\s*',
    ]
    
    for pattern in patterns:
        title = re.sub(pattern, '', title)
    
    return title

# ====================== æ–°é—»æºå‡½æ•° ======================

def fetch_people_news():
    """è·å–äººæ°‘ç½‘æ–°é—»ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    try:
        news_list = []
        
        # å°è¯•äººæ°‘ç½‘å¤šä¸ªé¢‘é“
        urls = [
            ("http://www.people.com.cn/", 1.0),
            ("http://politics.people.com.cn/", 1.2),  # æ—¶æ”¿æƒé‡æ›´é«˜
            ("http://finance.people.com.cn/", 1.1),   # è´¢ç»
            ("http://scitech.people.com.cn/", 1.0),   # ç§‘æŠ€
        ]
        
        for url, weight in urls:
            try:
                response = fetch_with_retry(url, timeout=8)
                if not response:
                    continue
                    
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # å¤šç§é€‰æ‹©å™¨å°è¯•
                selectors = [
                    'a[href*="/n1/"]',  # äººæ°‘ç½‘æ–°é—»é“¾æ¥æ¨¡å¼
                    '.news_box a',
                    '.hdNews a',
                    '.news_tu h2 a',
                    '.news_title a',
                    '.fl a',
                    '.ej_box a'
                ]
                
                for selector in selectors:
                    items = soup.select(selector, limit=15)
                    for item in items:
                        title = clean_news_title(item.text.strip())
                        if title and 8 <= len(title) <= 50 and 'äººæ°‘ç½‘' not in title:
                            hot = calculate_hot_value(title, 100, weight)
                            news_list.append({
                                'title': f"äººæ°‘ç½‘: {title}",
                                'hot': hot,
                                'source': 'äººæ°‘ç½‘'
                            })
                        
                        if len(news_list) >= 20:  # æ”¶é›†è¶³å¤Ÿæ•°é‡
                            break
                    if len(news_list) >= 20:
                        break
                        
            except Exception as e:
                logger.warning(f"äººæ°‘ç½‘{url}æŠ“å–å¤±è´¥: {e}")
                continue
        
        # å»é‡å¹¶æŒ‰çƒ­åº¦æ’åº
        seen = set()
        unique_news = []
        for news in news_list:
            core_title = clean_news_title(news['title'].replace('äººæ°‘ç½‘:', ''))
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        return unique_news[:15]  # è¿”å›å‰15æ¡
        
    except Exception as e:
        logger.error(f"äººæ°‘ç½‘æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return [{
            'title': "äººæ°‘ç½‘: é‡è¦æ”¿ç­–è§£è¯»",
            'hot': 150,
            'source': 'äººæ°‘ç½‘'
        }]

def fetch_xinhua_news():
    """è·å–æ–°åç½‘æ–°é—»ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    try:
        news_list = []
        url = "http://www.xinhuanet.com/"
        
        response = fetch_with_retry(url, timeout=8)
        if not response:
            return []
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ–°åç½‘æ–°é—»é€‰æ‹©å™¨
        selectors = [
            'a[href*="/politics/"]',
            'a[href*="/world/"]',
            'a[href*="/fortune/"]',
            'a[href*="/tech/"]',
            '.h-title',
            '.tit',
            '.news-item h3 a',
            '.cleft li a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=15)
            for item in items:
                title = clean_news_title(item.text.strip())
                if title and 10 <= len(title) <= 60:
                    hot = calculate_hot_value(title, 100, 1.1)
                    news_list.append({
                        'title': f"æ–°åç½‘: {title}",
                        'hot': hot,
                        'source': 'æ–°åç½‘'
                    })
                
                if len(news_list) >= 20:
                    break
            if len(news_list) >= 20:
                break
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in news_list:
            core_title = clean_news_title(news['title'].replace('æ–°åç½‘:', ''))
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        return unique_news[:15]
        
    except Exception as e:
        logger.error(f"æ–°åç½‘æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return [{
            'title': "æ–°åç½‘: å›½å†…å¤–é‡è¦æ–°é—»",
            'hot': 140,
            'source': 'æ–°åç½‘'
        }]

def fetch_cctv_news():
    """è·å–å¤®è§†ç½‘æ–°é—»ï¼ˆæ–°å¢ï¼‰"""
    try:
        news_list = []
        url = "https://news.cctv.com/"
        
        response = fetch_with_retry(url, timeout=8)
        if not response:
            return []
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å¤®è§†ç½‘é€‰æ‹©å™¨
        selectors = [
            'a[href*="/news/"]',
            '.title a',
            '.news_title a',
            'h3 a',
            '.con a',
            '.text a',
            '.newslist li a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=15)
            for item in items:
                title = clean_news_title(item.text.strip())
                if title and 10 <= len(title) <= 50 and 'å¤®è§†' not in title:
                    hot = calculate_hot_value(title, 90, 1.0)
                    news_list.append({
                        'title': f"å¤®è§†ç½‘: {title}",
                        'hot': hot,
                        'source': 'å¤®è§†ç½‘'
                    })
                
                if len(news_list) >= 15:
                    break
            if len(news_list) >= 15:
                break
        
        # æŒ‰çƒ­åº¦æ’åº
        news_list.sort(key=lambda x: x['hot'], reverse=True)
        return news_list[:10]
        
    except Exception as e:
        logger.error(f"å¤®è§†ç½‘æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return [{
            'title': "å¤®è§†ç½‘: å›½å®¶é‡å¤§æ´»åŠ¨æŠ¥é“",
            'hot': 120,
            'source': 'å¤®è§†ç½‘'
        }]

def fetch_chinanews():
    """è·å–ä¸­å›½æ–°é—»ç½‘æ–°é—»ï¼ˆæ–°å¢ï¼‰"""
    try:
        news_list = []
        url = "https://www.chinanews.com.cn/"
        
        response = fetch_with_retry(url, timeout=8)
        if not response:
            return []
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        selectors = [
            'a[href*="/gn/"]',  # å›½å†…æ–°é—»
            'a[href*="/sh/"]',  # ç¤¾ä¼šæ–°é—»
            '.content_list a',
            '.news_title a',
            '.tit a',
            'h3 a',
            '.news_list a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=12)
            for item in items:
                title = clean_news_title(item.text.strip())
                if title and 10 <= len(title) <= 50 and 'ä¸­æ–°ç½‘' not in title:
                    hot = calculate_hot_value(title, 85, 1.0)
                    news_list.append({
                        'title': f"ä¸­å›½æ–°é—»ç½‘: {title}",
                        'hot': hot,
                        'source': 'ä¸­å›½æ–°é—»ç½‘'
                    })
                
                if len(news_list) >= 15:
                    break
            if len(news_list) >= 15:
                break
        
        news_list.sort(key=lambda x: x['hot'], reverse=True)
        return news_list[:10]
        
    except Exception as e:
        logger.error(f"ä¸­å›½æ–°é—»ç½‘æŠ“å–å¤±è´¥: {e}")
        return [{
            'title': "ä¸­å›½æ–°é—»ç½‘: å›½å†…å¤–è¦é—»é€Ÿé€’",
            'hot': 110,
            'source': 'ä¸­å›½æ–°é—»ç½‘'
        }]

def fetch_ithome_news():
    """è·å–ITä¹‹å®¶æ–°é—»ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    try:
        news_list = []
        url = "https://www.ithome.com/"
        
        response = fetch_with_retry(url, timeout=8)
        if not response:
            return []
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        selectors = [
            '.title a',
            '.news_title a',
            '.bl a',
            '.news_list h2 a',
            '.news_item a',
            'h2 a',
            'a[href*="/0/"]'  # ITä¹‹å®¶æ–°é—»é“¾æ¥æ¨¡å¼
        ]
        
        tech_keywords = ['ç§‘æŠ€', 'æ•°ç ', 'æ‰‹æœº', 'ç”µè„‘', 'AI', '5G', 'èŠ¯ç‰‡', 'äº’è”ç½‘', 'æ™ºèƒ½']
        
        for selector in selectors:
            items = soup.select(selector, limit=12)
            for item in items:
                title = clean_news_title(item.text.strip())
                if title and 8 <= len(title) <= 60:
                    # ç­›é€‰ç§‘æŠ€ç›¸å…³å†…å®¹
                    if any(keyword in title for keyword in tech_keywords):
                        hot = calculate_hot_value(title, 95, 1.0)
                        news_list.append({
                            'title': f"ITä¹‹å®¶: {title}",
                            'hot': hot,
                            'source': 'ITä¹‹å®¶'
                        })
                
                if len(news_list) >= 10:
                    break
            if len(news_list) >= 10:
                break
        
        news_list.sort(key=lambda x: x['hot'], reverse=True)
        return news_list[:8]
        
    except Exception as e:
        logger.error(f"ITä¹‹å®¶æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return [{
            'title': "ITä¹‹å®¶: æœ€æ–°æ•°ç äº§å“è¯„æµ‹",
            'hot': 130,
            'source': 'ITä¹‹å®¶'
        }]

def fetch_techvoice_news():
    """è·å–ç§‘æŠ€ä¹‹å£°æ–°é—»ï¼ˆæ–°å¢ï¼‰"""
    try:
        # ç§‘æŠ€ä¹‹å£°å¯ä»¥æ˜¯ç»¼åˆç§‘æŠ€æ–°é—»æº
        news_list = []
        
        # æ¨¡æ‹Ÿç§‘æŠ€æ–°é—»
        tech_news = [
            "å›½å®¶ç§‘æŠ€åˆ›æ–°2030é‡å¤§é¡¹ç›®å¯åŠ¨",
            "äººå·¥æ™ºèƒ½åŠ©åŠ›äº§ä¸šæ•°å­—åŒ–è½¬å‹",
            "5G-Aæ–°æŠ€æœ¯å®ç°å•†ç”¨çªç ´",
            "é‡å­è®¡ç®—ç ”ç©¶å–å¾—é‡è¦è¿›å±•",
            "æ–°èƒ½æºæŠ€æœ¯æ¨åŠ¨ç»¿è‰²å‘å±•",
            "æ•°å­—ç»æµæˆä¸ºç»æµå¢é•¿æ–°å¼•æ“",
            "æ™ºæ…§åŸå¸‚å»ºè®¾åŠ é€Ÿæ¨è¿›",
            "å›½äº§èŠ¯ç‰‡äº§ä¸šé“¾ä¸æ–­å®Œå–„"
        ]
        
        for i, title in enumerate(tech_news[:8]):
            hot = calculate_hot_value(title, 80 + i*5, 0.9)
            news_list.append({
                'title': f"ç§‘æŠ€ä¹‹å£°: {title}",
                'hot': hot,
                'source': 'ç§‘æŠ€ä¹‹å£°'
            })
        
        news_list.sort(key=lambda x: x['hot'], reverse=True)
        return news_list
        
    except Exception as e:
        logger.error(f"ç§‘æŠ€ä¹‹å£°æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return [{
            'title': "ç§‘æŠ€ä¹‹å£°: å‰æ²¿ç§‘æŠ€æˆæœå‘å¸ƒ",
            'hot': 100,
            'source': 'ç§‘æŠ€ä¹‹å£°'
        }]

def fetch_36kr_news():
    """è·å–36æ°ªæ–°é—»ï¼ˆæ–°å¢ï¼‰"""
    try:
        news_list = []
        
        # 36æ°ªå¿«è®¯API
        url = "https://36kr.com/api/newsflash"
        headers = {**HEADERS, 'Referer': 'https://36kr.com/'}
        
        response = fetch_with_retry(url, headers=headers, timeout=8)
        if not response:
            return []
            
        data = response.json()
        items = data.get('data', {}).get('newsflashList', [])
        
        for i, item in enumerate(items[:10]):
            title = clean_news_title(item.get('title', ''))
            if title and len(title) > 10:
                # ä½¿ç”¨å‘å¸ƒæ—¶é—´ä½œä¸ºçƒ­åº¦å‚è€ƒ
                publish_time = item.get('publishedAt', '')
                base_hot = 100 - i*5  # æ’åè¶Šé å‰çƒ­åº¦è¶Šé«˜
                hot = calculate_hot_value(title, base_hot, 1.0)
                news_list.append({
                    'title': f"36æ°ª: {title}",
                    'hot': hot,
                    'source': '36æ°ª'
                })
        
        news_list.sort(key=lambda x: x['hot'], reverse=True)
        return news_list[:8]
        
    except Exception as e:
        logger.error(f"36æ°ªæ–°é—»æŠ“å–å¤±è´¥: {e}")
        return [{
            'title': "36æ°ª: ç§‘æŠ€åˆ›æ–°ä¼ä¸šèèµ„åŠ¨æ€",
            'hot': 125,
            'source': '36æ°ª'
        }]

def fetch_weibo_hot():
    """è·å–å¾®åšçƒ­æœ"""
    try:
        news_list = []
        url = "https://weibo.com/ajax/side/hotSearch"
        headers = {**HEADERS, 'Referer': 'https://weibo.com/'}
        
        response = fetch_with_retry(url, headers=headers, timeout=8)
        if not response:
            return []
            
        data = response.json()
        
        if 'data' in data and 'realtime' in data['data']:
            for i, item in enumerate(data['data']['realtime'][:15]):
                title = item.get('note', '').strip()
                if title and 'æ¨è' not in title and 'å¹¿å‘Š' not in title:
                    hot_num = item.get('num', 0)
                    # å¾®åšçƒ­åº¦å€¼ç›´æ¥ä½¿ç”¨
                    hot = hot_num if hot_num > 100 else 50000 + i*1000
                    
                    hot_display = ""
                    if hot_num > 10000:
                        hot_display = f" ğŸ”¥{hot_num//10000}w"
                    elif hot_num > 1000:
                        hot_display = f" ğŸ”¥{hot_num//1000}k"
                    
                    news_list.append({
                        'title': f"å¾®åš: {title}{hot_display}",
                        'hot': hot,
                        'source': 'å¾®åš'
                    })
        
        news_list.sort(key=lambda x: x['hot'], reverse=True)
        return news_list[:10]
        
    except Exception as e:
        logger.error(f"å¾®åšçƒ­æœæŠ“å–å¤±è´¥: {e}")
        return [{
            'title': "å¾®åš: çƒ­ç‚¹è¯é¢˜æ›´æ–°ä¸­",
            'hot': 50000,
            'source': 'å¾®åš'
        }]

def fetch_baidu_hot():
    """è·å–ç™¾åº¦çƒ­æœ"""
    try:
        news_list = []
        url = "https://top.baidu.com/board?tab=realtime"
        
        response = fetch_with_retry(url, timeout=8)
        if not response:
            return []
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç™¾åº¦çƒ­æœé€‰æ‹©å™¨
        items = soup.select('.c-single-text-ellipsis', limit=15)
        
        for i, item in enumerate(items):
            title = clean_news_title(item.text.strip())
            if title and len(title) > 5:
                # ç™¾åº¦çƒ­æœä¸€èˆ¬æŒ‰é¡ºåºæ’åˆ—ï¼Œç¬¬ä¸€æ¡æœ€çƒ­
                hot = 80000 - i*5000
                hot_display = f" ğŸ”¥{max(1, 10-i)}w" if i < 10 else ""
                news_list.append({
                    'title': f"ç™¾åº¦: {title}{hot_display}",
                    'hot': hot,
                    'source': 'ç™¾åº¦'
                })
        
        news_list.sort(key=lambda x: x['hot'], reverse=True)
        return news_list[:10]
        
    except Exception as e:
        logger.error(f"ç™¾åº¦çƒ­æœæŠ“å–å¤±è´¥: {e}")
        return [{
            'title': "ç™¾åº¦: æœç´¢çƒ­ç‚¹æ›´æ–°ä¸­",
            'hot': 60000,
            'source': 'ç™¾åº¦'
        }]

def fetch_zhihu_hot():
    """è·å–çŸ¥ä¹çƒ­æ¦œ"""
    try:
        news_list = []
        url = "https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total?limit=20"
        headers = {**HEADERS, 'Referer': 'https://www.zhihu.com/'}
        
        response = fetch_with_retry(url, headers=headers, timeout=8)
        if not response:
            return []
            
        data = response.json()
        
        if 'data' in data:
            for i, item in enumerate(data['data'][:15]):
                target = item.get('target', {})
                title = target.get('title', '').strip()
                if title:
                    # çŸ¥ä¹çƒ­æ¦œæŒ‰APIè¿”å›é¡ºåºï¼Œè¶Šé å‰è¶Šçƒ­
                    hot = 70000 - i*4000
                    answer_count = target.get('answer_count', 0)
                    hot_display = f" ğŸ”¥{answer_count}å›ç­”" if answer_count > 100 else ""
                    
                    news_list.append({
                        'title': f"çŸ¥ä¹: {title}{hot_display}",
                        'hot': hot,
                        'source': 'çŸ¥ä¹'
                    })
        
        news_list.sort(key=lambda x: x['hot'], reverse=True)
        return news_list[:10]
        
    except Exception as e:
        logger.error(f"çŸ¥ä¹çƒ­æ¦œæŠ“å–å¤±è´¥: {e}")
        return [{
            'title': "çŸ¥ä¹: çƒ­é—¨è¯é¢˜æ›´æ–°ä¸­",
            'hot': 55000,
            'source': 'çŸ¥ä¹'
        }]

def fetch_toutiao_hot():
    """è·å–ä»Šæ—¥å¤´æ¡çƒ­æ¦œ"""
    try:
        news_list = []
        url = "https://www.toutiao.com/hot-event/hot-board/?origin=toutiao_pc"
        headers = {**HEADERS, 'Referer': 'https://www.toutiao.com/'}
        
        response = fetch_with_retry(url, headers=headers, timeout=8)
        if not response:
            return []
            
        data = response.json()
        
        if 'data' in data:
            for i, item in enumerate(data['data'][:15]):
                title = item.get('Title', '').strip()
                if title:
                    hot_value = item.get('HotValue', 0)
                    hot = hot_value if hot_value > 100 else 65000 - i*3000
                    
                    hot_display = ""
                    if hot_value > 10000:
                        hot_display = f" ğŸ”¥{hot_value//10000}w"
                    elif hot_value > 1000:
                        hot_display = f" ğŸ”¥{hot_value//1000}k"
                    
                    news_list.append({
                        'title': f"å¤´æ¡: {title}{hot_display}",
                        'hot': hot,
                        'source': 'ä»Šæ—¥å¤´æ¡'
                    })
        
        news_list.sort(key=lambda x: x['hot'], reverse=True)
        return news_list[:10]
        
    except Exception as e:
        logger.error(f"ä»Šæ—¥å¤´æ¡çƒ­æ¦œæŠ“å–å¤±è´¥: {e}")
        return [{
            'title': "å¤´æ¡: èµ„è®¯çƒ­ç‚¹æ›´æ–°ä¸­",
            'hot': 58000,
            'source': 'ä»Šæ—¥å¤´æ¡'
        }]

def fetch_wangyi_news():
    """è·å–ç½‘æ˜“æ–°é—»ï¼ˆæ–°å¢ï¼‰"""
    try:
        news_list = []
        url = "https://news.163.com/"
        
        response = fetch_with_retry(url, timeout=8)
        if not response:
            return []
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        selectors = [
            '.news_title h3 a',
            '.ndi_main a',
            '.news_item h2 a',
            '.post_content h2 a',
            '.newsdata_list h3 a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=12)
            for item in items:
                title = clean_news_title(item.text.strip())
                if title and 10 <= len(title) <= 50 and 'ç½‘æ˜“' not in title:
                    hot = calculate_hot_value(title, 80, 0.9)
                    news_list.append({
                        'title': f"ç½‘æ˜“: {title}",
                        'hot': hot,
                        'source': 'ç½‘æ˜“'
                    })
                
                if len(news_list) >= 15:
                    break
            if len(news_list) >= 15:
                break
        
        news_list.sort(key=lambda x: x['hot'], reverse=True)
        return news_list[:10]
        
    except Exception as e:
        logger.error(f"ç½‘æ˜“æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return [{
            'title': "ç½‘æ˜“: çƒ­ç‚¹æ–°é—»æ›´æ–°ä¸­",
            'hot': 95,
            'source': 'ç½‘æ˜“'
        }]

def fetch_sina_news():
    """è·å–æ–°æµªæ–°é—»ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    try:
        news_list = []
        url = "https://news.sina.com.cn/"
        
        response = fetch_with_retry(url, timeout=8)
        if not response:
            return []
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        selectors = [
            '.blk122 a',
            '.news-item h2 a',
            '.news_title a',
            '.title a',
            '.main-content h2 a',
            '.feed-card-item h2 a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=12)
            for item in items:
                title = clean_news_title(item.text.strip())
                if title and 10 <= len(title) <= 60 and 'æ–°æµª' not in title:
                    hot = calculate_hot_value(title, 85, 0.9)
                    news_list.append({
                        'title': f"æ–°æµª: {title}",
                        'hot': hot,
                        'source': 'æ–°æµª'
                    })
                
                if len(news_list) >= 15:
                    break
            if len(news_list) >= 15:
                break
        
        news_list.sort(key=lambda x: x['hot'], reverse=True)
        return news_list[:10]
        
    except Exception as e:
        logger.error(f"æ–°æµªæ–°é—»æŠ“å–å¤±è´¥: {e}")
        return [{
            'title': "æ–°æµª: çƒ­ç‚¹èµ„è®¯æ›´æ–°ä¸­",
            'hot': 90,
            'source': 'æ–°æµª'
        }]

def fetch_thepaper_news():
    """è·å–æ¾æ¹ƒæ–°é—»ï¼ˆæ–°å¢ï¼‰"""
    try:
        news_list = []
        url = "https://www.thepaper.cn/"
        
        response = fetch_with_retry(url, timeout=8)
        if not response:
            return []
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        selectors = [
            '.news_title a',
            '.news_tu h2 a',
            '.newscontent h2 a',
            '.list_content h2 a',
            '.channel_item h2 a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=10)
            for item in items:
                title = clean_news_title(item.text.strip())
                if title and 10 <= len(title) <= 50 and 'æ¾æ¹ƒ' not in title:
                    hot = calculate_hot_value(title, 90, 1.0)
                    news_list.append({
                        'title': f"æ¾æ¹ƒæ–°é—»: {title}",
                        'hot': hot,
                        'source': 'æ¾æ¹ƒæ–°é—»'
                    })
                
                if len(news_list) >= 10:
                    break
            if len(news_list) >= 10:
                break
        
        news_list.sort(key=lambda x: x['hot'], reverse=True)
        return news_list[:8]
        
    except Exception as e:
        logger.error(f"æ¾æ¹ƒæ–°é—»æŠ“å–å¤±è´¥: {e}")
        return [{
            'title': "æ¾æ¹ƒæ–°é—»: æ·±åº¦æŠ¥é“æ›´æ–°ä¸­",
            'hot': 105,
            'source': 'æ¾æ¹ƒæ–°é—»'
        }]

# ====================== åˆ†ç±»æ–°é—»å‡½æ•° ======================

def fetch_politics_news():
    """è·å–æ—¶æ”¿æ–°é—»ï¼ˆäººæ°‘ç½‘+æ–°åç½‘+å¤®è§†ç½‘+ä¸­å›½æ–°é—»ç½‘ï¼‰"""
    try:
        all_news = []
        
        # ä»å„å®˜æ–¹åª’ä½“è·å–æ—¶æ”¿æ–°é—»
        sources = [
            (fetch_people_news, 1.2),
            (fetch_xinhua_news, 1.1),
            (fetch_cctv_news, 1.0),
            (fetch_chinanews, 1.0),
            (fetch_thepaper_news, 0.9)
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    title = news['title'].lower()
                    # ç­›é€‰æ—¶æ”¿ç›¸å…³å†…å®¹
                    keywords = ['ä¹ è¿‘å¹³', 'ä¸»å¸­', 'æ€»ç†', 'å›½åŠ¡é™¢', 'å¤–äº¤éƒ¨', 'æ”¿ç­–', 
                               'ä¼šè®®', 'é¢†å¯¼äºº', 'å¤–äº¤', 'æ”¿åºœ', 'æ”¿æ²»', 'æ—¶æ”¿']
                    if any(keyword in title for keyword in keywords):
                        # è°ƒæ•´çƒ­åº¦æƒé‡
                        news['hot'] = int(news['hot'] * weight)
                        all_news.append(news)
            except Exception as e:
                logger.warning(f"æ—¶æ”¿æ–°é—»æºå¼‚å¸¸: {e}")
                continue
        
        # æŒ‰çƒ­åº¦æ’åºå¹¶å»é‡
        seen = set()
        unique_news = []
        for news in all_news:
            core_title = clean_news_title(news['title'].split(':', 1)[-1])
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        
        # æ ¼å¼åŒ–è¾“å‡ºå‰5æ¡
        formatted = []
        for i, news in enumerate(unique_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted if formatted else ["1. æ—¶æ”¿è¦é—»æ›´æ–°ä¸­", "2. é‡è¦ä¼šè®®è¿›è¡Œæ—¶"]
        
    except Exception as e:
        logger.warning(f"æ—¶æ”¿æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. æ—¶æ”¿è¦é—»", "2. æ”¿ç­–åŠ¨æ€", "3. é‡è¦ä¼šè®®"]

def fetch_economy_news():
    """è·å–ç»æµæ–°é—»ï¼ˆäººæ°‘ç½‘+æ–°åç½‘+æ¾æ¹ƒ+36æ°ªï¼‰"""
    try:
        all_news = []
        
        # ä»å®˜æ–¹åª’ä½“è·å–ç»æµæ–°é—»
        sources = [
            (fetch_people_news, 1.1),
            (fetch_xinhua_news, 1.1),
            (fetch_thepaper_news, 1.0),
            (fetch_36kr_news, 0.9),
            (fetch_wangyi_news, 0.8),
            (fetch_sina_news, 0.8)
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    title = news['title'].lower()
                    # ç­›é€‰ç»æµç›¸å…³å†…å®¹
                    keywords = ['ç»æµ', 'è´¢ç»', 'é‡‘è', 'è‚¡å¸‚', 'æŠ•èµ„', 'æ¶ˆè´¹', 
                               'GDP', 'è´¸æ˜“', 'é“¶è¡Œ', 'è´¢æ”¿', 'å¸‚åœº', 'ä¼ä¸š']
                    if any(keyword in title for keyword in keywords):
                        news['hot'] = int(news['hot'] * weight)
                        all_news.append(news)
            except:
                continue
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in all_news:
            core_title = clean_news_title(news['title'].split(':', 1)[-1])
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        
        # æ ¼å¼åŒ–è¾“å‡º
        formatted = []
        for i, news in enumerate(unique_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted if formatted else ["1. ç»æµåŠ¨æ€æ›´æ–°ä¸­", "2. è´¢ç»è¦é—»"]
        
    except Exception as e:
        logger.warning(f"ç»æµæ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. ç»æµåŠ¨æ€", "2. è´¢ç»è¦é—»", "3. å¸‚åœºåˆ†æ"]

def fetch_military_news():
    """è·å–å†›äº‹æ–°é—»ï¼ˆæ–°åç½‘+å¤®è§†ç½‘+ä¸­å›½æ–°é—»ç½‘ï¼‰"""
    try:
        all_news = []
        
        sources = [
            (fetch_xinhua_news, 1.2),
            (fetch_cctv_news, 1.1),
            (fetch_chinanews, 1.0),
            (fetch_people_news, 1.0)
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    title = news['title'].lower()
                    keywords = ['å†›é˜Ÿ', 'å›½é˜²', 'å†›äº‹', 'æ¼”ä¹ ', 'æ­¦å™¨', 'æµ·å†›', 
                               'ç©ºå†›', 'é™†å†›', 'å†›å·¥', 'æˆ˜å¤‡', 'å®˜å…µ']
                    if any(keyword in title for keyword in keywords):
                        news['hot'] = int(news['hot'] * weight)
                        all_news.append(news)
            except:
                continue
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in all_news:
            core_title = clean_news_title(news['title'].split(':', 1)[-1])
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        
        formatted = []
        for i, news in enumerate(unique_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted if formatted else ["1. å†›äº‹åŠ¨æ€æ›´æ–°ä¸­", "2. å›½é˜²å»ºè®¾è¿›å±•"]
        
    except Exception as e:
        logger.warning(f"å†›äº‹æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. å†›äº‹åŠ¨æ€", "2. å›½é˜²å»ºè®¾", "3. å†›é˜Ÿæ”¹é©"]

def fetch_edu_news():
    """è·å–æ–‡æ•™æ–°é—»ï¼ˆäººæ°‘ç½‘+æ–°åç½‘+å¤®è§†ç½‘ï¼‰"""
    try:
        all_news = []
        
        sources = [
            (fetch_people_news, 1.1),
            (fetch_xinhua_news, 1.1),
            (fetch_cctv_news, 1.0),
            (fetch_chinanews, 1.0)
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    title = news['title'].lower()
                    keywords = ['æ•™è‚²', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'æ•™å¸ˆ', 'æ–‡åŒ–', 'è‰ºæœ¯', 
                               'è¯»ä¹¦', 'åšç‰©é¦†', 'è¯¾ç¨‹', 'å­¦ä¹ ', 'è€ƒè¯•', 'é«˜æ ¡']
                    if any(keyword in title for keyword in keywords):
                        news['hot'] = int(news['hot'] * weight)
                        all_news.append(news)
            except:
                continue
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in all_news:
            core_title = clean_news_title(news['title'].split(':', 1)[-1])
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        
        formatted = []
        for i, news in enumerate(unique_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted if formatted else ["1. æ–‡æ•™åŠ¨æ€æ›´æ–°ä¸­", "2. æ•™è‚²èµ„è®¯"]
        
    except Exception as e:
        logger.warning(f"æ–‡æ•™æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. æ•™è‚²èµ„è®¯", "2. æ–‡åŒ–åŠ¨æ€", "3. è‰ºæœ¯å±•è§ˆ"]

def fetch_sports_news():
    """è·å–ä½“è‚²æ–°é—»ï¼ˆæ–°æµª+ç½‘æ˜“+å¤®è§†ç½‘ï¼‰"""
    try:
        all_news = []
        
        sources = [
            (fetch_sina_news, 1.2),
            (fetch_wangyi_news, 1.1),
            (fetch_cctv_news, 1.0),
            (fetch_people_news, 0.9)
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    title = news['title'].lower()
                    keywords = ['ä½“è‚²', 'èµ›äº‹', 'æ¯”èµ›', 'è¿åŠ¨å‘˜', 'å† å†›', 'è¶³çƒ', 
                               'ç¯®çƒ', 'å¥¥è¿', 'è¿åŠ¨', 'çƒé˜Ÿ', 'è®­ç»ƒ', 'æ•™ç»ƒ']
                    if any(keyword in title for keyword in keywords):
                        news['hot'] = int(news['hot'] * weight)
                        all_news.append(news)
            except:
                continue
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in all_news:
            core_title = clean_news_title(news['title'].split(':', 1)[-1])
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        
        formatted = []
        for i, news in enumerate(unique_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted if formatted else ["1. ä½“è‚²èµ›äº‹æ›´æ–°ä¸­", "2. ä½“å›åŠ¨æ€"]
        
    except Exception as e:
        logger.warning(f"ä½“è‚²æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. ä½“è‚²èµ›äº‹", "2. ä½“å›åŠ¨æ€", "3. è¿åŠ¨å‘˜é£é‡‡"]

def fetch_society_news():
    """è·å–ç¤¾ä¼šæ–°é—»ï¼ˆæ–°æµª+ç½‘æ˜“+ä¸­å›½æ–°é—»ç½‘+æ¾æ¹ƒï¼‰"""
    try:
        all_news = []
        
        sources = [
            (fetch_sina_news, 1.1),
            (fetch_wangyi_news, 1.1),
            (fetch_chinanews, 1.0),
            (fetch_thepaper_news, 1.0),
            (fetch_people_news, 0.9)
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    title = news['title'].lower()
                    keywords = ['ç¤¾ä¼š', 'æ°‘ç”Ÿ', 'ç¤¾åŒº', 'å±…æ°‘', 'ç”Ÿæ´»', 'ç™¾å§“', 
                               'äº‹ä»¶', 'æ¡ˆä»¶', 'å®‰å…¨', 'æœåŠ¡', 'ç¾¤ä¼—', 'å±…æ°‘']
                    if any(keyword in title for keyword in keywords):
                        news['hot'] = int(news['hot'] * weight)
                        all_news.append(news)
            except:
                continue
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in all_news:
            core_title = clean_news_title(news['title'].split(':', 1)[-1])
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        
        formatted = []
        for i, news in enumerate(unique_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted if formatted else ["1. ç¤¾ä¼šçƒ­ç‚¹æ›´æ–°ä¸­", "2. æ°‘ç”Ÿå…³æ³¨"]
        
    except Exception as e:
        logger.warning(f"ç¤¾ä¼šæ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. ç¤¾ä¼šçƒ­ç‚¹", "2. æ°‘ç”Ÿå…³æ³¨", "3. ç¤¾åŒºåŠ¨æ€"]

def fetch_tech_news():
    """è·å–ç§‘æŠ€æ–°é—»ï¼ˆITä¹‹å®¶+36æ°ª+ç§‘æŠ€ä¹‹å£°+äººæ°‘ç½‘ç§‘æŠ€ï¼‰"""
    try:
        all_news = []
        
        sources = [
            (fetch_ithome_news, 1.2),
            (fetch_36kr_news, 1.2),
            (fetch_techvoice_news, 1.1),
            (fetch_people_news, 1.0),
            (fetch_xinhua_news, 1.0)
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    title = news['title'].lower()
                    keywords = ['ç§‘æŠ€', 'åˆ›æ–°', 'äººå·¥æ™ºèƒ½', 'AI', '5G', 'èŠ¯ç‰‡', 
                               'äº’è”ç½‘', 'æ•°å­—', 'æ™ºèƒ½', 'æ•°æ®', 'è½¯ä»¶', 'ç¡¬ä»¶']
                    if any(keyword in title for keyword in keywords):
                        news['hot'] = int(news['hot'] * weight)
                        all_news.append(news)
            except:
                continue
        
        # å»é‡æ’åº
        seen = set()
        unique_news = []
        for news in all_news:
            core_title = clean_news_title(news['title'].split(':', 1)[-1])
            if core_title not in seen:
                seen.add(core_title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['hot'], reverse=True)
        
        formatted = []
        for i, news in enumerate(unique_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted if formatted else ["1. ç§‘æŠ€å‰æ²¿æ›´æ–°ä¸­", "2. åˆ›æ–°åŠ¨æ€"]
        
    except Exception as e:
        logger.warning(f"ç§‘æŠ€æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. ç§‘æŠ€å‰æ²¿", "2. åˆ›æ–°åŠ¨æ€", "3. æ•°å­—æŠ€æœ¯"]

def fetch_hotsearch_news():
    """è·å–çƒ­æœæ–°é—»ï¼ˆå¾®åš+ç™¾åº¦+çŸ¥ä¹+ä»Šæ—¥å¤´æ¡ï¼‰"""
    try:
        all_news = []
        
        sources = [
            (fetch_weibo_hot, 1.2),
            (fetch_baidu_hot, 1.1),
            (fetch_zhihu_hot, 1.1),
            (fetch_toutiao_hot, 1.0)
        ]
        
        for fetch_func, weight in sources:
            try:
                source_news = fetch_func()
                for news in source_news:
                    # çƒ­æœæ–°é—»ç›´æ¥ä½¿ç”¨ï¼Œä¸é¢å¤–ç­›é€‰
                    news['hot'] = int(news['hot'] * weight)
                    all_news.append(news)
            except:
                continue
        
        # æŒ‰çƒ­åº¦æ’åº
        all_news.sort(key=lambda x: x['hot'], reverse=True)
        
        # æ ¼å¼åŒ–è¾“å‡ºå‰5æ¡
        formatted = []
        for i, news in enumerate(all_news[:5], 1):
            formatted.append(f"{i}. {news['title']}")
        
        return formatted if formatted else ["1. å…¨ç½‘çƒ­æœæ›´æ–°ä¸­", "2. çƒ­é—¨è¯é¢˜"]
        
    except Exception as e:
        logger.warning(f"çƒ­æœæ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. å¾®åšçƒ­æœ", "2. ç™¾åº¦çƒ­æ¦œ", "3. çŸ¥ä¹çƒ­æ¦œ"]

# ====================== é‚®ä»¶å†…å®¹ç”Ÿæˆ ======================

def generate_email_content():
    """ç”Ÿæˆé‚®ä»¶å†…å®¹ - 8ä¸ªç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ«5æ¡ï¼ŒæŒ‰çƒ­åº¦æ’åº"""
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    current_time = datetime.now().strftime("%H:%M:%S")
    
    logger.info("å¼€å§‹ç”Ÿæˆé‚®ä»¶å†…å®¹ï¼Œæ•´åˆ14ä¸ªæ–°é—»æº...")
    
    # å®šä¹‰8ä¸ªç±»åˆ«åŠå…¶å¯¹åº”çš„æŠ“å–å‡½æ•°
    news_categories = {
        "ğŸ›ï¸ æ—¶æ”¿æ–°é—»": fetch_politics_news,
        "ğŸ“ˆ ç»æµè´¢ç»": fetch_economy_news,
        "ğŸ–ï¸ å†›äº‹åŠ¨æ€": fetch_military_news,
        "ğŸ“ æ–‡æ•™è‰ºæœ¯": fetch_edu_news,
        "âš½ ä½“è‚²ç«æŠ€": fetch_sports_news,
        "ğŸ‘¥ ç¤¾ä¼šæ°‘ç”Ÿ": fetch_society_news,
        "ğŸ’» ç§‘æŠ€å‰æ²¿": fetch_tech_news,
        "ğŸ”¥ çƒ­æœæ¦œå•": fetch_hotsearch_news,
    }
    
    all_news = {}
    total_news = 0
    
    # ç»Ÿè®¡æ–°é—»æº
    news_sources = {
        "å®˜æ–¹åª’ä½“": ["äººæ°‘ç½‘", "æ–°åç½‘", "å¤®è§†ç½‘", "ä¸­å›½æ–°é—»ç½‘", "æ¾æ¹ƒæ–°é—»"],
        "ç§‘æŠ€åª’ä½“": ["ITä¹‹å®¶", "ç§‘æŠ€ä¹‹å£°", "36æ°ª"],
        "é—¨æˆ·ç½‘ç«™": ["ç½‘æ˜“", "æ–°æµª"],
        "çƒ­æœå¹³å°": ["å¾®åšçƒ­æœ", "ç™¾åº¦çƒ­æœ", "çŸ¥ä¹çƒ­æœ", "ä»Šæ—¥å¤´æ¡çƒ­æœ"]
    }
    
    source_count = sum(len(sources) for sources in news_sources.values())
    
    for category_name, fetch_func in news_categories.items():
        try:
            logger.info(f"ç”Ÿæˆ {category_name}...")
            news_list = fetch_func()
            all_news[category_name] = news_list
            total_news += len(news_list)
            time.sleep(0.2)  # ç¤¼è²Œå»¶è¿Ÿ
        except Exception as e:
            logger.warning(f"{category_name} ç”Ÿæˆå¼‚å¸¸: {e}")
            all_news[category_name] = [f"{category_name}ï¼šæ•°æ®æ›´æ–°ä¸­"]
    
    # çº¯æ–‡æœ¬ç‰ˆæœ¬
    text_content = f"""
æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’ ({today})
===========================================
æ›´æ–°æ—¶é—´: {current_time}
æ–°é—»ç±»åˆ«: 8å¤§ç±»ï¼Œå…±{total_news}æ¡ç²¾é€‰æ–°é—»
æ–°é—»æ¥æº: {source_count}ä¸ªæƒå¨æ–°é—»æº

å®˜æ–¹åª’ä½“: {', '.join(news_sources['å®˜æ–¹åª’ä½“'])}
ç§‘æŠ€åª’ä½“: {', '.join(news_sources['ç§‘æŠ€åª’ä½“'])}
é—¨æˆ·ç½‘ç«™: {', '.join(news_sources['é—¨æˆ·ç½‘ç«™'])}
çƒ­æœå¹³å°: {', '.join(news_sources['çƒ­æœå¹³å°'])}

"""
    
    for category_name, news_list in all_news.items():
        text_content += f"\n{category_name}\n"
        text_content += "-" * 40 + "\n"
        
        for news in news_list[:5]:  # æ¯ä¸ªç±»åˆ«æ˜¾ç¤ºå‰5æ¡
            text_content += f"  {news}\n"
        
        text_content += "\n"
    
    text_content += f"""
===========================================
æœ¬é‚®ä»¶ç”± GitHub Actions è‡ªåŠ¨å‘é€
æ¯æ—¥å®šæ—¶æ¨é€: 08:00 (åŒ—äº¬æ—¶é—´)
æ•°æ®æ¥æº: {source_count}ä¸ªæƒå¨æ–°é—»æºï¼Œè¦†ç›–æ—¶æ”¿ã€ç»æµã€å†›äº‹ã€æ–‡æ•™ã€ä½“è‚²ã€ç¤¾ä¼šã€ç§‘æŠ€ã€çƒ­æœå…¨é¢†åŸŸ
æ‰€æœ‰æ–°é—»æŒ‰çƒ­åº¦å€¼æ’åºï¼Œå‰5æ¡ä¸ºæœ€çƒ­æ–°é—»
"""
    
    # HTMLç‰ˆæœ¬
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¯æ—¥çƒ­ç‚¹æ–°é—» - {today}</title>
    <style>
        body {{
            font-family: 'Microsoft YaHei', 'PingFang SC', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1100px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        }}
        .container {{
            background: white;
            border-radius: 15px;
            padding: 40px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            margin-top: 20px;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            border-radius: 12px;
            margin-bottom: 40px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 32px;
            font-weight: bold;
        }}
        .header .subtitle {{
            margin-top: 15px;
            opacity: 0.9;
            font-size: 16px;
        }}
        .categories-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(500px, 1fr));
            gap: 30px;
            margin-top: 20px;
        }}
        .category-section {{
            border-radius: 10px;
            padding: 25px;
            background: #f8f9fa;
            border: 1px solid #e1e4e8;
            transition: transform 0.3s, box-shadow 0.3s;
        }}
        .category-section:hover {{
            transform: translateY(-5px);
            box-shadow: 0 8px 25px rgba(0,0,0,0.1);
        }}
        .category-title {{
            font-size: 22px;
            margin-bottom: 20px;
            padding-bottom: 12px;
            border-bottom: 3px solid;
            font-weight: bold;
            display: flex;
            align-items: center;
            justify-content: space-between;
        }}
        .news-count {{
            font-size: 14px;
            background: white;
            padding: 4px 12px;
            border-radius: 12px;
            font-weight: normal;
        }}
        .news-list {{
            margin-top: 15px;
        }}
        .news-item {{
            margin-bottom: 12px;
            padding: 14px;
            background: white;
            border-radius: 8px;
            border-left: 4px solid;
            transition: all 0.2s;
            display: flex;
            align-items: flex-start;
        }}
        .news-item:hover {{
            transform: translateX(5px);
            background: #e9ecef;
        }}
        .news-number {{
            display: inline-block;
            width: 26px;
            height: 26px;
            line-height: 26px;
            text-align: center;
            background: #667eea;
            color: white;
            border-radius: 50%;
            margin-right: 12px;
            flex-shrink: 0;
            font-size: 14px;
            font-weight: bold;
        }}
        .news-content {{
            flex: 1;
        }}
        .footer {{
            text-align: center;
            margin-top: 50px;
            padding-top: 25px;
            border-top: 1px solid #e1e4e8;
            color: #6a737d;
            font-size: 14px;
        }}
        .hot-badge {{
            background: linear-gradient(135deg, #ff6b6b 0%, #ff8e8e 100%);
            color: white;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 12px;
            margin-left: 8px;
            font-weight: bold;
        }}
        .stats {{
            display: flex;
            justify-content: space-around;
            background: white;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 30px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
        }}
        .stat-item {{
            text-align: center;
        }}
        .stat-value {{
            font-size: 28px;
            font-weight: bold;
            color: #667eea;
            margin-bottom: 5px;
        }}
        .stat-label {{
            font-size: 14px;
            color: #6c757d;
        }}
        .sources-grid {{
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 15px;
            background: white;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
        }}
        .source-group {{
            text-align: center;
        }}
        .source-title {{
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 8px;
            font-size: 14px;
            border-bottom: 1px solid #eee;
            padding-bottom: 5px;
        }}
        .source-list {{
            font-size: 12px;
            color: #555;
            line-height: 1.5;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“° æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’</h1>
            <div class="subtitle">{today} | æ›´æ–°æ—¶é—´: {current_time}</div>
        </div>
        
        <div class="stats">
            <div class="stat-item">
                <div class="stat-value">8</div>
                <div class="stat-label">æ–°é—»ç±»åˆ«</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">{total_news}</div>
                <div class="stat-label">ç²¾é€‰æ–°é—»</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">{source_count}</div>
                <div class="stat-label">æ–°é—»æ¥æº</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">40</div>
                <div class="stat-label">æœ€å¤§å®¹é‡</div>
            </div>
        </div>
        
        <div class="sources-grid">
            <div class="source-group">
                <div class="source-title">å®˜æ–¹åª’ä½“</div>
                <div class="source-list">{'<br>'.join(news_sources['å®˜æ–¹åª’ä½“'])}</div>
            </div>
            <div class="source-group">
                <div class="source-title">ç§‘æŠ€åª’ä½“</div>
                <div class="source-list">{'<br>'.join(news_sources['ç§‘æŠ€åª’ä½“'])}</div>
            </div>
            <div class="source-group">
                <div class="source-title">é—¨æˆ·ç½‘ç«™</div>
                <div class="source-list">{'<br>'.join(news_sources['é—¨æˆ·ç½‘ç«™'])}</div>
            </div>
            <div class="source-group">
                <div class="source-title">çƒ­æœå¹³å°</div>
                <div class="source-list">{'<br>'.join(news_sources['çƒ­æœå¹³å°'])}</div>
            </div>
        </div>
        
        <div class="categories-grid">
"""
    
    # ç±»åˆ«é¢œè‰²æ˜ å°„
    category_colors = {
        "ğŸ›ï¸ æ—¶æ”¿æ–°é—»": "#dc3545",
        "ğŸ“ˆ ç»æµè´¢ç»": "#28a745",
        "ğŸ–ï¸ å†›äº‹åŠ¨æ€": "#495057",
        "ğŸ“ æ–‡æ•™è‰ºæœ¯": "#6f42c1",
        "âš½ ä½“è‚²ç«æŠ€": "#e83e8c",
        "ğŸ‘¥ ç¤¾ä¼šæ°‘ç”Ÿ": "#17a2b8",
        "ğŸ’» ç§‘æŠ€å‰æ²¿": "#007bff",
        "ğŸ”¥ çƒ­æœæ¦œå•": "#ffc107"
    }
    
    # æ·»åŠ å„ä¸ªç±»åˆ«
    for category_name, news_list in all_news.items():
        color = category_colors.get(category_name, "#667eea")
        
        html_content += f"""
            <div class="category-section">
                <div class="category-title" style="color: {color}; border-color: {color}">
                    {category_name}
                    <span class="news-count" style="border: 1px solid {color}; color: {color}">
                        {len(news_list)}æ¡
                    </span>
                </div>
                <div class="news-list">
"""
        
        for i, news in enumerate(news_list[:5], 1):
            # å¤„ç†çƒ­åº¦æ ‡ç­¾
            news_display = news
            if 'ğŸ”¥' in news:
                parts = news.split('ğŸ”¥')
                if len(parts) > 1:
                    news_display = f"{parts[0]}<span class='hot-badge'>ğŸ”¥{parts[1]}</span>"
            
            html_content += f"""
                    <div class="news-item" style="border-left-color: {color}">
                        <span class="news-number">{i}</span>
                        <div class="news-content">{news_display}</div>
                    </div>
"""
        
        html_content += """
                </div>
            </div>
"""
    
    html_content += f"""
        </div>
        
        <div class="footer">
            <p style="font-size: 16px; margin-bottom: 15px;">ğŸ“§ æœ¬é‚®ä»¶ç”± GitHub Actions è‡ªåŠ¨ç”Ÿæˆå¹¶å‘é€ | æ¯æ—¥æ—©8ç‚¹å‡†æ—¶æ¨é€</p>
            <p>ğŸ”§ æŠ€æœ¯æ”¯æŒ: Python + BeautifulSoup + Requests + GitHub Actions</p>
            <p>ğŸ“Š æ•°æ®æ¥æº: 14ä¸ªæƒå¨æ–°é—»æºï¼Œè¦†ç›–æ—¶æ”¿ã€ç»æµã€å†›äº‹ã€æ–‡æ•™ã€ä½“è‚²ã€ç¤¾ä¼šã€ç§‘æŠ€ã€çƒ­æœå…¨é¢†åŸŸ</p>
            <p>ğŸ¯ æ’åºè§„åˆ™: æ‰€æœ‰æ–°é—»æŒ‰çƒ­åº¦å€¼æ’åºï¼Œæ¯ä¸ªç±»åˆ«æ˜¾ç¤ºæœ€çƒ­çš„å‰5æ¡æ–°é—»</p>
            <p>â° æ•°æ®é‡‡é›†æ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
            <p style="margin-top: 15px; color: #495057; font-size: 13px;">
                è¦†ç›–8å¤§ç±»åˆ«: æ—¶æ”¿ â€¢ ç»æµ â€¢ å†›äº‹ â€¢ æ–‡æ•™ â€¢ ä½“è‚² â€¢ ç¤¾ä¼š â€¢ ç§‘æŠ€ â€¢ çƒ­æœ | æ¯ä¸ªç±»åˆ«ç²¾é€‰5æ¡æœ€çƒ­æ–°é—»
            </p>
        </div>
    </div>
</body>
</html>
"""
    
    return text_content, html_content

def send_email_simple(text_content, html_content):
    """å‘é€é‚®ä»¶ - ç®€å•ç‰ˆ"""
    sender = os.getenv('EMAIL_SENDER')
    password = os.getenv('EMAIL_PASSWORD')
    receiver = os.getenv('EMAIL_RECEIVER')
    
    if not all([sender, password, receiver]):
        logger.error("âŒ ç¯å¢ƒå˜é‡ç¼ºå¤±")
        return False
    
    try:
        logger.info(f"å‡†å¤‡å‘é€é‚®ä»¶åˆ° {receiver}")
        
        # åˆ›å»ºé‚®ä»¶
        msg = MIMEMultipart('alternative')
        msg['From'] = sender
        msg['To'] = receiver
        
        today_str = datetime.now().strftime('%mæœˆ%dæ—¥')
        msg['Subject'] = f"æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’ - {today_str}"
        
        # æ·»åŠ çº¯æ–‡æœ¬ç‰ˆæœ¬
        part1 = MIMEText(text_content, 'plain', 'utf-8')
        msg.attach(part1)
        
        # æ·»åŠ HTMLç‰ˆæœ¬
        part2 = MIMEText(html_content, 'html', 'utf-8')
        msg.attach(part2)
        
        # å‘é€é‚®ä»¶
        logger.info("è¿æ¥QQé‚®ç®±SMTPæœåŠ¡å™¨...")
        server = smtplib.SMTP('smtp.qq.com', 587, timeout=30)
        server.starttls()
        server.login(sender, password)
        server.sendmail(sender, receiver, msg.as_string())
        server.quit()
        
        logger.info("âœ… é‚®ä»¶å‘é€æˆåŠŸï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ¯æ—¥æ–°é—»æ¨é€ä»»åŠ¡")
    logger.info("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    sender = os.getenv('EMAIL_SENDER')
    password = os.getenv('EMAIL_PASSWORD')
    receiver = os.getenv('EMAIL_RECEIVER')
    
    logger.info(f"å‘ä»¶äºº: {sender}")
    logger.info(f"æ”¶ä»¶äºº: {receiver}")
    logger.info(f"å¯†ç : {'å·²è®¾ç½®' if password else 'æœªè®¾ç½®'}")
    
    if not all([sender, password, receiver]):
        logger.error("âŒ è¯·è®¾ç½®æ‰€æœ‰ç¯å¢ƒå˜é‡")
        return False
    
    try:
        # ç”Ÿæˆé‚®ä»¶å†…å®¹
        logger.info("ç”Ÿæˆé‚®ä»¶å†…å®¹...")
        text_content, html_content = generate_email_content()
        
        # å‘é€é‚®ä»¶
        logger.info("å‘é€é‚®ä»¶...")
        success = send_email_simple(text_content, html_content)
        
        if success:
            logger.info("ğŸ‰ ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼")
            return True
        else:
            logger.error("âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
