#!/usr/bin/env python3
"""
æ¯æ—¥çƒ­ç‚¹æ–°é—»æ¨é€ - ä¸“ä¸šåˆ†ç±»ç‰ˆ
8ä¸ªç±»åˆ«ï¼šæ—¶æ”¿ã€å†›äº‹ã€ç¤¾ä¼šã€ç»æµã€ç§‘æŠ€ã€çƒ­æœã€ä½“è‚²ã€æ–‡æ•™
æ¯ä¸ªç±»åˆ«5æ¡ç²¾é€‰æ–°é—»
æ–°å¢æ–°é—»æºï¼šæŠ–éŸ³ã€36æ°ªã€ä»Šæ—¥å¤´æ¡çƒ­æ¦œ
"""

import os
import sys
import time
import logging
import smtplib
import requests
import json
from datetime import datetime
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from bs4 import BeautifulSoup
import re

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# é€šç”¨è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
}

# ====================== æ–°å¢æ–°é—»æºå‡½æ•° ======================

def fetch_douyin_hot():
    """è·å–æŠ–éŸ³çƒ­ç‚¹"""
    try:
        # æŠ–éŸ³çƒ­ç‚¹API
        url = "https://www.douyin.com/aweme/v1/web/hot/search/list/"
        headers = {
            **HEADERS,
            'Referer': 'https://www.douyin.com/',
            'Accept': 'application/json, text/plain, */*'
        }
        
        # ä½¿ç”¨éšæœºè®¾å¤‡å‚æ•°
        params = {
            'device_platform': 'webapp',
            'aid': '6383',
            'channel': 'channel_pc_web',
            'detail_list': '1',
            'source': '6',
            'pc_client_type': '1',
            'version_code': '190500',
            'version_name': '19.5.0'
        }
        
        response = requests.get(url, headers=headers, params=params, timeout=15)
        
        if response.status_code == 200:
            try:
                data = response.json()
                news_list = []
                
                if 'data' in data and 'word_list' in data['data']:
                    for i, item in enumerate(data['data']['word_list'][:5], 1):
                        sentence = item.get('sentence', '')
                        hot_value = item.get('hot_value', 0)
                        
                        if sentence:
                            if hot_value > 10000:
                                news_list.append(f"{i}. {sentence} ğŸ”¥{hot_value//10000}w")
                            else:
                                news_list.append(f"{i}. {sentence}")
                
                if news_list:
                    return news_list
            except json.JSONDecodeError:
                pass
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ç½‘é¡µç‰ˆ
        url2 = "https://www.douyin.com/hot"
        headers2 = {
            **HEADERS,
            'Referer': 'https://www.douyin.com/',
            'Cookie': '__ac_nonce=0645b127800c0e5b5b2f3'
        }
        
        response2 = requests.get(url2, headers=headers2, timeout=15)
        soup = BeautifulSoup(response2.text, 'html.parser')
        
        news_list = []
        # å°è¯•å¤šç§é€‰æ‹©å™¨
        selectors = [
            '.BfqNqZX9',
            '.Ny7lCzjh',
            '[class*="HotItem"]',
            '[class*="hot-item"]',
            '.CgEDpFFU'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=10)
            for i, item in enumerate(items[:5], 1):
                text = item.text.strip()
                if text and len(text) > 5:
                    # æ¸…ç†æ–‡æœ¬
                    clean_text = re.sub(r'\s+', ' ', text)
                    if clean_text not in [re.sub(r'\d+\.\s*', '', n) for n in news_list]:
                        news_list.append(f"{i}. {clean_text}")
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        if not news_list:
            # ä»é¡µé¢æ–‡æœ¬ä¸­æå–
            all_text = soup.get_text()
            lines = [line.strip() for line in all_text.split('\n') if len(line.strip()) > 10]
            for i, line in enumerate(lines[:5], 1):
                news_list.append(f"{i}. {line}")
        
        return news_list if news_list else ["1. æŠ–éŸ³çƒ­ç‚¹æ›´æ–°ä¸­", "2. çŸ­è§†é¢‘å¹³å°çƒ­é—¨å†…å®¹"]
        
    except Exception as e:
        logger.warning(f"æŠ–éŸ³çƒ­ç‚¹æŠ“å–å¤±è´¥: {e}")
        return ["1. æŠ–éŸ³çƒ­ç‚¹", "2. çŸ­è§†é¢‘çƒ­é—¨", "3. å¹³å°è¶‹åŠ¿"]

def fetch_36kr_hot():
    """è·å–36æ°ªçƒ­ç‚¹"""
    try:
        # 36æ°ªçƒ­ç‚¹API
        url = "https://36kr.com/pp/api/aggregation-entity"
        headers = {
            **HEADERS,
            'Referer': 'https://36kr.com/',
            'Accept': 'application/json, text/plain, */*'
        }
        
        # å°è¯•è·å–çƒ­ç‚¹èµ„è®¯
        response = requests.get(url, headers=headers, timeout=15)
        
        news_list = []
        
        try:
            if response.status_code == 200:
                data = response.json()
                # å°è¯•ä¸åŒçš„æ•°æ®è·¯å¾„
                if 'data' in data and 'items' in data['data']:
                    for i, item in enumerate(data['data']['items'][:5], 1):
                        title = item.get('title', '') or item.get('post', {}).get('title', '')
                        if title:
                            news_list.append(f"{i}. {title}")
        except:
            pass
        
        # ç½‘é¡µæŠ“å–å¤‡ç”¨æ–¹æ¡ˆ
        if not news_list:
            url2 = "https://36kr.com/hot-list/catalog"
            response2 = requests.get(url2, headers=HEADERS, timeout=15)
            soup = BeautifulSoup(response2.text, 'html.parser')
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            selectors = [
                '.kr-shadow-content .article-item-title',
                '.hotlist-item-toptwo-title',
                '.hotlist-item-title',
                '.article-item-title',
                '.title a',
                'h3 a',
                '.kr-flow-article-item-title'
            ]
            
            for selector in selectors:
                items = soup.select(selector, limit=10)
                for i, item in enumerate(items[:5], 1):
                    title = item.text.strip()
                    if title and len(title) > 8:
                        # å»é‡
                        if title not in [re.sub(r'\d+\.\s*', '', n).strip() for n in news_list]:
                            news_list.append(f"{i}. {title}")
                    if len(news_list) >= 5:
                        break
                if len(news_list) >= 5:
                    break
        
        if not news_list:
            # ä»é¡µé¢ä¸­æå–æ‰€æœ‰æ ‡é¢˜
            url3 = "https://36kr.com/"
            response3 = requests.get(url3, headers=HEADERS, timeout=10)
            soup3 = BeautifulSoup(response3.text, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æ ‡é¢˜çš„å…ƒç´ 
            title_elements = soup3.find_all(['h1', 'h2', 'h3', 'h4', 'h5'], class_=re.compile(r'title|Title'))
            for i, elem in enumerate(title_elements[:5], 1):
                title = elem.text.strip()
                if title and len(title) > 10:
                    news_list.append(f"{i}. {title}")
        
        return news_list if news_list else ["1. 36æ°ªçƒ­ç‚¹æ›´æ–°ä¸­", "2. åˆ›æŠ•ç§‘æŠ€èµ„è®¯"]
        
    except Exception as e:
        logger.warning(f"36æ°ªçƒ­ç‚¹æŠ“å–å¤±è´¥: {e}")
        return ["1. 36æ°ªçƒ­ç‚¹", "2. åˆ›æŠ•èµ„è®¯", "3. ç§‘æŠ€åˆ›ä¸š"]

def fetch_toutiao_hotlist():
    """è·å–ä»Šæ—¥å¤´æ¡çƒ­æ¦œ"""
    try:
        # ä»Šæ—¥å¤´æ¡çƒ­æ¦œAPI
        url = "https://www.toutiao.com/hot-event/hot-board/?origin=toutiao_pc"
        headers = {
            **HEADERS,
            'Referer': 'https://www.toutiao.com/',
            'Accept': 'application/json, text/plain, */*'
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            try:
                data = response.json()
                news_list = []
                
                if 'data' in data:
                    for i, item in enumerate(data['data'][:5], 1):
                        title = item.get('Title', '') or item.get('title', '')
                        hot_value = item.get('HotValue', 0) or item.get('hot_value', 0)
                        
                        if title:
                            if hot_value > 10000:
                                news_list.append(f"{i}. {title} ğŸ”¥{hot_value//10000}w")
                            else:
                                news_list.append(f"{i}. {title}")
                
                if news_list:
                    return news_list
            except json.JSONDecodeError:
                pass
        
        # ç½‘é¡µæŠ“å–å¤‡ç”¨æ–¹æ¡ˆ
        url2 = "https://www.toutiao.com/"
        response2 = requests.get(url2, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response2.text, 'html.parser')
        
        news_list = []
        
        # ä»Šæ—¥å¤´æ¡çƒ­æ¦œé€‰æ‹©å™¨
        selectors = [
            '[data-track*=hot]',
            '.hot-title',
            '.hot-list-item',
            '.tt-category-hot .title',
            '.feed-card-article-title',
            '.title-box a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=10)
            for i, item in enumerate(items[:5], 1):
                title = item.text.strip()
                if title and len(title) > 8 and 'å¤´æ¡' not in title:
                    # å»é‡
                    clean_title = re.sub(r'[\d\.\s]*', '', title).strip()
                    if clean_title and clean_title not in [re.sub(r'[\d\.\sğŸ”¥\w]*', '', n).strip() for n in news_list]:
                        news_list.append(f"{i}. {title}")
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        if not news_list:
            # ä»é¡µé¢æ–‡æœ¬ä¸­æå–
            all_text = soup.get_text()
            lines = [line.strip() for line in all_text.split('\n') if 10 < len(line.strip()) < 100]
            unique_lines = []
            for line in lines:
                if line not in unique_lines:
                    unique_lines.append(line)
            for i, line in enumerate(unique_lines[:5], 1):
                news_list.append(f"{i}. {line}")
        
        return news_list if news_list else ["1. ä»Šæ—¥å¤´æ¡çƒ­æ¦œæ›´æ–°ä¸­", "2. èµ„è®¯å¹³å°çƒ­ç‚¹"]
        
    except Exception as e:
        logger.warning(f"ä»Šæ—¥å¤´æ¡çƒ­æ¦œæŠ“å–å¤±è´¥: {e}")
        return ["1. ä»Šæ—¥å¤´æ¡çƒ­æ¦œ", "2. èµ„è®¯çƒ­ç‚¹", "3. å¹³å°çƒ­é—¨"]

# ====================== åŸæœ‰æ–°é—»æºå‡½æ•°ï¼ˆä¿æŒåŸæœ‰ç»“æ„ï¼‰ ======================

def fetch_politics_news():
    """è·å–æ—¶æ”¿æ–°é—»ï¼ˆäººæ°‘ç½‘+æ–°åç½‘ï¼‰"""
    try:
        news_list = []
        
        # äººæ°‘ç½‘æ—¶æ”¿
        url1 = "http://politics.people.com.cn/"
        response1 = requests.get(url1, headers=HEADERS, timeout=10)
        soup1 = BeautifulSoup(response1.text, 'html.parser')
        
        selectors1 = ['.news_box .news a', '.hdNews a', '.news_tu h2 a', '.news_title a']
        for selector in selectors1:
            items = soup1.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6 and 'äººæ°‘ç½‘' not in title:
                    keywords = ['ä¹ è¿‘å¹³', 'æ€»ç†', 'å›½åŠ¡é™¢', 'å¤–äº¤éƒ¨', 'æ”¿ç­–', 'ä¼šè®®', 'é¢†å¯¼äºº', 'å¤–äº¤']
                    if any(keyword in title for keyword in keywords):
                        if title not in news_list:
                            news_list.append(title)
                if len(news_list) >= 8:
                    break
            if len(news_list) >= 8:
                break
        
        # æ–°åç½‘æ—¶æ”¿
        url2 = "http://www.xinhuanet.com/politics/"
        response2 = requests.get(url2, headers=HEADERS, timeout=10)
        soup2 = BeautifulSoup(response2.text, 'html.parser')
        
        selectors2 = ['.tit', '.news-item h3', '.hdNews a', '.news_tu h2 a']
        for selector in selectors2:
            items = soup2.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6 and 'æ–°åç½‘' not in title:
                    if 'æ—¶æ”¿' in title or any(keyword in title for keyword in ['æ”¿æ²»', 'æ”¿åºœ', 'æ”¿ç­–']):
                        if title not in news_list:
                            news_list.append(title)
                if len(news_list) >= 8:
                    break
            if len(news_list) >= 8:
                break
        
        # æ ¼å¼åŒ–è¾“å‡ºå‰5æ¡
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["1. æ—¶æ”¿è¦é—»æ›´æ–°ä¸­", "2. é‡è¦ä¼šè®®è¿›è¡Œæ—¶"]
        
    except Exception as e:
        logger.warning(f"æ—¶æ”¿æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. æ—¶æ”¿è¦é—»", "2. æ”¿ç­–åŠ¨æ€", "3. é‡è¦ä¼šè®®"]

def fetch_military_news():
    """è·å–å†›äº‹æ–°é—»"""
    try:
        news_list = []
        
        # æ–°åç½‘å†›äº‹
        url = "http://www.xinhuanet.com/mil/"
        response = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        selectors = ['.tit', '.news-item h3', '.hdNews a', '.news_tu h2 a', '.title a']
        for selector in selectors:
            items = soup.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6:
                    keywords = ['å†›é˜Ÿ', 'å›½é˜²', 'å†›äº‹', 'æ¼”ä¹ ', 'æ­¦å™¨', 'æµ·å†›', 'ç©ºå†›', 'é™†å†›']
                    if any(keyword in title for keyword in keywords):
                        if title not in news_list:
                            news_list.append(title)
                if len(news_list) >= 8:
                    break
            if len(news_list) >= 8:
                break
        
        # å¦‚æœä¸å¤Ÿï¼Œä»äººæ°‘ç½‘è¡¥å……
        if len(news_list) < 5:
            url2 = "http://military.people.com.cn/"
            try:
                response2 = requests.get(url2, headers=HEADERS, timeout=8)
                soup2 = BeautifulSoup(response2.text, 'html.parser')
                items2 = soup2.select('a', limit=20)
                for item in items2:
                    title = item.text.strip()
                    if title and len(title) > 8 and 'å†›äº‹' in title:
                        if title not in news_list:
                            news_list.append(title)
                    if len(news_list) >= 8:
                        break
            except:
                pass
        
        # æ ¼å¼åŒ–è¾“å‡ºå‰5æ¡
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["1. å†›äº‹åŠ¨æ€æ›´æ–°ä¸­", "2. å›½é˜²å»ºè®¾è¿›å±•"]
        
    except Exception as e:
        logger.warning(f"å†›äº‹æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. å†›äº‹åŠ¨æ€", "2. å›½é˜²å»ºè®¾", "3. å†›é˜Ÿæ”¹é©"]

def fetch_society_news():
    """è·å–ç¤¾ä¼šæ–°é—»ï¼ˆæ–°æµª+ç½‘æ˜“+æŠ–éŸ³çƒ­ç‚¹ï¼‰"""
    try:
        news_list = []
        
        # æ–°æµªç¤¾ä¼šæ–°é—»
        url1 = "https://news.sina.com.cn/society/"
        response1 = requests.get(url1, headers=HEADERS, timeout=10)
        soup1 = BeautifulSoup(response1.text, 'html.parser')
        
        selectors1 = ['.blk122 a', '.news-item h2 a', '.news_title a', '.title a']
        for selector in selectors1:
            items = soup1.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 8:
                    keywords = ['ç¤¾ä¼š', 'æ°‘ç”Ÿ', 'ç¤¾åŒº', 'å±…æ°‘', 'ç”Ÿæ´»', 'ç™¾å§“']
                    if any(keyword in title for keyword in keywords) or ('äº‹ä»¶' in title):
                        if title not in news_list:
                            news_list.append(title)
                if len(news_list) >= 6:
                    break
            if len(news_list) >= 6:
                break
        
        # ç½‘æ˜“ç¤¾ä¼šæ–°é—»
        url2 = "https://news.163.com/shehui/"
        response2 = requests.get(url2, headers=HEADERS, timeout=10)
        soup2 = BeautifulSoup(response2.text, 'html.parser')
        
        selectors2 = ['.news_title h3 a', '.ndi_main a', '.news_item h2 a']
        for selector in selectors2:
            items = soup2.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 8:
                    if 'ç¤¾ä¼š' in title or 'æ°‘ç”Ÿ' in title:
                        if title not in news_list:
                            news_list.append(title)
                if len(news_list) >= 6:
                    break
            if len(news_list) >= 6:
                break
        
        # æŠ–éŸ³çƒ­ç‚¹ï¼ˆç¤¾ä¼šç±»ï¼‰
        try:
            douyin_news = fetch_douyin_hot()
            # ç­›é€‰ç¤¾ä¼šç›¸å…³å†…å®¹
            for news in douyin_news[:2]:
                if any(keyword in news for keyword in ['ç¤¾ä¼š', 'æ°‘ç”Ÿ', 'ç”Ÿæ´»', 'äº‹ä»¶']):
                    if news not in news_list:
                        news_list.append(news)
        except:
            pass
        
        # æ ¼å¼åŒ–è¾“å‡ºå‰5æ¡
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["1. ç¤¾ä¼šçƒ­ç‚¹æ›´æ–°ä¸­", "2. æ°‘ç”Ÿå…³æ³¨"]
        
    except Exception as e:
        logger.warning(f"ç¤¾ä¼šæ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. ç¤¾ä¼šçƒ­ç‚¹", "2. æ°‘ç”Ÿå…³æ³¨", "3. ç¤¾åŒºåŠ¨æ€"]

def fetch_economy_news():
    """è·å–ç»æµæ–°é—»ï¼ˆäººæ°‘ç½‘+æ–°åç½‘+36æ°ªï¼‰"""
    try:
        news_list = []
        
        # äººæ°‘ç½‘ç»æµ
        url1 = "http://finance.people.com.cn/"
        response1 = requests.get(url1, headers=HEADERS, timeout=10)
        soup1 = BeautifulSoup(response1.text, 'html.parser')
        
        selectors1 = ['.news_box .news a', '.hdNews a', '.news_tu h2 a', '.news_title a']
        for selector in selectors1:
            items = soup1.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6:
                    keywords = ['ç»æµ', 'é‡‘è', 'è‚¡å¸‚', 'æŠ•èµ„', 'æ¶ˆè´¹', 'GDP', 'è´¸æ˜“', 'é“¶è¡Œ']
                    if any(keyword in title for keyword in keywords):
                        if title not in news_list:
                            news_list.append(title)
                if len(news_list) >= 6:
                    break
            if len(news_list) >= 6:
                break
        
        # æ–°åç½‘ç»æµ
        url2 = "http://www.xinhuanet.com/fortune/"
        response2 = requests.get(url2, headers=HEADERS, timeout=10)
        soup2 = BeautifulSoup(response2.text, 'html.parser')
        
        selectors2 = ['.tit', '.news-item h3', '.hdNews a', '.news_tu h2 a']
        for selector in selectors2:
            items = soup2.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6:
                    if 'ç»æµ' in title or 'è´¢ç»' in title or 'é‡‘è' in title:
                        if title not in news_list:
                            news_list.append(title)
                if len(news_list) >= 6:
                    break
            if len(news_list) >= 6:
                break
        
        # 36æ°ªç»æµç±»æ–°é—»
        try:
            kr_news = fetch_36kr_hot()
            # ç­›é€‰ç»æµç›¸å…³å†…å®¹
            for news in kr_news[:2]:
                if any(keyword in news for keyword in ['ç»æµ', 'é‡‘è', 'æŠ•èµ„', 'åˆ›æŠ•', 'èèµ„']):
                    if news not in news_list:
                        news_list.append(news)
        except:
            pass
        
        # æ ¼å¼åŒ–è¾“å‡ºå‰5æ¡
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["1. ç»æµåŠ¨æ€æ›´æ–°ä¸­", "2. è´¢ç»è¦é—»"]
        
    except Exception as e:
        logger.warning(f"ç»æµæ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. ç»æµåŠ¨æ€", "2. è´¢ç»è¦é—»", "3. å¸‚åœºåˆ†æ"]

def fetch_tech_news():
    """è·å–ç§‘æŠ€æ–°é—»ï¼ˆäººæ°‘ç½‘+æ–°åç½‘+36æ°ªï¼‰"""
    try:
        news_list = []
        
        # äººæ°‘ç½‘ç§‘æŠ€
        url1 = "http://scitech.people.com.cn/"
        response1 = requests.get(url1, headers=HEADERS, timeout=10)
        soup1 = BeautifulSoup(response1.text, 'html.parser')
        
        selectors1 = ['.news_box .news a', '.hdNews a', '.news_tu h2 a', '.news_title a']
        for selector in selectors1:
            items = soup1.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6:
                    keywords = ['ç§‘æŠ€', 'åˆ›æ–°', 'äººå·¥æ™ºèƒ½', 'AI', '5G', 'èŠ¯ç‰‡', 'äº’è”ç½‘', 'æ•°å­—']
                    if any(keyword in title for keyword in keywords):
                        if title not in news_list:
                            news_list.append(title)
                if len(news_list) >= 6:
                    break
            if len(news_list) >= 6:
                break
        
        # æ–°åç½‘ç§‘æŠ€
        url2 = "http://www.xinhuanet.com/tech/"
        response2 = requests.get(url2, headers=HEADERS, timeout=10)
        soup2 = BeautifulSoup(response2.text, 'html.parser')
        
        selectors2 = ['.tit', '.news-item h3', '.hdNews a', '.news_tu h2 a']
        for selector in selectors2:
            items = soup2.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6:
                    if 'ç§‘æŠ€' in title or 'åˆ›æ–°' in title or 'æŠ€æœ¯' in title:
                        if title not in news_list:
                            news_list.append(title)
                if len(news_list) >= 6:
                    break
            if len(news_list) >= 6:
                break
        
        # 36æ°ªç§‘æŠ€æ–°é—»
        try:
            kr_news = fetch_36kr_hot()
            # ç­›é€‰ç§‘æŠ€ç›¸å…³å†…å®¹
            for news in kr_news[:3]:
                if any(keyword in news for keyword in ['ç§‘æŠ€', 'åˆ›æ–°', 'æŠ€æœ¯', 'äº’è”ç½‘', 'åˆ›ä¸š', 'èèµ„']):
                    if news not in news_list:
                        news_list.append(news)
        except:
            pass
        
        # ITä¹‹å®¶è¡¥å……
        try:
            url3 = "https://www.ithome.com/"
            response3 = requests.get(url3, headers=HEADERS, timeout=8)
            soup3 = BeautifulSoup(response3.text, 'html.parser')
            items3 = soup3.select('.title a', limit=3)
            for item in items3:
                title = item.text.strip()
                if title and len(title) > 6:
                    if title not in news_list:
                        news_list.append(title)
        except:
            pass
        
        # æ ¼å¼åŒ–è¾“å‡ºå‰5æ¡
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["1. ç§‘æŠ€å‰æ²¿æ›´æ–°ä¸­", "2. åˆ›æ–°åŠ¨æ€"]
        
    except Exception as e:
        logger.warning(f"ç§‘æŠ€æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. ç§‘æŠ€å‰æ²¿", "2. åˆ›æ–°åŠ¨æ€", "3. æ•°å­—æŠ€æœ¯"]

def fetch_hotsearch_news():
    """è·å–çƒ­æœæ–°é—»ï¼ˆå¾®åš+ç™¾åº¦+çŸ¥ä¹+æŠ–éŸ³+ä»Šæ—¥å¤´æ¡çƒ­æ¦œï¼‰"""
    try:
        news_list = []
        
        # å¾®åšçƒ­æœ
        try:
            url1 = "https://weibo.com/ajax/side/hotSearch"
            headers1 = {**HEADERS, 'Referer': 'https://weibo.com/'}
            response1 = requests.get(url1, headers=headers1, timeout=10)
            data1 = response1.json()
            
            if 'data' in data1 and 'realtime' in data1['data']:
                for item in data1['data']['realtime'][:3]:
                    title = item.get('note', '')
                    if title and 'æ¨è' not in title:
                        hot = item.get('num', 0)
                        if hot > 10000:
                            news_list.append(f"{title} ğŸ”¥{hot//10000}w")
                        else:
                            news_list.append(title)
        except:
            pass
        
        # ç™¾åº¦çƒ­æœ
        try:
            url2 = "https://top.baidu.com/board?tab=realtime"
            response2 = requests.get(url2, headers=HEADERS, timeout=10)
            soup2 = BeautifulSoup(response2.text, 'html.parser')
            
            items2 = soup2.select('.c-single-text-ellipsis', limit=3)
            for item in items2:
                title = item.text.strip()
                if title:
                    news_list.append(title)
        except:
            pass
        
        # çŸ¥ä¹çƒ­æ¦œ
        try:
            url3 = "https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total?limit=3"
            headers3 = {**HEADERS, 'Referer': 'https://www.zhihu.com/'}
            response3 = requests.get(url3, headers=headers3, timeout=10)
            data3 = response3.json()
            
            if 'data' in data3:
                for item in data3['data'][:3]:
                    title = item.get('target', {}).get('title', '')
                    if title:
                        news_list.append(title)
        except:
            pass
        
        # æŠ–éŸ³çƒ­ç‚¹
        try:
            douyin_news = fetch_douyin_hot()
            for news in douyin_news[:2]:
                if news not in news_list:
                    news_list.append(news)
        except:
            pass
        
        # ä»Šæ—¥å¤´æ¡çƒ­æ¦œ
        try:
            toutiao_news = fetch_toutiao_hotlist()
            for news in toutiao_news[:2]:
                if news not in news_list:
                    news_list.append(news)
        except:
            pass
        
        # æ ¼å¼åŒ–è¾“å‡ºå‰5æ¡
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["1. å…¨ç½‘çƒ­æœæ›´æ–°ä¸­", "2. çƒ­é—¨è¯é¢˜"]
        
    except Exception as e:
        logger.warning(f"çƒ­æœæ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. å¾®åšçƒ­æœ", "2. ç™¾åº¦çƒ­æ¦œ", "3. çŸ¥ä¹çƒ­æ¦œ"]

def fetch_sports_news():
    """è·å–ä½“è‚²æ–°é—»ï¼ˆæ–°æµª+ç½‘æ˜“+æŠ–éŸ³ä½“è‚²çƒ­ç‚¹ï¼‰"""
    try:
        news_list = []
        
        # æ–°æµªä½“è‚²
        url1 = "https://sports.sina.com.cn/"
        response1 = requests.get(url1, headers=HEADERS, timeout=10)
        soup1 = BeautifulSoup(response1.text, 'html.parser')
        
        selectors1 = ['.blk122 a', '.news-item h2 a', '.news_title a', '.title a']
        for selector in selectors1:
            items = soup1.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6:
                    keywords = ['ä½“è‚²', 'èµ›äº‹', 'æ¯”èµ›', 'è¿åŠ¨å‘˜', 'å† å†›', 'è¶³çƒ', 'ç¯®çƒ', 'å¥¥è¿']
                    if any(keyword in title for keyword in keywords):
                        if title not in news_list:
                            news_list.append(title)
                if len(news_list) >= 6:
                    break
            if len(news_list) >= 6:
                break
        
        # ç½‘æ˜“ä½“è‚²
        url2 = "https://sports.163.com/"
        response2 = requests.get(url2, headers=HEADERS, timeout=10)
        soup2 = BeautifulSoup(response2.text, 'html.parser')
        
        selectors2 = ['.news_title h3 a', '.ndi_main a', '.news_item h2 a']
        for selector in selectors2:
            items = soup2.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6:
                    if 'ä½“è‚²' in title or 'è¿åŠ¨' in title:
                        if title not in news_list:
                            news_list.append(title)
                if len(news_list) >= 6:
                    break
            if len(news_list) >= 6:
                break
        
        # æŠ–éŸ³ä½“è‚²çƒ­ç‚¹
        try:
            douyin_news = fetch_douyin_hot()
            # ç­›é€‰ä½“è‚²ç›¸å…³å†…å®¹
            for news in douyin_news[:2]:
                if any(keyword in news for keyword in ['ä½“è‚²', 'æ¯”èµ›', 'è¿åŠ¨', 'è¶³çƒ', 'ç¯®çƒ']):
                    if news not in news_list:
                        news_list.append(news)
        except:
            pass
        
        # æ ¼å¼åŒ–è¾“å‡ºå‰5æ¡
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["1. ä½“è‚²èµ›äº‹æ›´æ–°ä¸­", "2. ä½“å›åŠ¨æ€"]
        
    except Exception as e:
        logger.warning(f"ä½“è‚²æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. ä½“è‚²èµ›äº‹", "2. ä½“å›åŠ¨æ€", "3. è¿åŠ¨å‘˜é£é‡‡"]

def fetch_edu_news():
    """è·å–æ–‡æ•™æ–°é—»ï¼ˆæ•™è‚²+æ–‡åŒ–+æŠ–éŸ³çŸ¥è¯†ç±»ï¼‰"""
    try:
        news_list = []
        
        # äººæ°‘ç½‘æ•™è‚²
        url1 = "http://edu.people.com.cn/"
        response1 = requests.get(url1, headers=HEADERS, timeout=10)
        soup1 = BeautifulSoup(response1.text, 'html.parser')
        
        selectors1 = ['.news_box .news a', '.hdNews a', '.news_tu h2 a', '.news_title a']
        for selector in selectors1:
            items = soup1.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6:
                    keywords = ['æ•™è‚²', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'æ•™å¸ˆ', 'æ–‡åŒ–', 'è‰ºæœ¯', 'è¯»ä¹¦', 'åšç‰©é¦†']
                    if any(keyword in title for keyword in keywords):
                        if title not in news_list:
                            news_list.append(title)
                if len(news_list) >= 6:
                    break
            if len(news_list) >= 6:
                break
        
        # æ–°åç½‘æ–‡åŒ–
        url2 = "http://www.xinhuanet.com/culture/"
        response2 = requests.get(url2, headers=HEADERS, timeout=10)
        soup2 = BeautifulSoup(response2.text, 'html.parser')
        
        selectors2 = ['.tit', '.news-item h3', '.hdNews a', '.news_tu h2 a']
        for selector in selectors2:
            items = soup2.select(selector, limit=10)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6:
                    if 'æ–‡åŒ–' in title or 'æ•™è‚²' in title or 'è‰ºæœ¯' in title:
                        if title not in news_list:
                            news_list.append(title)
                if len(news_list) >= 6:
                    break
            if len(news_list) >= 6:
                break
        
        # æŠ–éŸ³çŸ¥è¯†ç±»å†…å®¹
        try:
            douyin_news = fetch_douyin_hot()
            # ç­›é€‰çŸ¥è¯†ã€æ•™è‚²ç›¸å…³å†…å®¹
            for news in douyin_news[:2]:
                if any(keyword in news for keyword in ['çŸ¥è¯†', 'å­¦ä¹ ', 'æ•™è‚²', 'æ–‡åŒ–', 'è‰ºæœ¯']):
                    if news not in news_list:
                        news_list.append(news)
        except:
            pass
        
        # æ ¼å¼åŒ–è¾“å‡ºå‰5æ¡
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["1. æ–‡æ•™åŠ¨æ€æ›´æ–°ä¸­", "2. æ•™è‚²èµ„è®¯"]
        
    except Exception as e:
        logger.warning(f"æ–‡æ•™æ–°é—»æŠ“å–å¤±è´¥: {e}")
        return ["1. æ•™è‚²èµ„è®¯", "2. æ–‡åŒ–åŠ¨æ€", "3. è‰ºæœ¯å±•è§ˆ"]

# ====================== é‚®ä»¶å†…å®¹ç”Ÿæˆ ======================

def generate_email_content():
    """ç”Ÿæˆé‚®ä»¶å†…å®¹ - 8ä¸ªç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ«5æ¡ï¼Œæ•´åˆæ–°æ–°é—»æº"""
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    current_time = datetime.now().strftime("%H:%M:%S")
    
    logger.info("å¼€å§‹æŠ“å–8ä¸ªç±»åˆ«æ–°é—»ï¼Œæ•´åˆæŠ–éŸ³ã€36æ°ªã€ä»Šæ—¥å¤´æ¡çƒ­æ¦œ...")
    
    # å®šä¹‰8ä¸ªç±»åˆ«åŠå…¶å¯¹åº”çš„æŠ“å–å‡½æ•°
    news_categories = {
        "ğŸ›ï¸ æ—¶æ”¿æ–°é—»": fetch_politics_news,
        "ğŸ–ï¸ å†›äº‹åŠ¨æ€": fetch_military_news,
        "ğŸ‘¥ ç¤¾ä¼šæ°‘ç”Ÿ": fetch_society_news,
        "ğŸ“ˆ ç»æµè´¢ç»": fetch_economy_news,
        "ğŸ’» ç§‘æŠ€å‰æ²¿": fetch_tech_news,
        "ğŸ”¥ çƒ­æœæ¦œå•": fetch_hotsearch_news,
        "âš½ ä½“è‚²ç«æŠ€": fetch_sports_news,
        "ğŸ“ æ–‡æ•™è‰ºæœ¯": fetch_edu_news,
    }
    
    all_news = {}
    total_news = 0
    sources_count = {
        "æŠ–éŸ³": False,
        "36æ°ª": False,
        "ä»Šæ—¥å¤´æ¡çƒ­æ¦œ": False,
        "äººæ°‘ç½‘": True,
        "æ–°åç½‘": True,
        "å¾®åš": True,
        "ç™¾åº¦": True,
        "çŸ¥ä¹": True,
        "æ–°æµª": True,
        "ç½‘æ˜“": True,
        "ITä¹‹å®¶": True
    }
    
    # æµ‹è¯•æ–°æ–°é—»æºå¯ç”¨æ€§
    logger.info("æµ‹è¯•æ–°æ–°é—»æºå¯ç”¨æ€§...")
    try:
        test_douyin = fetch_douyin_hot()
        if len(test_douyin) > 0 and "æ›´æ–°ä¸­" not in test_douyin[0]:
            sources_count["æŠ–éŸ³"] = True
            logger.info("âœ… æŠ–éŸ³çƒ­ç‚¹å¯ç”¨")
    except:
        logger.warning("âŒ æŠ–éŸ³çƒ­ç‚¹ä¸å¯ç”¨")
    
    try:
        test_36kr = fetch_36kr_hot()
        if len(test_36kr) > 0 and "æ›´æ–°ä¸­" not in test_36kr[0]:
            sources_count["36æ°ª"] = True
            logger.info("âœ… 36æ°ªçƒ­ç‚¹å¯ç”¨")
    except:
        logger.warning("âŒ 36æ°ªçƒ­ç‚¹ä¸å¯ç”¨")
    
    try:
        test_toutiao = fetch_toutiao_hotlist()
        if len(test_toutiao) > 0 and "æ›´æ–°ä¸­" not in test_toutiao[0]:
            sources_count["ä»Šæ—¥å¤´æ¡çƒ­æ¦œ"] = True
            logger.info("âœ… ä»Šæ—¥å¤´æ¡çƒ­æ¦œå¯ç”¨")
    except:
        logger.warning("âŒ ä»Šæ—¥å¤´æ¡çƒ­æ¦œä¸å¯ç”¨")
    
    # æŠ“å–æ‰€æœ‰ç±»åˆ«æ–°é—»
    for category_name, fetch_func in news_categories.items():
        try:
            logger.info(f"æŠ“å– {category_name}...")
            news_list = fetch_func()
            all_news[category_name] = news_list
            total_news += len(news_list)
            time.sleep(0.3)  # ç¤¼è²Œé—´éš”
        except Exception as e:
            logger.warning(f"{category_name} æŠ“å–å¼‚å¸¸: {e}")
            all_news[category_name] = [f"{category_name}ï¼šæ•°æ®æ›´æ–°ä¸­"]
    
    # ç»Ÿè®¡å¯ç”¨æ–°é—»æºæ•°é‡
    available_sources = sum(1 for v in sources_count.values() if v)
    
    # çº¯æ–‡æœ¬ç‰ˆæœ¬
    text_content = f"""
æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’ ({today})
===========================================
æ›´æ–°æ—¶é—´: {current_time}
æ–°é—»ç±»åˆ«: 8å¤§ç±»ï¼Œå…±{total_news}æ¡ç²¾é€‰æ–°é—»
æ–°é—»æ¥æº: {available_sources}ä¸ªå¯ç”¨æºï¼ˆæ–°å¢æŠ–éŸ³ã€36æ°ªã€ä»Šæ—¥å¤´æ¡çƒ­æ¦œï¼‰

"""
    
    for category_name, news_list in all_news.items():
        text_content += f"\n{category_name}\n"
        text_content += "=" * 40 + "\n"
        
        for news in news_list[:5]:  # æ¯ä¸ªç±»åˆ«æ˜¾ç¤ºå‰5æ¡
            text_content += f"  {news}\n"
        
        text_content += "\n"
    
    text_content += f"""
===========================================
æœ¬é‚®ä»¶ç”± GitHub Actions è‡ªåŠ¨å‘é€
æ¯æ—¥å®šæ—¶æ¨é€: 08:00 (åŒ—äº¬æ—¶é—´)
è¦†ç›–8å¤§ç±»åˆ«: æ—¶æ”¿ã€å†›äº‹ã€ç¤¾ä¼šã€ç»æµã€ç§‘æŠ€ã€çƒ­æœã€ä½“è‚²ã€æ–‡æ•™
æ–°å¢æ–°é—»æº: æŠ–éŸ³ã€36æ°ªã€ä»Šæ—¥å¤´æ¡çƒ­æ¦œï¼ˆå…±{available_sources}ä¸ªæ–°é—»æºï¼‰
"""
    
    # HTMLç‰ˆæœ¬
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¯æ—¥çƒ­ç‚¹æ–°é—» - {today}</title>
    <style>
        body {{
            font-family: 'Microsoft YaHei', 'PingFang SC', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1100px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        }}
        .container {{
            background: white;
            border-radius: 15px;
            padding: 40px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            margin-top: 20px;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            border-radius: 12px;
            margin-bottom: 40px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 32px;
            font-weight: bold;
        }}
        .header .subtitle {{
            margin-top: 15px;
            opacity: 0.9;
            font-size: 16px;
        }}
        .categories-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(500px, 1fr));
            gap: 30px;
            margin-top: 20px;
        }}
        .category-section {{
            border-radius: 10px;
            padding: 25px;
            background: #f8f9fa;
            border: 1px solid #e1e4e8;
            transition: transform 0.3s, box-shadow 0.3s;
        }}
        .category-section:hover {{
            transform: translateY(-5px);
            box-shadow: 0 8px 25px rgba(0,0,0,0.1);
        }}
        .category-title {{
            font-size: 22px;
            margin-bottom: 20px;
            padding-bottom: 12px;
            border-bottom: 3px solid;
            font-weight: bold;
            display: flex;
            align-items: center;
            justify-content: space-between;
        }}
        .news-count {{
            font-size: 14px;
            background: white;
            padding: 4px 12px;
            border-radius: 12px;
            font-weight: normal;
        }}
        .news-list {{
            margin-top: 15px;
        }}
        .news-item {{
            margin-bottom: 12px;
            padding: 14px;
            background: white;
            border-radius: 8px;
            border-left: 4px solid;
            transition: all 0.2s;
            display: flex;
            align-items: flex-start;
        }}
        .news-item:hover {{
            transform: translateX(5px);
            background: #e9ecef;
        }}
        .news-number {{
            display: inline-block;
            width: 26px;
            height: 26px;
            line-height: 26px;
            text-align: center;
            background: #667eea;
            color: white;
            border-radius: 50%;
            margin-right: 12px;
            flex-shrink: 0;
            font-size: 14px;
            font-weight: bold;
        }}
        .news-content {{
            flex: 1;
        }}
        .footer {{
            text-align: center;
            margin-top: 50px;
            padding-top: 25px;
            border-top: 1px solid #e1e4e8;
            color: #6a737d;
            font-size: 14px;
        }}
        .hot-badge {{
            background: linear-gradient(135deg, #ff6b6b 0%, #ff8e8e 100%);
            color: white;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 12px;
            margin-left: 8px;
            font-weight: bold;
        }}
        .new-source-badge {{
            background: linear-gradient(135deg, #28a745 0%, #20c997 100%);
            color: white;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 12px;
            margin-left: 8px;
            font-weight: bold;
        }}
        .stats {{
            display: flex;
            justify-content: space-around;
            background: white;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 30px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
        }}
        .stat-item {{
            text-align: center;
        }}
        .stat-value {{
            font-size: 28px;
            font-weight: bold;
            color: #667eea;
            margin-bottom: 5px;
        }}
        .stat-label {{
            font-size: 14px;
            color: #6c757d;
        }}
        .new-features {{
            background: linear-gradient(135deg, #fff3cd 0%, #ffeaa7 100%);
            border: 2px solid #ffc107;
            border-radius: 10px;
            padding: 20px;
            margin-bottom: 25px;
            text-align: center;
        }}
        .new-features h3 {{
            color: #e65100;
            margin-top: 0;
            margin-bottom: 10px;
        }}
        .source-tags {{
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            justify-content: center;
            margin-top: 15px;
        }}
        .source-tag {{
            background: #e9ecef;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 13px;
            border: 1px solid #dee2e6;
        }}
        .source-tag.new {{
            background: #d4edda;
            color: #155724;
            border-color: #c3e6cb;
            font-weight: bold;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“° æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’</h1>
            <div class="subtitle">{today} | æ›´æ–°æ—¶é—´: {current_time}</div>
        </div>
        
        <div class="new-features">
            <h3>ğŸ‰ æ–°å¢æ–°é—»æºï¼å†…å®¹æ›´å…¨é¢</h3>
            <p>æ–°å¢æŠ–éŸ³ã€36æ°ªã€ä»Šæ—¥å¤´æ¡çƒ­æ¦œï¼Œæ•´åˆåˆ°å„å¤§æ–°é—»ç±»åˆ«ä¸­</p>
            <div class="source-tags">
                <span class="source-tag new">æŠ–éŸ³çƒ­ç‚¹</span>
                <span class="source-tag new">36æ°ªèµ„è®¯</span>
                <span class="source-tag new">ä»Šæ—¥å¤´æ¡çƒ­æ¦œ</span>
                <span class="source-tag">äººæ°‘ç½‘</span>
                <span class="source-tag">æ–°åç½‘</span>
                <span class="source-tag">å¾®åšçƒ­æœ</span>
                <span class="source-tag">ç™¾åº¦çƒ­æœ</span>
                <span class="source-tag">çŸ¥ä¹çƒ­æ¦œ</span>
                <span class="source-tag">æ–°æµªæ–°é—»</span>
                <span class="source-tag">ç½‘æ˜“æ–°é—»</span>
                <span class="source-tag">ITä¹‹å®¶</span>
            </div>
        </div>
        
        <div class="stats">
            <div class="stat-item">
                <div class="stat-value">8</div>
                <div class="stat-label">æ–°é—»ç±»åˆ«</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">{total_news}</div>
                <div class="stat-label">ç²¾é€‰æ–°é—»</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">{available_sources}</div>
                <div class="stat-label">æ–°é—»æ¥æº</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">{datetime.now().strftime('%H:%M')}</div>
                <div class="stat-label">å‘å¸ƒæ—¶é—´</div>
            </div>
        </div>
        
        <div class="categories-grid">
"""
    
    # ç±»åˆ«é¢œè‰²æ˜ å°„
    category_colors = {
        "ğŸ›ï¸ æ—¶æ”¿æ–°é—»": "#dc3545",
        "ğŸ–ï¸ å†›äº‹åŠ¨æ€": "#495057",
        "ğŸ‘¥ ç¤¾ä¼šæ°‘ç”Ÿ": "#17a2b8",
        "ğŸ“ˆ ç»æµè´¢ç»": "#28a745",
        "ğŸ’» ç§‘æŠ€å‰æ²¿": "#007bff",
        "ğŸ”¥ çƒ­æœæ¦œå•": "#ffc107",
        "âš½ ä½“è‚²ç«æŠ€": "#e83e8c",
        "ğŸ“ æ–‡æ•™è‰ºæœ¯": "#6f42c1"
    }
    
    # æ–°æ–°é—»æºåœ¨å„ç±»åˆ«ä¸­çš„æ ‡è¯†
    new_sources_in_categories = {
        "ğŸ‘¥ ç¤¾ä¼šæ°‘ç”Ÿ": ["æŠ–éŸ³"],
        "ğŸ“ˆ ç»æµè´¢ç»": ["36æ°ª"],
        "ğŸ’» ç§‘æŠ€å‰æ²¿": ["36æ°ª"],
        "ğŸ”¥ çƒ­æœæ¦œå•": ["æŠ–éŸ³", "ä»Šæ—¥å¤´æ¡çƒ­æ¦œ"],
        "âš½ ä½“è‚²ç«æŠ€": ["æŠ–éŸ³"],
        "ğŸ“ æ–‡æ•™è‰ºæœ¯": ["æŠ–éŸ³"]
    }
    
    # æ·»åŠ å„ä¸ªç±»åˆ«
    for category_name, news_list in all_news.items():
        color = category_colors.get(category_name, "#667eea")
        new_sources = new_sources_in_categories.get(category_name, [])
        
        html_content += f"""
            <div class="category-section">
                <div class="category-title" style="color: {color}; border-color: {color}">
                    <span>
                        {category_name}
                        {'' if not new_sources else ''.join([f'<span class="new-source-badge" style="margin-left: 8px;">{src}</span>' for src in new_sources])}
                    </span>
                    <span class="news-count" style="border: 1px solid {color}; color: {color}">
                        {len(news_list)}æ¡
                    </span>
                </div>
                <div class="news-list">
"""
        
        for i, news in enumerate(news_list[:5], 1):
            # å¤„ç†çƒ­åº¦æ ‡ç­¾
            news_display = news
            if 'ğŸ”¥' in news:
                parts = news.split('ğŸ”¥')
                if len(parts) > 1:
                    news_display = f"{parts[0]}<span class='hot-badge'>ğŸ”¥{parts[1]}</span>"
            
            html_content += f"""
                    <div class="news-item" style="border-left-color: {color}">
                        <span class="news-number">{i}</span>
                        <div class="news-content">{news_display}</div>
                    </div>
"""
        
        html_content += """
                </div>
            </div>
"""
    
    html_content += f"""
        </div>
        
        <div class="footer">
            <p style="font-size: 16px; margin-bottom: 15px;">ğŸ“§ æœ¬é‚®ä»¶ç”± GitHub Actions è‡ªåŠ¨ç”Ÿæˆå¹¶å‘é€ | æ¯æ—¥æ—©8ç‚¹å‡†æ—¶æ¨é€</p>
            <p>ğŸ”§ æŠ€æœ¯æ”¯æŒ: Python + BeautifulSoup + Requests + GitHub Actions</p>
            <p>ğŸ“Š æ•°æ®æ¥æº: å…±{available_sources}ä¸ªæ–°é—»æºï¼Œæ–°å¢æŠ–éŸ³ã€36æ°ªã€ä»Šæ—¥å¤´æ¡çƒ­æ¦œ</p>
            <p>â° æ•°æ®é‡‡é›†æ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
            <p style="margin-top: 15px; color: #495057; font-size: 13px;">
                è¦†ç›–8å¤§ç±»åˆ«: æ—¶æ”¿ â€¢ å†›äº‹ â€¢ ç¤¾ä¼š â€¢ ç»æµ â€¢ ç§‘æŠ€ â€¢ çƒ­æœ â€¢ ä½“è‚² â€¢ æ–‡æ•™ | æ¯ä¸ªç±»åˆ«ç²¾é€‰5æ¡æ–°é—»
            </p>
            <p style="margin-top: 10px; color: #28a745; font-weight: bold;">
                âœ… æ–°å¢æŠ–éŸ³ã€36æ°ªã€ä»Šæ—¥å¤´æ¡çƒ­æ¦œï¼Œå†…å®¹æ¥æºæ›´ä¸°å¯Œï¼
            </p>
        </div>
    </div>
</body>
</html>
"""
    
    return text_content, html_content

def send_email_simple(text_content, html_content):
    """å‘é€é‚®ä»¶ - æœ€ç®€å•ç‰ˆï¼ˆè§£å†³QQé‚®ç®±æ ¼å¼é—®é¢˜ï¼‰"""
    sender = os.getenv('EMAIL_SENDER')
    password = os.getenv('EMAIL_PASSWORD')
    receiver = os.getenv('EMAIL_RECEIVER')
    
    if not all([sender, password, receiver]):
        logger.error("âŒ ç¯å¢ƒå˜é‡ç¼ºå¤±")
        return False
    
    try:
        logger.info(f"å‡†å¤‡å‘é€é‚®ä»¶åˆ° {receiver}")
        
        # åˆ›å»ºé‚®ä»¶ - ä½¿ç”¨æœ€ç®€å•çš„æ ¼å¼
        msg = MIMEMultipart('alternative')
        
        # å…³é”®ï¼šåªä½¿ç”¨é‚®ç®±åœ°å€ï¼Œä¸æ·»åŠ ä»»ä½•é¢å¤–ä¿¡æ¯
        msg['From'] = sender
        msg['To'] = receiver
        
        today_str = datetime.now().strftime('%mæœˆ%dæ—¥')
        # ç®€åŒ–ä¸»é¢˜
        msg['Subject'] = f"æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’ - {today_str}"
        
        # æ·»åŠ çº¯æ–‡æœ¬ç‰ˆæœ¬
        part1 = MIMEText(text_content, 'plain', 'utf-8')
        msg.attach(part1)
        
        # æ·»åŠ HTMLç‰ˆæœ¬
        part2 = MIMEText(html_content, 'html', 'utf-8')
        msg.attach(part2)
        
        # å‘é€é‚®ä»¶
        logger.info("è¿æ¥QQé‚®ç®±SMTPæœåŠ¡å™¨...")
        server = smtplib.SMTP('smtp.qq.com', 587, timeout=30)
        
        logger.info("å¯åŠ¨TLSåŠ å¯†...")
        server.starttls()
        
        logger.info(f"ç™»å½•é‚®ç®±...")
        server.login(sender, password)
        
        logger.info("å‘é€é‚®ä»¶...")
        server.sendmail(sender, receiver, msg.as_string())
        
        logger.info("å…³é—­è¿æ¥...")
        server.quit()
        
        logger.info("âœ… é‚®ä»¶å‘é€æˆåŠŸï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ¯æ—¥æ–°é—»æ¨é€ä»»åŠ¡")
    logger.info("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    sender = os.getenv('EMAIL_SENDER')
    password = os.getenv('EMAIL_PASSWORD')
    receiver = os.getenv('EMAIL_RECEIVER')
    
    logger.info(f"å‘ä»¶äºº: {sender}")
    logger.info(f"æ”¶ä»¶äºº: {receiver}")
    logger.info(f"å¯†ç : {'å·²è®¾ç½®' if password else 'æœªè®¾ç½®'}")
    
    if not all([sender, password, receiver]):
        logger.error("âŒ è¯·è®¾ç½®æ‰€æœ‰ç¯å¢ƒå˜é‡")
        return False
    
    try:
        # ç”Ÿæˆé‚®ä»¶å†…å®¹
        logger.info("ç”Ÿæˆé‚®ä»¶å†…å®¹...")
        text_content, html_content = generate_email_content()
        
        # å‘é€é‚®ä»¶
        logger.info("å‘é€é‚®ä»¶...")
        success = send_email_simple(text_content, html_content)
        
        if success:
            logger.info("ğŸ‰ ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼")
            logger.info("ğŸ’¡ æç¤ºï¼šå¦‚æœæ²¡æ”¶åˆ°é‚®ä»¶ï¼Œè¯·æ£€æŸ¥åƒåœ¾é‚®ä»¶ç®±")
            return True
        else:
            logger.error("âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
