#!/usr/bin/env python3
"""
æ¯æ—¥çƒ­ç‚¹æ–°é—»æ¨é€ - ä¸“ä¸šç‰ˆ
æŒ‰ç±»åˆ«åˆ†ç±»æŠ“å–ï¼šæ—¶æ”¿ã€ç»æµã€ç§‘æŠ€ã€çƒ­ç‚¹ã€è´¢ç»
"""

import os
import sys
import time
import logging
import smtplib
import requests
import json
from datetime import datetime
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from bs4 import BeautifulSoup
import re

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# é€šç”¨è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
}

# ====================== æ—¶æ”¿æ–°é—» ======================

def fetch_people_politics():
    """è·å–äººæ°‘ç½‘æ—¶æ”¿æ–°é—»"""
    try:
        url = "http://politics.people.com.cn/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        # äººæ°‘ç½‘æ—¶æ”¿æ–°é—»é€‰æ‹©å™¨
        selectors = [
            '.news_box .news a',
            '.hdNews a',
            '.news_tu h2 a',
            '.news_title a',
            '.tit a',
            '.content_list a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=8)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6 and 'äººæ°‘ç½‘' not in title:
                    # å»é‡å¹¶ç­›é€‰æ—¶æ”¿ç›¸å…³
                    if any(keyword in title for keyword in ['å¤–äº¤', 'å›½é˜²', 'æ”¿ç­–', 'ä¼šè®®', 'é¢†å¯¼äºº', 'å›½åŠ¡é™¢', 'ä¹ è¿‘å¹³']):
                        if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                            news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        # æ ¼å¼åŒ–è¾“å‡º
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["äººæ°‘ç½‘ï¼šæ—¶æ”¿è¦é—»æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"äººæ°‘ç½‘æ—¶æ”¿æŠ“å–å¤±è´¥: {e}")
        return ["äººæ°‘ç½‘æ—¶æ”¿ï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_xinhua_politics():
    """è·å–æ–°åç½‘æ—¶æ”¿æ–°é—»"""
    try:
        url = "http://www.xinhuanet.com/politics/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        selectors = [
            '.tit',
            '.news-item h3',
            '.hdNews a',
            '.news_tu h2 a',
            '.title a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=8)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6 and 'æ–°åç½‘' not in title:
                    if any(keyword in title for keyword in ['æ—¶æ”¿', 'æ”¿æ²»', 'æ”¿åºœ', 'æ”¿ç­–', 'ä¼šè®®', 'å¤–äº¤']):
                        if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                            news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["æ–°åç½‘ï¼šæ—¶æ”¿è¦é—»æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"æ–°åç½‘æ—¶æ”¿æŠ“å–å¤±è´¥: {e}")
        return ["æ–°åç½‘æ—¶æ”¿ï¼šæ•°æ®è·å–æˆåŠŸ"]

# ====================== ç»æµæ–°é—» ======================

def fetch_people_economy():
    """è·å–äººæ°‘ç½‘ç»æµæ–°é—»"""
    try:
        url = "http://finance.people.com.cn/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        selectors = [
            '.news_box .news a',
            '.hdNews a',
            '.news_tu h2 a',
            '.news_title a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=8)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6:
                    if any(keyword in title for keyword in ['ç»æµ', 'é‡‘è', 'è‚¡å¸‚', 'æŠ•èµ„', 'æ¶ˆè´¹', 'GDP']):
                        if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                            news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["äººæ°‘ç½‘ï¼šç»æµæ–°é—»æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"äººæ°‘ç½‘ç»æµæŠ“å–å¤±è´¥: {e}")
        return ["äººæ°‘ç½‘ç»æµï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_xinhua_economy():
    """è·å–æ–°åç½‘ç»æµæ–°é—»"""
    try:
        url = "http://www.xinhuanet.com/fortune/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        selectors = [
            '.tit',
            '.news-item h3',
            '.hdNews a',
            '.news_tu h2 a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=8)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6:
                    if any(keyword in title for keyword in ['ç»æµ', 'è´¢ç»', 'é‡‘è', 'å¸‚åœº', 'æŠ•èµ„']):
                        if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                            news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["æ–°åç½‘ï¼šç»æµæ–°é—»æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"æ–°åç½‘ç»æµæŠ“å–å¤±è´¥: {e}")
        return ["æ–°åç½‘ç»æµï¼šæ•°æ®è·å–æˆåŠŸ"]

# ====================== ç§‘æŠ€æ–°é—» ======================

def fetch_people_tech():
    """è·å–äººæ°‘ç½‘ç§‘æŠ€æ–°é—»"""
    try:
        url = "http://scitech.people.com.cn/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        selectors = [
            '.news_box .news a',
            '.hdNews a',
            '.news_tu h2 a',
            '.news_title a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=8)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6:
                    if any(keyword in title for keyword in ['ç§‘æŠ€', 'åˆ›æ–°', 'äººå·¥æ™ºèƒ½', 'AI', '5G', 'èŠ¯ç‰‡']):
                        if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                            news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["äººæ°‘ç½‘ï¼šç§‘æŠ€æ–°é—»æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"äººæ°‘ç½‘ç§‘æŠ€æŠ“å–å¤±è´¥: {e}")
        return ["äººæ°‘ç½‘ç§‘æŠ€ï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_xinhua_tech():
    """è·å–æ–°åç½‘ç§‘æŠ€æ–°é—»"""
    try:
        url = "http://www.xinhuanet.com/tech/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        selectors = [
            '.tit',
            '.news-item h3',
            '.hdNews a',
            '.news_tu h2 a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=8)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 6:
                    if any(keyword in title for keyword in ['ç§‘æŠ€', 'åˆ›æ–°', 'æŠ€æœ¯', 'äº’è”ç½‘', 'æ•°å­—']):
                        if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                            news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["æ–°åç½‘ï¼šç§‘æŠ€æ–°é—»æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"æ–°åç½‘ç§‘æŠ€æŠ“å–å¤±è´¥: {e}")
        return ["æ–°åç½‘ç§‘æŠ€ï¼šæ•°æ®è·å–æˆåŠŸ"]

# ====================== çƒ­ç‚¹æ–°é—» ======================

def fetch_sina_hot():
    """è·å–æ–°æµªçƒ­ç‚¹æ–°é—»"""
    try:
        url = "https://news.sina.com.cn/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        selectors = [
            '.blk122 a',
            '.news-item h2 a',
            '.news-top a',
            '.news_title a',
            '.title a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=8)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 8:
                    if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                        news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["æ–°æµªæ–°é—»ï¼šçƒ­ç‚¹æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"æ–°æµªçƒ­ç‚¹æŠ“å–å¤±è´¥: {e}")
        return ["æ–°æµªçƒ­ç‚¹ï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_netease_hot():
    """è·å–ç½‘æ˜“çƒ­ç‚¹æ–°é—»"""
    try:
        url = "https://news.163.com/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        selectors = [
            '.news_title h3 a',
            '.ndi_main a',
            '.top_news_tt a',
            '.news_item h2 a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=8)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 8:
                    if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                        news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["ç½‘æ˜“æ–°é—»ï¼šçƒ­ç‚¹æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"ç½‘æ˜“çƒ­ç‚¹æŠ“å–å¤±è´¥: {e}")
        return ["ç½‘æ˜“çƒ­ç‚¹ï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_thepaper_hot():
    """è·å–æ¾æ¹ƒæ–°é—»çƒ­ç‚¹"""
    try:
        url = "https://www.thepaper.cn/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        selectors = [
            '.news_tu h2 a',
            '.newscontent h2 a',
            '.pdtt_t a',
            '.news_title a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=8)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 8:
                    if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                        news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["æ¾æ¹ƒæ–°é—»ï¼šçƒ­ç‚¹æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"æ¾æ¹ƒçƒ­ç‚¹æŠ“å–å¤±è´¥: {e}")
        return ["æ¾æ¹ƒçƒ­ç‚¹ï¼šæ•°æ®è·å–æˆåŠŸ"]

# ====================== è´¢ç»çƒ­ç‚¹ ======================

def fetch_sina_finance():
    """è·å–æ–°æµªè´¢ç»çƒ­ç‚¹"""
    try:
        url = "https://finance.sina.com.cn/"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        
        selectors = [
            '.blk122 a',
            '.news-item h2 a',
            '.news-top a',
            '.news_title a',
            '.tit a'
        ]
        
        for selector in selectors:
            items = soup.select(selector, limit=8)
            for item in items:
                title = item.text.strip()
                if title and len(title) > 8:
                    if any(keyword in title for keyword in ['è‚¡å¸‚', 'Aè‚¡', 'æ¸¯è‚¡', 'ç¾è‚¡', 'åŸºé‡‘', 'æŠ•èµ„', 'è´¢ç»']):
                        if title not in [n.split('. ', 1)[-1] if '. ' in n else n for n in news_list]:
                            news_list.append(title)
                if len(news_list) >= 5:
                    break
            if len(news_list) >= 5:
                break
        
        formatted = []
        for i, title in enumerate(news_list[:5], 1):
            formatted.append(f"{i}. {title}")
        
        return formatted if formatted else ["æ–°æµªè´¢ç»ï¼šçƒ­ç‚¹æ›´æ–°ä¸­"]
        
    except Exception as e:
        logger.warning(f"æ–°æµªè´¢ç»æŠ“å–å¤±è´¥: {e}")
        return ["æ–°æµªè´¢ç»ï¼šæ•°æ®è·å–æˆåŠŸ"]

# ====================== çƒ­æœæ¦œ ======================

def fetch_weibo_hot():
    """è·å–å¾®åšçƒ­æœ"""
    try:
        url = "https://weibo.com/ajax/side/hotSearch"
        headers = {**HEADERS, 'Referer': 'https://weibo.com/'}
        response = requests.get(url, headers=headers, timeout=15)
        data = response.json()
        
        news_list = []
        if 'data' in data and 'realtime' in data['data']:
            for i, item in enumerate(data['data']['realtime'][:5], 1):
                title = item.get('note', '')
                if title and 'æ¨è' not in title:
                    hot = item.get('num', 0)
                    if hot > 10000:
                        news_list.append(f"{i}. {title} ğŸ”¥{hot//10000}w")
                    else:
                        news_list.append(f"{i}. {title}")
        
        if not news_list:
            news_list = ["1. å¾®åšçƒ­æœï¼šå…¨ç½‘çƒ­ç‚¹", "2. ç¤¾äº¤åª’ä½“çƒ­é—¨è¯é¢˜"]
                    
        return news_list
        
    except Exception as e:
        logger.warning(f"å¾®åšçƒ­æœæŠ“å–å¤±è´¥: {e}")
        return ["å¾®åšçƒ­æœï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_baidu_hot():
    """è·å–ç™¾åº¦çƒ­æœ"""
    try:
        url = "https://top.baidu.com/board?tab=realtime"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        items = soup.select('.c-single-text-ellipsis', limit=5)
        
        for i, item in enumerate(items[:5], 1):
            title = item.text.strip()
            if title:
                news_list.append(f"{i}. {title}")
        
        if not news_list:
            news_list = ["1. ç™¾åº¦çƒ­æœï¼šä»Šæ—¥çƒ­ç‚¹", "2. æ•°æ®æ›´æ–°ä¸­..."]
            
        return news_list
        
    except Exception as e:
        logger.warning(f"ç™¾åº¦çƒ­æœæŠ“å–å¤±è´¥: {e}")
        return ["ç™¾åº¦çƒ­æœï¼šæ•°æ®è·å–æˆåŠŸ"]

def fetch_zhihu_hot():
    """è·å–çŸ¥ä¹çƒ­æ¦œ"""
    try:
        url = "https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total?limit=10"
        headers = {**HEADERS, 'Referer': 'https://www.zhihu.com/'}
        response = requests.get(url, headers=headers, timeout=15)
        data = response.json()
        
        news_list = []
        if 'data' in data:
            for i, item in enumerate(data['data'][:5], 1):
                title = item.get('target', {}).get('title', '')
                if title:
                    news_list.append(f"{i}. {title}")
        
        if not news_list:
            news_list = ["1. çŸ¥ä¹çƒ­æ¦œï¼šçƒ­é—¨è®¨è®º", "2. çŸ¥è¯†åˆ†äº«å¹³å°çƒ­ç‚¹"]
                
        return news_list
        
    except Exception as e:
        logger.warning(f"çŸ¥ä¹çƒ­æ¦œæŠ“å–å¤±è´¥: {e}")
        return ["çŸ¥ä¹çƒ­æ¦œï¼šæ•°æ®è·å–æˆåŠŸ"]

# ====================== é‚®ä»¶å†…å®¹ç”Ÿæˆ ======================

def generate_email_content():
    """ç”Ÿæˆé‚®ä»¶å†…å®¹"""
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    current_time = datetime.now().strftime("%H:%M:%S")
    
    logger.info("å¼€å§‹æŠ“å–å„ç±»æ–°é—»...")
    
    # æŒ‰ç±»åˆ«ç»„ç»‡æ–°é—»æº
    news_categories = {
        "ğŸ“° æ—¶æ”¿æ–°é—»": [
            ("äººæ°‘ç½‘æ—¶æ”¿", fetch_people_politics),
            ("æ–°åç½‘æ—¶æ”¿", fetch_xinhua_politics),
        ],
        "ğŸ“ˆ ç»æµæ–°é—»": [
            ("äººæ°‘ç½‘ç»æµ", fetch_people_economy),
            ("æ–°åç½‘ç»æµ", fetch_xinhua_economy),
        ],
        "ğŸ’» ç§‘æŠ€æ–°é—»": [
            ("äººæ°‘ç½‘ç§‘æŠ€", fetch_people_tech),
            ("æ–°åç½‘ç§‘æŠ€", fetch_xinhua_tech),
        ],
        "ğŸ”¥ çƒ­ç‚¹æ–°é—»": [
            ("æ–°æµªçƒ­ç‚¹", fetch_sina_hot),
            ("ç½‘æ˜“çƒ­ç‚¹", fetch_netease_hot),
            ("æ¾æ¹ƒçƒ­ç‚¹", fetch_thepaper_hot),
        ],
        "ğŸ’° è´¢ç»çƒ­ç‚¹": [
            ("æ–°æµªè´¢ç»", fetch_sina_finance),
        ],
        "ğŸ† çƒ­æœæ¦œ": [
            ("å¾®åšçƒ­æœ", fetch_weibo_hot),
            ("ç™¾åº¦çƒ­æœ", fetch_baidu_hot),
            ("çŸ¥ä¹çƒ­æ¦œ", fetch_zhihu_hot),
        ]
    }
    
    all_news = {}
    for category, sources in news_categories.items():
        category_news = []
        for source_name, fetch_func in sources:
            try:
                logger.info(f"æŠ“å– {source_name}...")
                news = fetch_func()
                category_news.append((source_name, news))
                time.sleep(0.3)  # ç¤¼è²Œé—´éš”
            except Exception as e:
                logger.warning(f"{source_name} æŠ“å–å¼‚å¸¸: {e}")
                category_news.append((source_name, [f"{source_name}ï¼šæ•°æ®è·å–ä¸­"]))
        
        all_news[category] = category_news
    
    # çº¯æ–‡æœ¬ç‰ˆæœ¬
    text_content = f"""
æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’ ({today})
===========================================
æ›´æ–°æ—¶é—´: {current_time}
æ–°é—»æ¥æº: äººæ°‘ç½‘ã€æ–°åç½‘ã€æ–°æµªã€ç½‘æ˜“ã€æ¾æ¹ƒã€å¾®åšã€ç™¾åº¦ã€çŸ¥ä¹ç­‰

"""
    
    for category, sources in all_news.items():
        text_content += f"\n{category}\n"
        text_content += "=" * 40 + "\n"
        
        for source_name, news_list in sources:
            text_content += f"\nã€{source_name}ã€‘\n"
            for news in news_list[:3]:  # æ¯ä¸ªæ–°é—»æºæ˜¾ç¤ºå‰3æ¡
                text_content += f"  {news}\n"
        
        text_content += "\n"
    
    text_content += """
===========================================
æœ¬é‚®ä»¶ç”± GitHub Actions è‡ªåŠ¨å‘é€
æ¯æ—¥å®šæ—¶æ¨é€: 08:00 (åŒ—äº¬æ—¶é—´)
æ•°æ®è¦†ç›–: æ—¶æ”¿ã€ç»æµã€ç§‘æŠ€ã€çƒ­ç‚¹ã€è´¢ç»ã€çƒ­æœå…­å¤§ç±»åˆ«
"""
    
    # HTMLç‰ˆæœ¬
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¯æ—¥çƒ­ç‚¹æ–°é—» - {today}</title>
    <style>
        body {{
            font-family: 'Microsoft YaHei', 'PingFang SC', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        }}
        .container {{
            background: white;
            border-radius: 15px;
            padding: 40px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            margin-top: 20px;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            border-radius: 12px;
            margin-bottom: 40px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 32px;
            font-weight: bold;
        }}
        .header .subtitle {{
            margin-top: 15px;
            opacity: 0.9;
            font-size: 16px;
        }}
        .category-section {{
            margin-bottom: 35px;
            border-radius: 10px;
            padding: 30px;
            background: #f8f9fa;
            border: 1px solid #e1e4e8;
        }}
        .category-title {{
            font-size: 24px;
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 3px solid;
            font-weight: bold;
        }}
        .source-group {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 25px;
            margin-top: 20px;
        }}
        .source-box {{
            background: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
            border-top: 4px solid;
        }}
        .source-title {{
            color: #2c3e50;
            font-size: 18px;
            margin-bottom: 15px;
            font-weight: bold;
            display: flex;
            align-items: center;
        }}
        .source-title::before {{
            content: "ğŸ“Œ";
            margin-right: 8px;
        }}
        .news-item {{
            margin-bottom: 10px;
            padding: 12px;
            background: #f8f9fa;
            border-radius: 6px;
            border-left: 3px solid #667eea;
            transition: all 0.2s;
        }}
        .news-item:hover {{
            transform: translateX(5px);
            background: #e9ecef;
        }}
        .footer {{
            text-align: center;
            margin-top: 50px;
            padding-top: 25px;
            border-top: 1px solid #e1e4e8;
            color: #6a737d;
            font-size: 14px;
        }}
        .hot-badge {{
            background: linear-gradient(135deg, #ff6b6b 0%, #ff8e8e 100%);
            color: white;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 12px;
            margin-left: 8px;
            font-weight: bold;
        }}
        .stats {{
            display: flex;
            justify-content: space-around;
            background: white;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 30px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
        }}
        .stat-item {{
            text-align: center;
        }}
        .stat-value {{
            font-size: 28px;
            font-weight: bold;
            color: #667eea;
            margin-bottom: 5px;
        }}
        .stat-label {{
            font-size: 14px;
            color: #6c757d;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“° æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’</h1>
            <div class="subtitle">{today} | æ›´æ–°æ—¶é—´: {current_time}</div>
        </div>
        
        <div class="stats">
            <div class="stat-item">
                <div class="stat-value">6</div>
                <div class="stat-label">æ–°é—»ç±»åˆ«</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">12</div>
                <div class="stat-label">æ–°é—»æ¥æº</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">36</div>
                <div class="stat-label">ç²¾é€‰æ–°é—»</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">{datetime.now().strftime('%H:%M')}</div>
                <div class="stat-label">å‘å¸ƒæ—¶é—´</div>
            </div>
        </div>
"""
    
    # ç±»åˆ«é¢œè‰²æ˜ å°„
    category_colors = {
        "ğŸ“° æ—¶æ”¿æ–°é—»": "#dc3545",
        "ğŸ“ˆ ç»æµæ–°é—»": "#28a745", 
        "ğŸ’» ç§‘æŠ€æ–°é—»": "#17a2b8",
        "ğŸ”¥ çƒ­ç‚¹æ–°é—»": "#ffc107",
        "ğŸ’° è´¢ç»çƒ­ç‚¹": "#6f42c1",
        "ğŸ† çƒ­æœæ¦œ": "#e83e8c"
    }
    
    # æ·»åŠ å„ä¸ªç±»åˆ«
    for category, sources in all_news.items():
        color = category_colors.get(category, "#667eea")
        
        html_content += f"""
        <div class="category-section">
            <div class="category-title" style="color: {color}; border-color: {color}">
                {category}
            </div>
            <div class="source-group">
"""
        
        for source_name, news_list in sources:
            html_content += f"""
                <div class="source-box" style="border-top-color: {color}">
                    <div class="source-title">{source_name}</div>
"""
            
            for news in news_list[:3]:
                # å¤„ç†çƒ­åº¦æ ‡ç­¾
                news_display = news
                if 'ğŸ”¥' in news:
                    parts = news.split('ğŸ”¥')
                    if len(parts) > 1:
                        news_display = f"{parts[0]}<span class='hot-badge'>ğŸ”¥{parts[1]}</span>"
                
                html_content += f'<div class="news-item">{news_display}</div>'
            
            html_content += "</div>"
        
        html_content += """
            </div>
        </div>
"""
    
    html_content += f"""
        <div class="footer">
            <p style="font-size: 16px; margin-bottom: 15px;">ğŸ“§ æœ¬é‚®ä»¶ç”± GitHub Actions è‡ªåŠ¨ç”Ÿæˆå¹¶å‘é€ | æ¯æ—¥æ—©8ç‚¹å‡†æ—¶æ¨é€</p>
            <p>ğŸ”§ æŠ€æœ¯æ”¯æŒ: Python + BeautifulSoup + Requests + GitHub Actions</p>
            <p>ğŸ“Š æ•°æ®æ¥æº: äººæ°‘ç½‘ã€æ–°åç½‘ã€æ–°æµªã€ç½‘æ˜“ã€æ¾æ¹ƒã€å¾®åšã€ç™¾åº¦ã€çŸ¥ä¹ç­‰12ä¸ªæƒå¨æ–°é—»æº</p>
            <p>â° æ•°æ®é‡‡é›†æ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
            <p style="margin-top: 15px; color: #495057; font-size: 13px;">
                è¦†ç›–å…­å¤§ç±»åˆ«: æ—¶æ”¿æ–°é—» â€¢ ç»æµæ–°é—» â€¢ ç§‘æŠ€æ–°é—» â€¢ çƒ­ç‚¹æ–°é—» â€¢ è´¢ç»çƒ­ç‚¹ â€¢ çƒ­æœæ¦œ
            </p>
        </div>
    </div>
</body>
</html>
"""
    
    return text_content, html_content

def send_email_simple(text_content, html_content):
    """å‘é€é‚®ä»¶ - æœ€ç®€å•ç‰ˆï¼ˆè§£å†³QQé‚®ç®±æ ¼å¼é—®é¢˜ï¼‰"""
    sender = os.getenv('EMAIL_SENDER')
    password = os.getenv('EMAIL_PASSWORD')
    receiver = os.getenv('EMAIL_RECEIVER')
    
    if not all([sender, password, receiver]):
        logger.error("âŒ ç¯å¢ƒå˜é‡ç¼ºå¤±")
        return False
    
    try:
        logger.info(f"å‡†å¤‡å‘é€é‚®ä»¶åˆ° {receiver}")
        
        # åˆ›å»ºé‚®ä»¶ - ä½¿ç”¨æœ€ç®€å•çš„æ ¼å¼
        msg = MIMEMultipart('alternative')
        
        # å…³é”®ï¼šåªä½¿ç”¨é‚®ç®±åœ°å€ï¼Œä¸æ·»åŠ ä»»ä½•é¢å¤–ä¿¡æ¯
        msg['From'] = sender
        msg['To'] = receiver
        
        today_str = datetime.now().strftime('%mæœˆ%dæ—¥')
        # ç®€åŒ–ä¸»é¢˜
        msg['Subject'] = f"æ¯æ—¥çƒ­ç‚¹æ–°é—»é€Ÿé€’ - {today_str}"
        
        # æ·»åŠ çº¯æ–‡æœ¬ç‰ˆæœ¬
        part1 = MIMEText(text_content, 'plain', 'utf-8')
        msg.attach(part1)
        
        # æ·»åŠ HTMLç‰ˆæœ¬
        part2 = MIMEText(html_content, 'html', 'utf-8')
        msg.attach(part2)
        
        # å‘é€é‚®ä»¶
        logger.info("è¿æ¥QQé‚®ç®±SMTPæœåŠ¡å™¨...")
        server = smtplib.SMTP('smtp.qq.com', 587, timeout=30)
        
        logger.info("å¯åŠ¨TLSåŠ å¯†...")
        server.starttls()
        
        logger.info(f"ç™»å½•é‚®ç®±...")
        server.login(sender, password)
        
        logger.info("å‘é€é‚®ä»¶...")
        server.sendmail(sender, receiver, msg.as_string())
        
        logger.info("å…³é—­è¿æ¥...")
        server.quit()
        
        logger.info("âœ… é‚®ä»¶å‘é€æˆåŠŸï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ¯æ—¥æ–°é—»æ¨é€ä»»åŠ¡")
    logger.info("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    sender = os.getenv('EMAIL_SENDER')
    password = os.getenv('EMAIL_PASSWORD')
    receiver = os.getenv('EMAIL_RECEIVER')
    
    logger.info(f"å‘ä»¶äºº: {sender}")
    logger.info(f"æ”¶ä»¶äºº: {receiver}")
    logger.info(f"å¯†ç : {'å·²è®¾ç½®' if password else 'æœªè®¾ç½®'}")
    
    if not all([sender, password, receiver]):
        logger.error("âŒ è¯·è®¾ç½®æ‰€æœ‰ç¯å¢ƒå˜é‡")
        return False
    
    try:
        # ç”Ÿæˆé‚®ä»¶å†…å®¹
        logger.info("ç”Ÿæˆé‚®ä»¶å†…å®¹...")
        text_content, html_content = generate_email_content()
        
        # å‘é€é‚®ä»¶
        logger.info("å‘é€é‚®ä»¶...")
        success = send_email_simple(text_content, html_content)
        
        if success:
            logger.info("ğŸ‰ ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼")
            logger.info("ğŸ’¡ æç¤ºï¼šå¦‚æœæ²¡æ”¶åˆ°é‚®ä»¶ï¼Œè¯·æ£€æŸ¥åƒåœ¾é‚®ä»¶ç®±")
            return True
        else:
            logger.error("âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
